{
  "camera sensor attestation": [
    "Get one of our Figma kits for Android or Material Design and start designing your app's UI today.\nDesign a beautiful user interface using Android best practices.\nDesign robust, testable, and maintainable app logic and services.\nPlan for app quality and align with Play store guidelines.\nLocalize your appTest your app with pseudolocalesUnicode and internationalization supportLanguage and locale resolutionPer-app language preferencesComplex XML resources\nAbout resource typesAnimationColor state listDrawableLayoutMenuStringStyleFontMore types\nAbout app manifests<action><activity><activity-alias><application><category><compatible-screens><data><grant-uri-permission><instrumentation><intent-filter><layout><manifest><meta-data><path-permission><permission><permission-group><permission-tree><profileable><property><provider><queries><receiver><service><supports-gl-texture><supports-screens><uses-configuration><uses-feature><uses-library><uses-native-library><uses-permission><uses-permission-sdk-23><uses-sdk>\nAbout device compatibilityAbout screen compatibilityEnhanced letterboxingSupport different pixel densitiesDeclare restricted screen support\nAbout multiple APKsCreate multiple APKs for different API levelsCreate multiple APKs for different screen sizesCreate multiple APKs for different GL texturesCreate multiple APKs with several dimensionsSupport different languages and culturesSupport different platform versionsFilters on Google Play64-bit Support\nGet started with large screensLarge screen canonical layouts\nMulti-window supportMedia projectionInput compatibility on large screens\nSupport different screen sizesMigrate your UI to responsive layoutsNavigation for responsive UIsActivity embedding\nLearn about foldablesMake your app fold awareTest your app on foldablesAdvanced stylusLarge screen cookbook\nGet startedPrinciples of Wear OS developmentUpgrade to the latest version of Wear OSTry out the latest version of Wear OS Developer PreviewWear OS versus mobile developmentWear OS user interfacesAccessibility on Wear OS\nWear OS developer pathwayCreate and run a wearable appDebug a Wear OS appCapture Wear UI screenshots\nAbout Wear OS appsStandalone appsAuthenticationRequest permissionsAdd a splash screenDetect locationPlaying audio on wearablesAppear in recents and app resume\nUse Jetpack Compose on Wear OSCompose performanceRotary input on ComposeNavigation with Compose for Wear OS\nBuild View-based UIs on Wear OSHandle different watch shapesCreate listsNavigationExit full screen activities on WearShow confirmationsKeep your app visible on Wear\nAbout tilesGet started with tilesUpdate tilesInteract with tilesWatch face complicationsExpose data to complications\nNotifications on Wear OSBridging options for notificationsOngoing Activities\nSend and sync data on Wear OSNetwork access and sync on Wear OSAccess the Wearable Data LayerTransfer assetsSend and receive messagesHandle data layer eventsSync data items with the Data Layer API\nPhysical buttonsRotary input on ViewsCreate input method editors in WearVoice\nAbout health servicesActive data and exercisePassive data updatesUse synthetic data providersEnhance app compatibility\nDesign watch facesBuild a watch face serviceDraw watch facesAdding complications to a watch faceCreating interactive watch facesProvide configuration activitiesAddress common issuesImprove performace with hardware accelerationOptimize performance and battery lifeSelf-tag watch facesWear app qualityPackage Wear OS appsDistribute to Wear OSCreate Wear OS apps for ChinaWear OS release notes\nIn this guideGet started with TV appsAndroidX TV librariesHandle TV hardwareManage TV controllersBuild TV layoutsOn-screen keyboardCreate TV navigationBest practices for driving engagement on Google TV\nUse Jetpack Compose on Android TVCreate a catalog browserBuild a details screen\nCreate a catalog browserProvide a card viewBuild a details viewUse transport controlsIntroduce first-time users to your appAdd a guided stepAmbient modePlayback controls on TVImplement a media sessionBackground playback in a Now Playing cardAudio capabilitiesMatch content frame rate\nAbout recommending TV contentChannels on the home screenVideo program attributesAudio program attributesGame program attributes\nAdd programsAttributesGuidelines for app developersGuidelines for TV providersPreview videosRecommendations in Android N and earlierMake TV apps searchableSearch within TV Apps\nAbout TV gamesUse Stream Protect for latency-sensitive streaming apps\nAbout TV input servicesDevelop a TV input serviceWork with channel dataManage TV user interactionSupport time-shiftingSupport content recording\nAccessibility best practicesTalkBack evaluation examplesAdopt system caption settingsCustom view accessibility supportCustom view accessibility sampleTV Apps checklistDistribute to Android TV\nBuild media apps for carsAdd support for Android AutoAdd support for Android Automotive OSBuild messaging apps for Android Auto\nBuild point of interest, internet of things, and navigation apps for cars\nUsing the Android for Cars App LibraryBuild point of interest apps for carsBuild internet of things apps for carsBuild navigation apps for carsAdd support for Android AutoAdd support for Android Automotive OS\nAbout parked appsBuild video appsBuild gamesTest Android apps for carsDistribute Android apps for carsGoogle Play services for carsNotifications on Android Automotive OS\nAbout ChromeOSBuilding apps for ChromeOSOptimizing Apps for ChromeOSPreparing your development environmentApp Manifest Compatibility for ChromebooksChromeOS Device Support for AppsApp Rendering Differences on ChromebooksWindow managementAdapting Games on ChromeOSSmooth animation on ChromeOSTest Cases for Android Apps on ChromeOS\nAbout the UI layerUI eventsState holders and UI stateState productionDomain layer\nAbout the data layerOffline firstArchitecture recommendationsLearning pathway\nAbout view bindingMigrate from Kotlin synthetics to view binding\nAbout data bindingGet startedLayouts and binding expressionsWork with observable data objectsGenerated binding classesBinding adaptersBind layout views to Architecture ComponentsTwo-way data binding\nAbout ViewModelCreate ViewModels with dependenciesViewModel Scoping APIsSaved State module for ViewModelViewModel APIs cheat sheetLiveDataSave UI statesUse Kotlin coroutines with lifecycle-aware components\nAbout pagingLoad and display paged dataPage from network and databaseTransform data streamsManage and present loading statesTest your Paging implementationMigrate to Paging 3\nDefining your WorkRequestsWork statesManaging workObserving intermediate Worker progressChaining work togetherTesting Worker implementationIntegration tests with WorkManagerDebugging WorkManager\nAbout threading in WorkManagerThreading in WorkerThreading in CoroutineWorkerThreading in RxWorkerThreading in ListenableWorkerSupport for long-running workersMigrating from Firebase JobDispatcherMigrating from GCMNetworkManager\nIntroduction to activitiesThe activity lifecycleActivity state changesTest your app's activitiesTasks and the back stackProcesses and app lifecycleParcelables and bundlesLoadersRecents screenRestrictions on starting activities from the background\nAbout app shortcutsCreate shortcutsAdd capabilitiesManage shortcutsBest practices for shortcuts\nPrinciples of navigationDesign for different form factorsHandle configuration changes\nAbout the navigation componentGetting startedCreate destinationsDesign navigation graphsNested graphsGlobal actionsNavigate to a destinationSupport multiple back stacksConditional navigationPass data between destinationsCreate a deep link for a destinationAnimate transitions between destinationsUpdate UI components with NavigationUIKotlin DSLType safe navigation with ComposeInteract programmaticallyNavigate with feature modulesBest practices for multi-module projectsTest navigationAdd new destination typesMigrate to the Navigation component\nAbout fragmentsCreate a fragmentFragment managerFragment transactionsAnimate transitions between fragmentsFragment lifecycleSaving state with fragmentsCommunicate with fragmentsWorking with the app barDisplaying dialogs with DialogFragmentDebug your fragmentsTest your fragments\nAbout app linksEnabling links to app contentVerify app linksCreate app links for instant appsCreate swipe views with tabs using ViewPagerCreate swipe views with tabs using ViewPager2\nAbout dependency injectionManual dependency injectionDependency injection with HiltHilt in multi-module appsUse Hilt with other Jetpack librariesHilt testing guideHilt and Dagger annotations cheat sheet\nDagger basicsUsing Dagger in Android appsUsing Dagger in multi-module appsApp Startup\nAbout backward-compatible UIsAbstracting the new APIsProxying to the new APIsCreating an implementation with older APIsUsing the version-aware component\nAbout app compatibilityCompatibility framework toolsRestrictions on non-SDK interfaces\nAbout interacting with other appsSending the user to another appGetting a result from an activityAllowing other apps to start your activity\nAbout package visibilityKnow which packages are visible automaticallyDeclare package visibility needsFulfill common use casesTest package visibility\nUse a media session to manage playbackPlay media in the background\nHello worldPlayer eventsPlaylistsMedia itemsMedia sourcesTrack selectionUI componentsDownloading mediaAd insertionRetrieving metadataLive streamingNetwork stacksDebug loggingAnalytics\nDigital rights managementTroubleshootingCustomizationBattery consumptionAPK shrinkingOEM testing\nMigration guideExoPlayer to Media3 mappingsPros and consDemo applicationSupported formatsSupported devicesGlossaryRelease notesJavadocGitHubBlog\nHello worldDemo applicationTransformationsSupported formats\nCustomizationTroubleshootingRelease notesJavadocGitHubExoPlayer BlogSpatial AudioHDR video playback\nAbout audio appsBuilding a media browser serviceBuilding a media browser clientMedia session callbacksUsing the media controller test app\nAbout video appsBuilding a video player activityMedia session callbacksCompatible media transcodingResponding to media buttonsHandling changes in audio outputManage audio focusMedia controls\nThe Google Assistant and media appsMedia apps on Google Assistant driving mode\nAbout routingAbout MediaRouterAbout MediaRouteProvider\nSupported media formatsUltra HDR Image Format SpecificationMedia codecsAbout MediaPlayerAbout MediaRecorderControl amplitude with VolumeShaperSharing audio inputCapture video and audio playbackFrame rateBest practices for sharing videoAdditional resources for media\nAbout servicesForeground servicesBound servicesAbout AIDL\nAbout background tasksBackground optimizationsManage awake state\nAbout async workJava threadsCoroutinesListenable future\nWork statesManage workChain work togetherSupport for long-running workersObserve intermediate worker progressUpdate work\nAbout threadingThreading in WorkerThreading in CoRoutineWorkerThreading in RxWorkerThreading in ListenableWorker\nMigrate from Firebase JobDispatcherMigrate from GCMNetworkManager\nDebug WorkManagerIntegration testingTest worker implementation\nRequest runtime permissionsRequest special permissionsExplain access to more sensitive informationApp permissions best practicesPermissions used only in default handlersRestrict interactions with other appsDefine custom permissions\nAbout app data and filesAbout storageSave to app-specific storage\nAbout shared storageMediaPhoto pickerDocuments and other filesDatasetsManage all files on a storage deviceSave key-value data\nAbout the local databaseDefine data using entitiesAccess data using DAOsDefine relationships between objectsWrite asynchronous DAO queriesCreate views into a databasePrepopulate your databaseMigrate your databaseTest and debug your databaseReference complex dataMigrate from SQLite to RoomSave data using SQLiteStorage use cases and best practices\nAbout sharing simple dataSending simple data to other appsReceiving simple data from other appsProvide Direct Share targets\nAbout sharing filesSetting up file sharingSharing a fileRequesting a shared fileRetrieving file information\nAbout sharing using NFCSending files to another deviceReceiving files from another device\nAbout printingPrinting photosPrinting HTML documentsPrinting custom documents\nAbout content providersContent provider basicsCreating a content providerOpen files using storage access frameworkCreate a custom document providerApp install location\nAbout user dataAdd sign-in workflowShow a biometric authentication dialog\nAbout autofillOptimize your app for autofillBuild autofill servicesIntegrate autofill with keyboardsIdentify developer-owned appsGet a user-resettable advertising IDAbout the calendar provider\nAbout the contacts providerRetrieving a list of contactsRetrieving details for a contactModifying contacts using intentsDisplaying the quick contact badgeAccount transfer\nAbout backupBack up user dataBack up key-value pairsTest backup and restoreBest practices for unique identifiers\nAbout authenticationRemember your userAuthenticate to OAuth2 servicesCreate a custom account type\nAbout user locationRequest location permissionsGet the last known locationChange location settingsRequest location updatesAccess location in the backgroundCreate and monitor geofencesDetect when users start an activityOptimize location for batteryTest location workflowsMigrate to location and context APIsAdd maps\nExtensions APIML Kit AnalyzerRotationsTransform outputDevicesCamera1 to CameraX migration guide\nAbout Camera2Camera capture sessions and requestsCamera lenses and capabilitiesUse multiple camera streams simultaneouslyCamera previewHDR video captureMulti-Camera APIExtensions API\nAbout Camera (deprecated)Take photos (deprecated)Record videos (deprecated)Control the camera (deprecated)Camera API (deprecated)\nIn this guideAbout sensorsMotion sensorsPosition sensorsEnvironment sensorsRaw GNSS measurements\nAbout the Cross device SDKGet startedDevice discovery APISecure connection APISessions APITest and debugAPI Reference\nAbout performing network operationsConnect to the networkManage network usageReading network stateOptimize network accessOptimize network data usageMonitor connectivity status and connection meteringParse XML data\nAbout CronetSend a simple requestCronet request lifecycleUse Cronet with other libraries\nOverviewLibraryLoaderCronetExceptionInlineExecutionProhibitedExceptionNetworkExceptionQuicExceptionUploadDataProviderUploadDataProvidersUploadDataSink\nOverviewBuilderCallbackStatusStatusListenerUrlResponseInfo\nOverviewByteArrayCronetCallbackCronetRequestCompletionListenerCronetResponseImplicitFlowControlCallbackInMemoryTransformCronetCallbackJsonCronetCallbackRedirectHandlerRedirectHandlersStringCronetCallbackUploadDataProviders\nOverviewCallbackAndResponseFuturePairEnhance your apps with 5GBuild client-server applications with gRPC\nAbout efficient downloadsOptimize downloads for efficient network accessMinimize the effect of regular updatesAvoid unoptimized downloads\nAbout preserving batteryCollecting network traffic dataAnalyzing data trafficOptimize network accessOptimize user-initiated network useOptimize app-initiated network useOptimize server-initiated network useOptimizing general network use\nAbout sync adaptersCreate a Stub AuthenticatorCreate a Stub Content ProviderCreate a Sync AdapterRun a Sync Adapter\nAbout BluetoothSet up BluetoothFind Bluetooth devicesConnect Bluetooth devicesTransfer Bluetooth dataBluetooth profilesCompanion device pairing\nAbout BLEFind BLE devicesConnect to a GATT serverTransfer BLE data\nAbout BLE AudioAudio Manager self-managed callsTelecom API managed callsAudio recording\nAbout NFCNFC basicsAdvanced NFCAbout host-based card emulation\nAbout telecomBuild a calling appPrevent caller ID spoofingTelephony IDs\nAbout Wi-Fi scanningWi-Fi peer-to-peerAbout Wi-Fi AwareWi-Fi location with RTTLocal-only hotspot\nAbout Network Service Discovery (NSD)Use network service discoveryCreate P2P connections with Wi-Fi DirectUse Wi-Fi Direct for service discoveryWi-Fi Easy Connect\nAbout Wi-Fi infrastructureWi-Fi suggestion API for internet connectivityWi-Fi Network Request API for peer-to-peer connectivityPasspointSave networks and Passpoint configurations\nAbout USBAbout accessory modeAbout USB HostUWBVPNAbout Session Initiation Protocol (SIP)Open Mobile API reader support\nAbout RenderscriptAdvanced RenderScriptMigrate from RenderScript\nAbout Runtime APINumerical typesObject typesConversion functionsMathematical constants and functionsVector math functionsMatrix functionsQuaternion functionsAtomic update functionsTime functions and typesAllocation data access functionsObject characteristics functionsKernel invocation functions and typesInput/output functionsDebugging functionsGraphics functions and typesIndex\nAbout app bundlesConfigure the base moduleBuild and test your app bundleAdd code transparencyAbout the app bundle formatFrequently asked questions\nAbout Engage SDKEngage SDK Watch integration guideEngage SDK Listen integration guideEngage SDK Read integration guideEngage SDK Shopping integration guideEngage SDK Food integration guideEngage SDK integration workflowEngage SDK Cluster publishing guidelinesEngage SDK Frequently asked questions\nAbout Play PointsCreate products and promotionsDetect and deliver productsTest products\nAbout Play Asset DeliveryIntegrate asset delivery (Kotlin & Java)Integrate asset delivery (native)Integrate asset delivery (Unity)Target texture compression formatsTest asset delivery\nAbout Play Feature DeliveryConfigure install-time deliveryConfigure conditional deliveryConfigure on-demand deliveryOn-demand delivery best practicesConfigure instant deliveryAdditional resources\nOverviewIntegrate using Kotlin or JavaIntegrate using native codeIntegrate using UnityTest in-app reviews\nAbout in-app updatesSupport in-app updates (Kotlin or Java)Support in-app updates (Native)Support in-app updates (Unity)Test in-app updates\nCreate an instant-enabled app bundleUX best practices for apps\nAbout instant gamesUnity pluginUX best practices for gamesMigrate to Android App BundlesImplement cloud delivery of assetsSupport Google Play Games ServicesInstant Play gamesInstant Play games checklistReduce the size of your instant app or gameAdd ads to your instant app or gameProvide multiple entry points\nAdd Google Analytics for Firebase to your instant appUse Firebase Dynamic Links with instant appsTechnical requirements checklistGoogle Play Instant policy\nReferenceCode samplesSDK release notesInstant App Intents\nOverviewReferenceRelease notesPlay Install Referrer APIPlay Integrity API\nPlay PoliciesTarget API LevelSupport 64-bit architectures\nOverviewLicensing OverviewSetting Up for LicensingAdding Server-Side VerificationAdding Client-Side VerificationLicensing ReferenceAPK Expansion FilesApp updates\nAbout App ActionsImplement built-in intentsCreate shortcuts.xmlPush dynamic shortcuts to AssistantRelease notes\nCustom intentsAndroid widgetsForeground app invocationInline inventoryWeb inventoryAssistant sharingRead It\nActions.xml migration guideAbout Actions.xmlBuild App ActionsCreate actions.xmlWeb inventoryApp Actions test toolAndroid SlicesTroubleshootingSupportSDK Extensions\nInstall and configure projects for AndroidSupport multiple form factors and screen sizesExport to Android\nInstall and configure projects for AndroidGodot renderer optionsSupport multiple form factors and screen sizesExport to Android\nOverviewGet started on game development with UnityCreate an Android App Bundle with UnityIntegrate Play Asset DeliveryUnity Lighting in Mobile Games\nOverviewEnable the APIIntegrate the pluginInitialize the library and verify operationDefine annotations, fidelity parameters, and quality levelsAdd loading time recording functionsInclude Addressables scenesRun the monitor appReview and publishTroubleshoot common errorsReferenceSymbolicate Android crashes and ANR for Unity gamesGet started with the Memory Advice API for Unity gamesDevelop with UnrealRequest permissions for data accessSecure your game\nAbout the GameActivity libraryGet started with GameActivityUse game text inputMigrate from NativeActiviyConfigure graphicsUnderstand Android game loops\nOverviewUse the game controller libraryUse custom controller device mappingsAdd mouse supportSupport sensor input\nIntegrateUpdate your build settingsAdd frame pacing functionsVerify frame pacing improvement\nIntegrateUpdate your build settingsAdd frame pacing functionsVerify frame pacing improvementFrame Pacing API Reference\nAbout the library wrapperGet started with library wrapper\nAbout Android Performance TunerRun the APT demo appEnable the Android Performance Parameters APIUpdate your build settingsDefine annotations, fidelity parameters, and settingsAdd frame timing functionsAdd loading time recording functionsValidate, package, and publish the APKTroubleshoot common errorsAdvanced usagePerformance Tuner API Reference\n64-bit architecturesScreen typesVulkan pre-rotation\nAbout the Oboe audio libraryUpdate build settingsCreate an audio streamSee Oboe code samplesSee the Oboe developer guide\nAbout managing memoryDebug native memory useDeliver assetsDetect and diagnose crashes\nAndroid Game Development Extension (AGDE) for Visual Studio\nAbout the AGDEGet started with AGDEConfigure your projectDebug your projectDebug memory corruption using Address SanitizerMeasure app performance\nOverviewConfigure Profile-Guided OptimizationModify build.gradle files for Android StudioSee AGDE code samplesAGDE release notes\nAbout Google Play Games on PCGet startedReview the release checklist\nSet up your game for PC compatibilityConfigure your graphics\nOverviewUpgrade from 1.0 to 1.1 Java/KotlinUpgrade from 1.0 to 1.1 UnityUpgrade from 0.0.4 to 1.0 Java/KotlinUpgrade from 0.0.4 to 1.0 Unity\nAbout continuity in cross-device playAbout continuity requirementsVerify your game's complianceFederate the identity servicesThird-party login\nTest your gameUse the developer emulatorUse ChromeOS devicesTroubleshoot the developer emulator\nPackage your game for Google Play ServicesSubmit your gameUpdate your gameIntegrity ProtectionFAQManage, debug, and profile in Android Studio\nAbout optimization toolsConfigure system tracingReduce game size\nAbout system profilingView a system profileGPU performance counters\nFrame processing timesMemory efficiencyTexture memory bandwidth usageVertex memory bandwidth usageThread scheduling\nMost expensive render passesVertex formatsShader performance\nPerformance paneCommands paneFramebuffer paneGeometry paneReport paneShader paneMemory paneState paneTextures paneTexture panePipeline view paneSupported Vulkan extensionsTroubleshoot AGIAndroid Performance Tuner (APT)Android Dynamic Performance Framework (ADPF)Optimize 3D assetsManage vertex data\nAbout the Memory Advice APIGet started with the Memory Advice API\nAbout the Game Mode API and interventionsUse the Game Mode APIUse Game Mode interventions\nAbout Google Play Games ServicesGet startedDownloadsSet up Play Games ServicesEnable Play Games Services featuresManage project settings in Google CloudPublish through Google Play Console\nSign inAchievementsLeaderboardsEventsSaved gamesFriendsNext generation Player IDs\nAbout the Google Play Games plugin for UnityGet started\nAchievementsLeaderboardsSaved gamesEventsFriendsPlayer stats\nGet startedSign inEnable server-side accessAnti-piracy\nAchievementsLeaderboardsFriendsSaved gamesEventsPlayer statsTroubleshooting\nAbout the Publishing APIGet startedUpload imagesManagement API\nQuality checklistBranding guidelinesQuota and rate limitsData disclosure requirementsTerms of serviceGet support\nAvailabilityArchitectureDeveloper functionalityGet started\nIntroductionData typesDifferential changes APIUser Privacy\nWrite dataRead raw dataRead aggregated dataDelete dataSynchronize dataWork with sessionsExceptionsBest practicesFrequently asked questionsVideo repositorySDK release notes\nHealth Connect API comparison guideFit Android API to Health Connect migration guideMigrate from Android 13 to 14\nFundamentals of testing Android appsWhat to test in AndroidUsing test doubles in Android\nSet up project for AndroidX TestJUnit4 rules with AndroidX TestAndroidJUnitRunner\nEspressoEspresso basicsEspresso setup instructionsEspresso cheat sheetEspresso idling resourcesEspresso-IntentsEspresso listsMultiprocess EspressoEspresso recipesEspresso WebAccessibility checkingAdditional Resources for Espresso\nTest content providersTest your serviceWrite automated tests with UI AutomatorPerformance\nMinimize your permission requestsReset unused permissions\nUse coarse location accuracyAccess location in the background only when necessaryAccess nearby Bluetooth devicesAccess nearby Wi-Fi devices\nDeclare package visibility needsWork with user-resettable identifiersSupport scoped storage\nWorkflow for requesting permissionsProvide prominent disclosure and consentExplain why your app needs permissionsAudit access to dataHandle permission denialsReview how your app collects and shares user data\nIndicatorsPasting an intentPrivacy DashboardApp hibernation\nChoose a developer programEnroll your platform with the Privacy SandboxSet up your development environmentConfigure AdServicesSet up a device or emulator imageConfigure devices to use Privacy Sandbox on Android\nOverview of Protected AudienceProtected Audience mediationProtected Audience frequency cappingProtected Audience app install ads filtering\nOverview and app measurementCross app and web measurementMeasurement Simulation Library\nDeveloper's guideProfile a Protected Audience auctionAttribution Reporting\nSDK Runtime APITopicsProtected Audience on AndroidAttribution Reporting APIAPI referenceSamples\nAbout security on AndroidApp security best practicesApp security improvement program\nCommon risksandroid:debuggableandroid:exportedContent resolversExposed directories to FileProviderIntent RedirectionHardcoded Cryptographic SecretsLog info disclosurePath traversalPending intentsSticky BroadcastSQL injectionTapjackingWeak PRNGZip Path Traversal\nAbout SafetyNetPlay Integrity APISafetyNet Safe Browsing APISafetyNet reCAPTCHA APISafetyNet Verify Apps APIVerifying hardware-backed key pairs with key attestationSafetyNet Attestation APIDiscontinuing SafetyNet Attestation\nWork with data more securelyCryptographyAndroid Keystore SystemRunning embedded DEX code directly from APK\nSecurity with network protocolsNetwork security configurationUpdate your security provider to protect against SSL exploitsAndroid Protected ConfirmationSafetyNet Safe Browsing APIMinimize use of optimized but unverified codePerform actions before initial device unlock\nAbout enterprise appsDeveloper guideWork profilesSet up managed configurations\nSend app feedback to EMMsTest app feedbackWork contactsDevice management policies\nOverviewLock task modeMultiple usersCookbookDevice controlNetworking and telephonySecuritySystem updatesNetwork activity logging\nAbout the versionsAndroid 13Android 12Android 11Android 10Android 9Android 8.0Android 7.0Device administration\nWarning! The SafetyNet Attestation API is deprecated and has been replaced by the Play Integrity API. Learn more.\nSave and categorize content based on your preferences.\nThe SafetyNet Attestation API is an anti-abuse API that allows app developers to\nassess the Android device their app is running on. The API should be used as a\npart of your abuse detection system to help determine whether your servers are\ninteracting with your genuine app running on a genuine Android device.\nThe SafetyNet Attestation API provides a cryptographically-signed attestation,\nassessing the device's integrity. In order to create the attestation, the API\nexamines the device's software and hardware environment, looking for integrity\nissues, and comparing it with the reference data for approved Android devices.\nThe generated attestation is bound to the nonce that the caller app provides.\nThe attestation also contains a generation timestamp and metadata about the\nThe API is not designed to fulfill the following use cases:\nAct as a stand-alone anti-abuse or app-security mechanism. Please use it in\ncombination with the published best practices for app\nFunction when the device isn't connected to the internet. The API returns an\nHave its response interpreted directly in the calling app. Move all\nanti-abuse decision logic to a server under your control.\nProvide fine-grained signals about system modifications. The API offers\nboolean values that express different levels of system integrity.\nContain signals for app-specific use-cases, such as device identifiers, GPS\nPurely to check whether the device is rooted, as the API is designed to check\nThe SafetyNet Attestation API uses the following workflow:\nThe SafetyNet Attestation API receives a call from your app. This call\nThe SafetyNet Attestation service evaluates the runtime environment and\nrequests a signed attestation of the assessment results from Google's servers.\nGoogle's servers send the signed attestation to the SafetyNet Attestation\nThe SafetyNet Attestation service returns this signed attestation to your\nYour app forwards the signed attestation to your server.\nThis server validates the response and uses it for anti-abuse decisions. Your\nA graphical depiction of this process appears in Figure 1:\nNote: To see an Android app that contains a complete implementation of the\nThroughout the initialization, configuration and activation of the\nSafetyNet Attestation API and in addition to this main documentation, be aware\npost: 10 things you might be doing wrong when using the SafetyNet\nIn order to call the methods of the SafetyNet Attestation API, you must use an\nAPI key. Because the SafetyNet Attestation API is deprecated, new keys are only\ngranted for exceptional cases. You can request a key using\nOnce granted access to create a key, complete the following steps:\nSearch for, and select, Android Device Verification (DEPRECATED). The\nAndroid Device Verification dashboard screen appears.\nIf the Create credentials button appears, click on it to generate an API\nkey. Otherwise, click the All API credentials drop-down list, then select\nthe API key that's associated with your project that has enabled the Android\nIn the sidebar on the left, click Credentials. Copy the API key that\nAfter creating this API key, join the SafetyNet API clients mailing\nThe default quota allotment per project for calling the SafetyNet Attestation\nAPI is 10,000 requests per day across your user base. To make a higher volume of\nincreased quota based on the instructions in the Play Integrity API\ndocumentation. Note that quota requests take a few business days to be\nproject, individual app instances are throttled to a maximum of 5 requests per\nminute. If this limit is exceeded, all remaining requests during that\nKeep this behavior in mind when implementing your app's retry\nBefore you use the SafetyNet Attestation API, you must ensure that the correct\nversion of Google Play services is installed on the user's device. If an\nincorrect version is installed, your app might stop responding after calling the\nAPI. If your app detects that an incorrect version is installed, you should ask\nTo check whether the installed version of Google Play services is compatible\nwith the version of the Android SDK you're using, call the\nif (GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(context)\nOn devices running Google Play Services v13.0 and above, the SafetyNet\nAttestation API also supports app-restricted API keys. This feature reduces the\nrisk of accidental or unauthorized usage of quota-restricted API keys. To use\nthis optional feature, check that the minimum version of Google Play Services on\nthe device is at least v13.0, as shown in the following code snippet:\n.isGooglePlayServicesAvailable(context, 13000000) ==\nAfter you obtain an API key that is valid for the Android Device\nVerification API in the Google APIs Console, your app can use the SafetyNet\nAttestation service. To do so, complete the following steps:\nUse the response on your server, along with your other anti-abuse signals, to\nTo keep your app responsive, execute these steps outside of your app's main\nexecution thread. To learn more about how to create separate execution threads,\nNote: The SafetyNet Attestation API uses network resources, so the latency\nbetween a request and its corresponding response varies depending on the\nYou should perform this check to protect all critical actionsincluding logins,\npurchase events and acquisition of new in-app productsin your app. Calls to the\nSafetyNet Attestation API incur increased latency, mobile data usage, and\nbattery usage, however, so it makes sense to find a balance between security and\nusability. As an example, you might choose to request a SafetyNet attestation\nupon login and run re-checks at most once every 30 minutes. You can also let\nyour server decide when your app requests an attestation, to make it harder for\nWhen calling the SafetyNet Attestation API, you must pass in a nonce. The\nresulting attestation contains this nonce, allowing you to determine that the\nattestation belongs to your API call and isn't replayed by an attacker.\nA nonce used with a SafetyNet request should be at least 16 bytes in length. You\nshould introduce variability in your nonce, ensuring that the same nonce is\nnever used twice. As a best practice, derive part of the nonce from the data\nbeing sent to your servers. For example, concatenate the hash of the username\nImportant: Include as many pieces of data in the nonce as possible. In\ndoing so, you make it more difficult for attackers to carry out replay\nattacks. For example, deriving the nonce from the username limits replay\nattacks to the same account. However, deriving the nonce from all the details\nof a purchase event limits the use of the API's response message to that\nUpon receiving the signed response from the API, always compare the nonce in the\nsigned response with the one you reconstruct from the rest of the message sent\nto your servers. This check ensures that attackers cannot reuse signed\nattestations harvested from good devices for other, maliciously-crafted\nFor additional information on using cryptography functions, see the guide on how\nAfter you have established a connection to Google Play services and have created\na nonce, you're ready to make a SafetyNet attestation request. The response to\nyour request may not be immediate, so it's best to set up a callback listener to\nhandle the response from the service. An example listener appears in the\n// Indicates communication with the service was successful.\n// Use response.getJwsResult() to get the result data.\n// An error occurred while communicating with the service.\n// An error with the Google Play services API contains some\n// The nonce should be at least 16 bytes in length.\n// You must generate the value of API_KEY in the Google APIs dashboard.\nnew OnSuccessListener<SafetyNetApi.AttestationResponse>() {\npublic void onSuccess(SafetyNetApi.AttestationResponse response) {\n// Indicates communication with the service was successful.\n// Use response.getJwsResult() to get the result data.\n.addOnFailureListener(this, new OnFailureListener() {\n// An error occurred while communicating with the service.\n// An error with the Google Play services API contains some\nmethod indicates that communication with the service was successful, but it\ndoesn't indicate whether the device has passed the SafetyNet attestation. The\nnext section discusses how to read the attestation result and verify its\nNote: The preceding code snippet uses a non-blocking version of the attest()\nmethod. If you're making a blocking call instead, always set a response timeout.\nThat way, your app remains responsive, even if a slow network connection causes\na delayed response from the SafetyNet Attestation API.\nTransfer the SafetyNet attestation response to your server\nWhen your app communicates with SafetyNet, the service provides a response\ncontaining the result of the SafetyNet attestation and includes additional\ninformation to help you verify the integrity of the message. The result is\nmethod to obtain the data of the request. The response is formatted as a JSON\nSend the JWS object back to your server for validation and use.\nCaution: You should send the entire JWS response to your own server, using a\nsecure connection, for verification. We don't recommend that you perform the\nverification directly in your app. By verifying directly in your app, you don't\nhave any protection against attackers who may remove the verification logic from\nthe modified version of your app, or may obstruct it from working in\nUse the SafetyNet attestation response on your server\nThe following JWS excerpt shows the format and sample contents of the payload\n\"apkPackageName\": \"com.package.name.of.requesting.app\",\n\"apkCertificateDigestSha256\": [\"base64 encoded, SHA-256 hash of the\nA signed attestation's payload typically contains the following fields:\ntimestampMs: Milliseconds past the UNIX epoch when the JWS response message\nnonce: The single-use token that the calling app passes to the API.\napkCertificateDigestSha256: Base-64 encoded representation(s) of the SHA-256\nctsProfileMatch: A stricter verdict of device integrity. If the value of\nctsProfileMatch is true, then the profile of the device running your app\nmatches the profile of a device that has passed Android compatibility testing\nand has been approved as a Google-certified Android device.\nbasicIntegrity: A more lenient verdict of device integrity. If only the\nvalue of basicIntegrity is true, then the device running your app likely\nwasn't tampered with. However, the device hasn't necessarily passed Android\nFor more information about Android compatibility testing, see Design an\nNote: If you don't need the device running your app to pass CTS, use the\nbasicIntegrity parameter, as it allows your app to run on a wider variety\nof devices. If your app requires stronger device integrity guarantees, you\nshould use only the result from ctsProfileMatch. For more information, see\nthe potential integrity verdicts section.Caution: You should trust the APK information only if the value of\ndeprecationInformation: A string which contains information for developers\nabout the deprecation of the SafetyNet Attestation API.\nerror: Encoded error information relevant to the current API request.\nadvice: A suggestion for how to get a device back into a good state.\nevaluationType: Types of measurements that contributed to the current API\nThe JWS message contains two parameters that indicate the result of the check\nfor device compatibility: ctsProfileMatch and basicIntegrity. The status of\nthe device running your app could affect the value for each parameter, as shown\nTable 1. Examples of how device status could affect the values of\nGenuine but uncertified device, such as when the manufacturer doesn't\nSigns of system integrity compromise, one of which may be rooting\nThe JWS message can also show several types of error conditions:\nA null result indicates that the call to the service didn't complete\nAn error parameter in the JWS indicates that an issue occurred, such as a\nnetwork error or an error that an attacker feigned. Most errors are transient\nand should be absent if you make another call to the service. You might want to\nretry a few more times with increasing delays between each retry.\nIf the device is tamperedthat is, if basicIntegrity is set to false in the\nresponsethe verdict might not contain data about the calling app, such as the\napkPackageName and apkCertificateDigestSha256. This occurs when our systems\nWhat to do when the signed attestation reports an error?\nRetry. Errors on legitimate devices are temporary and should go away if\nCheck that your app doesn't call the API more than 5 times per minute on\nthe affected devices and that your project's API quota hasn't been exhausted\nAssume that it might be an attacker intentionally triggering an error case\nWhen present, the advice parameter provides information to help explain why\nthe SafetyNet Attestation API set either ctsProfileMatch or basicIntegrity\nto false in a particular result. The parameter's value contains a list of\nstrings, such as the ones in the following example:\n{\"advice\": \"LOCK_BOOTLOADER,RESTORE_TO_FACTORY_ROM\"}\nIn your app, you can translate the values in the advice parameter into\nuser-friendly messages to help the user pass future SafetyNet attestations, as\nThe user should restore their device to a clean factory ROM.\nThe evaluationType parameter is present whenever the ctsProfileMatch and\nThe parameter provides information about the types of measurements used to\ncompute fields like ctsProfileMatch and basicIntegrity for a particular response.\nThe parameters value contains a list of strings, like in the following example:\nHardware-backed security features were used. This includes features such as\non devices that shipped with Android 8.0 (API level 26) and higher.\nIn your app, you can treat the presence of HARDWARE_BACKED in the\nevaluationType parameter as an indicator of a stronger device integrity\nrecommended only for apps that already use the ctsProfileMatch\nverdict and which require the highest level of device integrity guarantees,\neven at the cost of limiting their user base. In most cases, you should\nctsProfileMatch verdicts to detect abuse. These verdicts already\nincorporate hardware-backed security features, where applicable.\nIf you decide to depend on the presence of a certain value in the\nevaluationType parameter, then you should consider implementing a retry\nmechanism in your app in case there are temporary errors.\nYou should take steps to make sure that the SafetyNet attestation response\nactually came from the SafetyNet service and includes data matching your\nTo verify the origin of the JWS message, complete the following steps:\nExtract the SSL certificate chain from the JWS message.\nValidate the SSL certificate chain and use SSL hostname matching to verify\nthat the leaf certificate was issued to the hostname attest.android.com.\nUse the certificate to verify the signature of the JWS message.\nCheck the data of the JWS message to make sure it matches the data within\nyour original request. In particular, make sure that the timestamp has been\nvalidated and that the nonce, package name, and hashes of the app's signing\nYou should verify the JWS statement using standard cryptographic solutions, such\nas the ones found in the android-play-safetynet sample API\nDuring initial testing and development (but not in production), you can call an\nonline API for verifying the signature of the JWS statement. This process has\nalso been shown in the android-play-safetynet sample\nusage made available on GitHub. Note that online verification API is solely for\nearly-stage testing, and you have a fixed quota of 10,000 requests per day.\nImportant: The use of the online verification API only validates that\nthe JWS message was signed by the SafetyNet Attestation API's servers. This\nonline API cannot verify whether the fields in the payload match the values\nWe recommend planning your usage so that it takes changes and outages into\nNew (experimental) fields may appear in the verdict any time. Make sure these\nextra fields don't break your parser or usage logic. In particular, don't rely\non experimental fields before they are announced on the SafetyNet API clients\nIn the unlikely event of the SafetyNet Attestation API being unavailable,\nusers of this API are strongly recommended to build server-side capabilities\nto dynamically control the dependence on the availability as well as\nTypical strategies should include the ability to dynamically instruct your apps\nto stop calling this API, as well as device- and user-based allowlists to\nignore the SafetyNet Attestation API results for certain classes of devices and\nFor additional guidance on working with the SafetyNet APIs, view the sample\nWe strongly recommend joining the SafetyNet API clients mailing\nfeedback to prioritize new features and capabilities for this API.\nTo learn more about the best-practices when using the SafetyNet Attestation API,\nBlog post: 10 things you might be doing wrong when using the SafetyNet\nBy accessing or using the SafetyNet APIs, you agree to the Google APIs Terms of\nPlease read and understand all applicable terms and policies before accessing\nAs with any data collected in large volume from in-the-field observation, there\nis a chance of both false positives and false negatives. We are presenting the\ndata to the best of our understanding. We extensively test our detection\nmechanisms to ensure accuracy, and we are committed to improving those methods\nover time to ensure they continue to remain accurate.\nYou agree to comply with all applicable law, regulation, and third party rights\n(including without limitation laws regarding the import or export of data or\nsoftware, privacy, and local laws). You will not use the APIs to encourage or\npromote illegal activity or violation of third party rights. You will not\nviolate any other terms of service with Google (or its affiliates).\nYou acknowledge and understand that the SafetyNet API works by collecting\nhardware and software information, such as device and application data and the\nresults of SafetyNet attestations, and sending that data to Google for analysis.\nPursuant to Section 3(d) of the Google APIs Terms of\nAPIs that it is your responsibility to provide any necessary notices or consents\nfor the collection and sharing of this data with Google.\nfor developers to disclose their apps' data collection, sharing, and security\npractices. To help you complete the data safety section requirements, you can\nuse the following information regarding how the SafetyNet Attestation API\nHow the SafetyNet Attestation API applies the practice\nA device attestation token generated by Google Play services\nThe data collected is used to verify the application integrity and the device integrity.\nData is deleted following a fixed retention period.\nTo complete your data disclosure, you can use Android's\nwhich data types best describe the collected data. In your data disclosure,\nmake sure to also account for how your specific app shares and uses the\nWhile we aim to be as transparent as possible in supporting you, we can't speak\nfor you, and you are solely responsible for deciding how to respond to Google\nPlay's data safety section form regarding your app's user data collection,\nContent and code samples on this page are subject to the licenses described in the Content License. Java and OpenJDK are trademarks or registered trademarks of Oracle and/or its affiliates.\nConnect with the Android Developers community on LinkedIn",
    "Verifying hardware-backed key pairs with Key Attestation | Android Developers\nGet one of our Figma kits for Android or Material Design and start designing your app's UI today.\nDesign a beautiful user interface using Android best practices.\nDesign robust, testable, and maintainable app logic and services.\nPlan for app quality and align with Play store guidelines.\nLocalize your appTest your app with pseudolocalesUnicode and internationalization supportLanguage and locale resolutionPer-app language preferencesComplex XML resources\nAbout resource typesAnimationColor state listDrawableLayoutMenuStringStyleFontMore types\nAbout app manifests<action><activity><activity-alias><application><category><compatible-screens><data><grant-uri-permission><instrumentation><intent-filter><layout><manifest><meta-data><path-permission><permission><permission-group><permission-tree><profileable><property><provider><queries><receiver><service><supports-gl-texture><supports-screens><uses-configuration><uses-feature><uses-library><uses-native-library><uses-permission><uses-permission-sdk-23><uses-sdk>\nAbout device compatibilityAbout screen compatibilityEnhanced letterboxingSupport different pixel densitiesDeclare restricted screen support\nAbout multiple APKsCreate multiple APKs for different API levelsCreate multiple APKs for different screen sizesCreate multiple APKs for different GL texturesCreate multiple APKs with several dimensionsSupport different languages and culturesSupport different platform versionsFilters on Google Play64-bit Support\nGet started with large screensLarge screen canonical layouts\nMulti-window supportMedia projectionInput compatibility on large screens\nSupport different screen sizesMigrate your UI to responsive layoutsNavigation for responsive UIsActivity embedding\nLearn about foldablesMake your app fold awareTest your app on foldablesAdvanced stylusLarge screen cookbook\nGet startedPrinciples of Wear OS developmentUpgrade to the latest version of Wear OSTry out the latest version of Wear OS Developer PreviewWear OS versus mobile developmentWear OS user interfacesAccessibility on Wear OS\nWear OS developer pathwayCreate and run a wearable appDebug a Wear OS appCapture Wear UI screenshots\nAbout Wear OS appsStandalone appsAuthenticationRequest permissionsAdd a splash screenDetect locationPlaying audio on wearablesAppear in recents and app resume\nUse Jetpack Compose on Wear OSCompose performanceRotary input on ComposeNavigation with Compose for Wear OS\nBuild View-based UIs on Wear OSHandle different watch shapesCreate listsNavigationExit full screen activities on WearShow confirmationsKeep your app visible on Wear\nAbout tilesGet started with tilesUpdate tilesInteract with tilesWatch face complicationsExpose data to complications\nNotifications on Wear OSBridging options for notificationsOngoing Activities\nSend and sync data on Wear OSNetwork access and sync on Wear OSAccess the Wearable Data LayerTransfer assetsSend and receive messagesHandle data layer eventsSync data items with the Data Layer API\nPhysical buttonsRotary input on ViewsCreate input method editors in WearVoice\nAbout health servicesActive data and exercisePassive data updatesUse synthetic data providersEnhance app compatibility\nDesign watch facesBuild a watch face serviceDraw watch facesAdding complications to a watch faceCreating interactive watch facesProvide configuration activitiesAddress common issuesImprove performace with hardware accelerationOptimize performance and battery lifeSelf-tag watch facesWear app qualityPackage Wear OS appsDistribute to Wear OSCreate Wear OS apps for ChinaWear OS release notes\nIn this guideGet started with TV appsAndroidX TV librariesHandle TV hardwareManage TV controllersBuild TV layoutsOn-screen keyboardCreate TV navigationBest practices for driving engagement on Google TV\nUse Jetpack Compose on Android TVCreate a catalog browserBuild a details screen\nCreate a catalog browserProvide a card viewBuild a details viewUse transport controlsIntroduce first-time users to your appAdd a guided stepAmbient modePlayback controls on TVImplement a media sessionBackground playback in a Now Playing cardAudio capabilitiesMatch content frame rate\nAbout recommending TV contentChannels on the home screenVideo program attributesAudio program attributesGame program attributes\nAdd programsAttributesGuidelines for app developersGuidelines for TV providersPreview videosRecommendations in Android N and earlierMake TV apps searchableSearch within TV Apps\nAbout TV gamesUse Stream Protect for latency-sensitive streaming apps\nAbout TV input servicesDevelop a TV input serviceWork with channel dataManage TV user interactionSupport time-shiftingSupport content recording\nAccessibility best practicesTalkBack evaluation examplesAdopt system caption settingsCustom view accessibility supportCustom view accessibility sampleTV Apps checklistDistribute to Android TV\nBuild media apps for carsAdd support for Android AutoAdd support for Android Automotive OSBuild messaging apps for Android Auto\nBuild point of interest, internet of things, and navigation apps for cars\nUsing the Android for Cars App LibraryBuild point of interest apps for carsBuild internet of things apps for carsBuild navigation apps for carsAdd support for Android AutoAdd support for Android Automotive OS\nAbout parked appsBuild video appsBuild gamesTest Android apps for carsDistribute Android apps for carsGoogle Play services for carsNotifications on Android Automotive OS\nAbout ChromeOSBuilding apps for ChromeOSOptimizing Apps for ChromeOSPreparing your development environmentApp Manifest Compatibility for ChromebooksChromeOS Device Support for AppsApp Rendering Differences on ChromebooksWindow managementAdapting Games on ChromeOSSmooth animation on ChromeOSTest Cases for Android Apps on ChromeOS\nAbout the UI layerUI eventsState holders and UI stateState productionDomain layer\nAbout the data layerOffline firstArchitecture recommendationsLearning pathway\nAbout view bindingMigrate from Kotlin synthetics to view binding\nAbout data bindingGet startedLayouts and binding expressionsWork with observable data objectsGenerated binding classesBinding adaptersBind layout views to Architecture ComponentsTwo-way data binding\nAbout ViewModelCreate ViewModels with dependenciesViewModel Scoping APIsSaved State module for ViewModelViewModel APIs cheat sheetLiveDataSave UI statesUse Kotlin coroutines with lifecycle-aware components\nAbout pagingLoad and display paged dataPage from network and databaseTransform data streamsManage and present loading statesTest your Paging implementationMigrate to Paging 3\nDefining your WorkRequestsWork statesManaging workObserving intermediate Worker progressChaining work togetherTesting Worker implementationIntegration tests with WorkManagerDebugging WorkManager\nAbout threading in WorkManagerThreading in WorkerThreading in CoroutineWorkerThreading in RxWorkerThreading in ListenableWorkerSupport for long-running workersMigrating from Firebase JobDispatcherMigrating from GCMNetworkManager\nIntroduction to activitiesThe activity lifecycleActivity state changesTest your app's activitiesTasks and the back stackProcesses and app lifecycleParcelables and bundlesLoadersRecents screenRestrictions on starting activities from the background\nAbout app shortcutsCreate shortcutsAdd capabilitiesManage shortcutsBest practices for shortcuts\nPrinciples of navigationDesign for different form factorsHandle configuration changes\nAbout the navigation componentGetting startedCreate destinationsDesign navigation graphsNested graphsGlobal actionsNavigate to a destinationSupport multiple back stacksConditional navigationPass data between destinationsCreate a deep link for a destinationAnimate transitions between destinationsUpdate UI components with NavigationUIKotlin DSLType safe navigation with ComposeInteract programmaticallyNavigate with feature modulesBest practices for multi-module projectsTest navigationAdd new destination typesMigrate to the Navigation component\nAbout fragmentsCreate a fragmentFragment managerFragment transactionsAnimate transitions between fragmentsFragment lifecycleSaving state with fragmentsCommunicate with fragmentsWorking with the app barDisplaying dialogs with DialogFragmentDebug your fragmentsTest your fragments\nAbout app linksEnabling links to app contentVerify app linksCreate app links for instant appsCreate swipe views with tabs using ViewPagerCreate swipe views with tabs using ViewPager2\nAbout dependency injectionManual dependency injectionDependency injection with HiltHilt in multi-module appsUse Hilt with other Jetpack librariesHilt testing guideHilt and Dagger annotations cheat sheet\nDagger basicsUsing Dagger in Android appsUsing Dagger in multi-module appsApp Startup\nAbout backward-compatible UIsAbstracting the new APIsProxying to the new APIsCreating an implementation with older APIsUsing the version-aware component\nAbout app compatibilityCompatibility framework toolsRestrictions on non-SDK interfaces\nAbout interacting with other appsSending the user to another appGetting a result from an activityAllowing other apps to start your activity\nAbout package visibilityKnow which packages are visible automaticallyDeclare package visibility needsFulfill common use casesTest package visibility\nUse a media session to manage playbackPlay media in the background\nHello worldPlayer eventsPlaylistsMedia itemsMedia sourcesTrack selectionUI componentsDownloading mediaAd insertionRetrieving metadataLive streamingNetwork stacksDebug loggingAnalytics\nDigital rights managementTroubleshootingCustomizationBattery consumptionAPK shrinkingOEM testing\nMigration guideExoPlayer to Media3 mappingsPros and consDemo applicationSupported formatsSupported devicesGlossaryRelease notesJavadocGitHubBlog\nHello worldDemo applicationTransformationsSupported formats\nCustomizationTroubleshootingRelease notesJavadocGitHubExoPlayer BlogSpatial AudioHDR video playback\nAbout audio appsBuilding a media browser serviceBuilding a media browser clientMedia session callbacksUsing the media controller test app\nAbout video appsBuilding a video player activityMedia session callbacksCompatible media transcodingResponding to media buttonsHandling changes in audio outputManage audio focusMedia controls\nThe Google Assistant and media appsMedia apps on Google Assistant driving mode\nAbout routingAbout MediaRouterAbout MediaRouteProvider\nSupported media formatsUltra HDR Image Format SpecificationMedia codecsAbout MediaPlayerAbout MediaRecorderControl amplitude with VolumeShaperSharing audio inputCapture video and audio playbackFrame rateBest practices for sharing videoAdditional resources for media\nAbout servicesForeground servicesBound servicesAbout AIDL\nAbout background tasksBackground optimizationsManage awake state\nAbout async workJava threadsCoroutinesListenable future\nWork statesManage workChain work togetherSupport for long-running workersObserve intermediate worker progressUpdate work\nAbout threadingThreading in WorkerThreading in CoRoutineWorkerThreading in RxWorkerThreading in ListenableWorker\nMigrate from Firebase JobDispatcherMigrate from GCMNetworkManager\nDebug WorkManagerIntegration testingTest worker implementation\nRequest runtime permissionsRequest special permissionsExplain access to more sensitive informationApp permissions best practicesPermissions used only in default handlersRestrict interactions with other appsDefine custom permissions\nAbout app data and filesAbout storageSave to app-specific storage\nAbout shared storageMediaPhoto pickerDocuments and other filesDatasetsManage all files on a storage deviceSave key-value data\nAbout the local databaseDefine data using entitiesAccess data using DAOsDefine relationships between objectsWrite asynchronous DAO queriesCreate views into a databasePrepopulate your databaseMigrate your databaseTest and debug your databaseReference complex dataMigrate from SQLite to RoomSave data using SQLiteStorage use cases and best practices\nAbout sharing simple dataSending simple data to other appsReceiving simple data from other appsProvide Direct Share targets\nAbout sharing filesSetting up file sharingSharing a fileRequesting a shared fileRetrieving file information\nAbout sharing using NFCSending files to another deviceReceiving files from another device\nAbout printingPrinting photosPrinting HTML documentsPrinting custom documents\nAbout content providersContent provider basicsCreating a content providerOpen files using storage access frameworkCreate a custom document providerApp install location\nAbout user dataAdd sign-in workflowShow a biometric authentication dialog\nAbout autofillOptimize your app for autofillBuild autofill servicesIntegrate autofill with keyboardsIdentify developer-owned appsGet a user-resettable advertising IDAbout the calendar provider\nAbout the contacts providerRetrieving a list of contactsRetrieving details for a contactModifying contacts using intentsDisplaying the quick contact badgeAccount transfer\nAbout backupBack up user dataBack up key-value pairsTest backup and restoreBest practices for unique identifiers\nAbout authenticationRemember your userAuthenticate to OAuth2 servicesCreate a custom account type\nAbout user locationRequest location permissionsGet the last known locationChange location settingsRequest location updatesAccess location in the backgroundCreate and monitor geofencesDetect when users start an activityOptimize location for batteryTest location workflowsMigrate to location and context APIsAdd maps\nExtensions APIML Kit AnalyzerRotationsTransform outputDevicesCamera1 to CameraX migration guide\nAbout Camera2Camera capture sessions and requestsCamera lenses and capabilitiesUse multiple camera streams simultaneouslyCamera previewHDR video captureMulti-Camera APIExtensions API\nAbout Camera (deprecated)Take photos (deprecated)Record videos (deprecated)Control the camera (deprecated)Camera API (deprecated)\nIn this guideAbout sensorsMotion sensorsPosition sensorsEnvironment sensorsRaw GNSS measurements\nAbout the Cross device SDKGet startedDevice discovery APISecure connection APISessions APITest and debugAPI Reference\nAbout performing network operationsConnect to the networkManage network usageReading network stateOptimize network accessOptimize network data usageMonitor connectivity status and connection meteringParse XML data\nAbout CronetSend a simple requestCronet request lifecycleUse Cronet with other libraries\nOverviewLibraryLoaderCronetExceptionInlineExecutionProhibitedExceptionNetworkExceptionQuicExceptionUploadDataProviderUploadDataProvidersUploadDataSink\nOverviewBuilderCallbackStatusStatusListenerUrlResponseInfo\nOverviewByteArrayCronetCallbackCronetRequestCompletionListenerCronetResponseImplicitFlowControlCallbackInMemoryTransformCronetCallbackJsonCronetCallbackRedirectHandlerRedirectHandlersStringCronetCallbackUploadDataProviders\nOverviewCallbackAndResponseFuturePairEnhance your apps with 5GBuild client-server applications with gRPC\nAbout efficient downloadsOptimize downloads for efficient network accessMinimize the effect of regular updatesAvoid unoptimized downloads\nAbout preserving batteryCollecting network traffic dataAnalyzing data trafficOptimize network accessOptimize user-initiated network useOptimize app-initiated network useOptimize server-initiated network useOptimizing general network use\nAbout sync adaptersCreate a Stub AuthenticatorCreate a Stub Content ProviderCreate a Sync AdapterRun a Sync Adapter\nAbout BluetoothSet up BluetoothFind Bluetooth devicesConnect Bluetooth devicesTransfer Bluetooth dataBluetooth profilesCompanion device pairing\nAbout BLEFind BLE devicesConnect to a GATT serverTransfer BLE data\nAbout BLE AudioAudio Manager self-managed callsTelecom API managed callsAudio recording\nAbout NFCNFC basicsAdvanced NFCAbout host-based card emulation\nAbout telecomBuild a calling appPrevent caller ID spoofingTelephony IDs\nAbout Wi-Fi scanningWi-Fi peer-to-peerAbout Wi-Fi AwareWi-Fi location with RTTLocal-only hotspot\nAbout Network Service Discovery (NSD)Use network service discoveryCreate P2P connections with Wi-Fi DirectUse Wi-Fi Direct for service discoveryWi-Fi Easy Connect\nAbout Wi-Fi infrastructureWi-Fi suggestion API for internet connectivityWi-Fi Network Request API for peer-to-peer connectivityPasspointSave networks and Passpoint configurations\nAbout USBAbout accessory modeAbout USB HostUWBVPNAbout Session Initiation Protocol (SIP)Open Mobile API reader support\nAbout RenderscriptAdvanced RenderScriptMigrate from RenderScript\nAbout Runtime APINumerical typesObject typesConversion functionsMathematical constants and functionsVector math functionsMatrix functionsQuaternion functionsAtomic update functionsTime functions and typesAllocation data access functionsObject characteristics functionsKernel invocation functions and typesInput/output functionsDebugging functionsGraphics functions and typesIndex\nAbout app bundlesConfigure the base moduleBuild and test your app bundleAdd code transparencyAbout the app bundle formatFrequently asked questions\nAbout Engage SDKEngage SDK Watch integration guideEngage SDK Listen integration guideEngage SDK Read integration guideEngage SDK Shopping integration guideEngage SDK Food integration guideEngage SDK integration workflowEngage SDK Cluster publishing guidelinesEngage SDK Frequently asked questions\nAbout Play PointsCreate products and promotionsDetect and deliver productsTest products\nAbout Play Asset DeliveryIntegrate asset delivery (Kotlin & Java)Integrate asset delivery (native)Integrate asset delivery (Unity)Target texture compression formatsTest asset delivery\nAbout Play Feature DeliveryConfigure install-time deliveryConfigure conditional deliveryConfigure on-demand deliveryOn-demand delivery best practicesConfigure instant deliveryAdditional resources\nOverviewIntegrate using Kotlin or JavaIntegrate using native codeIntegrate using UnityTest in-app reviews\nAbout in-app updatesSupport in-app updates (Kotlin or Java)Support in-app updates (Native)Support in-app updates (Unity)Test in-app updates\nCreate an instant-enabled app bundleUX best practices for apps\nAbout instant gamesUnity pluginUX best practices for gamesMigrate to Android App BundlesImplement cloud delivery of assetsSupport Google Play Games ServicesInstant Play gamesInstant Play games checklistReduce the size of your instant app or gameAdd ads to your instant app or gameProvide multiple entry points\nAdd Google Analytics for Firebase to your instant appUse Firebase Dynamic Links with instant appsTechnical requirements checklistGoogle Play Instant policy\nReferenceCode samplesSDK release notesInstant App Intents\nOverviewReferenceRelease notesPlay Install Referrer APIPlay Integrity API\nPlay PoliciesTarget API LevelSupport 64-bit architectures\nOverviewLicensing OverviewSetting Up for LicensingAdding Server-Side VerificationAdding Client-Side VerificationLicensing ReferenceAPK Expansion FilesApp updates\nAbout App ActionsImplement built-in intentsCreate shortcuts.xmlPush dynamic shortcuts to AssistantRelease notes\nCustom intentsAndroid widgetsForeground app invocationInline inventoryWeb inventoryAssistant sharingRead It\nActions.xml migration guideAbout Actions.xmlBuild App ActionsCreate actions.xmlWeb inventoryApp Actions test toolAndroid SlicesTroubleshootingSupportSDK Extensions\nInstall and configure projects for AndroidSupport multiple form factors and screen sizesExport to Android\nInstall and configure projects for AndroidGodot renderer optionsSupport multiple form factors and screen sizesExport to Android\nOverviewGet started on game development with UnityCreate an Android App Bundle with UnityIntegrate Play Asset DeliveryUnity Lighting in Mobile Games\nOverviewEnable the APIIntegrate the pluginInitialize the library and verify operationDefine annotations, fidelity parameters, and quality levelsAdd loading time recording functionsInclude Addressables scenesRun the monitor appReview and publishTroubleshoot common errorsReferenceSymbolicate Android crashes and ANR for Unity gamesGet started with the Memory Advice API for Unity gamesDevelop with UnrealRequest permissions for data accessSecure your game\nAbout the GameActivity libraryGet started with GameActivityUse game text inputMigrate from NativeActiviyConfigure graphicsUnderstand Android game loops\nOverviewUse the game controller libraryUse custom controller device mappingsAdd mouse supportSupport sensor input\nIntegrateUpdate your build settingsAdd frame pacing functionsVerify frame pacing improvement\nIntegrateUpdate your build settingsAdd frame pacing functionsVerify frame pacing improvementFrame Pacing API Reference\nAbout the library wrapperGet started with library wrapper\nAbout Android Performance TunerRun the APT demo appEnable the Android Performance Parameters APIUpdate your build settingsDefine annotations, fidelity parameters, and settingsAdd frame timing functionsAdd loading time recording functionsValidate, package, and publish the APKTroubleshoot common errorsAdvanced usagePerformance Tuner API Reference\n64-bit architecturesScreen typesVulkan pre-rotation\nAbout the Oboe audio libraryUpdate build settingsCreate an audio streamSee Oboe code samplesSee the Oboe developer guide\nAbout managing memoryDebug native memory useDeliver assetsDetect and diagnose crashes\nAndroid Game Development Extension (AGDE) for Visual Studio\nAbout the AGDEGet started with AGDEConfigure your projectDebug your projectDebug memory corruption using Address SanitizerMeasure app performance\nOverviewConfigure Profile-Guided OptimizationModify build.gradle files for Android StudioSee AGDE code samplesAGDE release notes\nAbout Google Play Games on PCGet startedReview the release checklist\nSet up your game for PC compatibilityConfigure your graphics\nOverviewUpgrade from 1.0 to 1.1 Java/KotlinUpgrade from 1.0 to 1.1 UnityUpgrade from 0.0.4 to 1.0 Java/KotlinUpgrade from 0.0.4 to 1.0 Unity\nAbout continuity in cross-device playAbout continuity requirementsVerify your game's complianceFederate the identity servicesThird-party login\nTest your gameUse the developer emulatorUse ChromeOS devicesTroubleshoot the developer emulator\nPackage your game for Google Play ServicesSubmit your gameUpdate your gameIntegrity ProtectionFAQManage, debug, and profile in Android Studio\nAbout optimization toolsConfigure system tracingReduce game size\nAbout system profilingView a system profileGPU performance counters\nFrame processing timesMemory efficiencyTexture memory bandwidth usageVertex memory bandwidth usageThread scheduling\nMost expensive render passesVertex formatsShader performance\nPerformance paneCommands paneFramebuffer paneGeometry paneReport paneShader paneMemory paneState paneTextures paneTexture panePipeline view paneSupported Vulkan extensionsTroubleshoot AGIAndroid Performance Tuner (APT)Android Dynamic Performance Framework (ADPF)Optimize 3D assetsManage vertex data\nAbout the Memory Advice APIGet started with the Memory Advice API\nAbout the Game Mode API and interventionsUse the Game Mode APIUse Game Mode interventions\nAbout Google Play Games ServicesGet startedDownloadsSet up Play Games ServicesEnable Play Games Services featuresManage project settings in Google CloudPublish through Google Play Console\nSign inAchievementsLeaderboardsEventsSaved gamesFriendsNext generation Player IDs\nAbout the Google Play Games plugin for UnityGet started\nAchievementsLeaderboardsSaved gamesEventsFriendsPlayer stats\nGet startedSign inEnable server-side accessAnti-piracy\nAchievementsLeaderboardsFriendsSaved gamesEventsPlayer statsTroubleshooting\nAbout the Publishing APIGet startedUpload imagesManagement API\nQuality checklistBranding guidelinesQuota and rate limitsData disclosure requirementsTerms of serviceGet support\nAvailabilityArchitectureDeveloper functionalityGet started\nIntroductionData typesDifferential changes APIUser Privacy\nWrite dataRead raw dataRead aggregated dataDelete dataSynchronize dataWork with sessionsExceptionsBest practicesFrequently asked questionsVideo repositorySDK release notes\nHealth Connect API comparison guideFit Android API to Health Connect migration guideMigrate from Android 13 to 14\nFundamentals of testing Android appsWhat to test in AndroidUsing test doubles in Android\nSet up project for AndroidX TestJUnit4 rules with AndroidX TestAndroidJUnitRunner\nEspressoEspresso basicsEspresso setup instructionsEspresso cheat sheetEspresso idling resourcesEspresso-IntentsEspresso listsMultiprocess EspressoEspresso recipesEspresso WebAccessibility checkingAdditional Resources for Espresso\nTest content providersTest your serviceWrite automated tests with UI AutomatorPerformance\nMinimize your permission requestsReset unused permissions\nUse coarse location accuracyAccess location in the background only when necessaryAccess nearby Bluetooth devicesAccess nearby Wi-Fi devices\nDeclare package visibility needsWork with user-resettable identifiersSupport scoped storage\nWorkflow for requesting permissionsProvide prominent disclosure and consentExplain why your app needs permissionsAudit access to dataHandle permission denialsReview how your app collects and shares user data\nIndicatorsPasting an intentPrivacy DashboardApp hibernation\nChoose a developer programEnroll your platform with the Privacy SandboxSet up your development environmentConfigure AdServicesSet up a device or emulator imageConfigure devices to use Privacy Sandbox on Android\nOverview of Protected AudienceProtected Audience mediationProtected Audience frequency cappingProtected Audience app install ads filtering\nOverview and app measurementCross app and web measurementMeasurement Simulation Library\nDeveloper's guideProfile a Protected Audience auctionAttribution Reporting\nSDK Runtime APITopicsProtected Audience on AndroidAttribution Reporting APIAPI referenceSamples\nAbout security on AndroidApp security best practicesApp security improvement program\nCommon risksandroid:debuggableandroid:exportedContent resolversExposed directories to FileProviderIntent RedirectionHardcoded Cryptographic SecretsLog info disclosurePath traversalPending intentsSticky BroadcastSQL injectionTapjackingWeak PRNGZip Path Traversal\nAbout SafetyNetPlay Integrity APISafetyNet Safe Browsing APISafetyNet reCAPTCHA APISafetyNet Verify Apps APIVerifying hardware-backed key pairs with key attestationSafetyNet Attestation APIDiscontinuing SafetyNet Attestation\nWork with data more securelyCryptographyAndroid Keystore SystemRunning embedded DEX code directly from APK\nSecurity with network protocolsNetwork security configurationUpdate your security provider to protect against SSL exploitsAndroid Protected ConfirmationSafetyNet Safe Browsing APIMinimize use of optimized but unverified codePerform actions before initial device unlock\nAbout enterprise appsDeveloper guideWork profilesSet up managed configurations\nSend app feedback to EMMsTest app feedbackWork contactsDevice management policies\nOverviewLock task modeMultiple usersCookbookDevice controlNetworking and telephonySecuritySystem updatesNetwork activity logging\nAbout the versionsAndroid 13Android 12Android 11Android 10Android 9Android 8.0Android 7.0Device administration\nVerifying hardware-backed key pairs with Key Attestation\nSave and categorize content based on your preferences.\nKey Attestation gives you more confidence that the keys you use in your app\nare stored in a device's hardware-backed keystore. The following sections\ndescribe how to verify the properties of hardware-backed keys and how to\ninterpret the schema of the attestation certificate's extension data.\nNote: Before you verify the properties of a device's\nhardware-backed keys in a production-level environment, make sure\nthat the device supports hardware-level key attestation. To do so,\ncheck that the attestation certificate chain contains a root certificate that\nis signed with the Google attestation root key and that the\nattestationSecurityLevel element within the key description data structure\nIn addition, it's important to verify the signatures in the certificate chain\nand to confirm that none of the keys in the chain has been revoked by checking\nUnless all are valid and the root is the Google root key, don't\nfully trust the attestation. Note, however, that devices containing\nrevoked certificates are still at least as trustworthy as devices that only\nsupport software attestation. Having a fully valid attestation is a strong\npositive indicator. Not having one is a neutralnot\nDuring key attestation, you specify the alias of a key pair. The attestation\ntool, in return, provides a certificate chain, which you can use to verify\nIf the device supports hardware-level key attestation, the root certificate\nwithin this chain is signed using an attestation root key, which the device\nmanufacturer injects into the device's hardware-backed keystore at the\nattestation, Android 7.0 (API level 24) or higher, and Google Play services,\nthe root certificate is signed with the Google attestation root key.\nVerify that this root certificate is among those listed in the section on\nTo implement key attestation, complete the following steps:\nmethod to get a reference to the chain of X.509 certificates associated with\nmethod. Also verify that the root certificate is trustworthy.\nCaution: Although you can complete this process within\nyour app directly, it's safer to check the certificates'\nOn a separate server that you trust, obtain a reference to the ASN.1\nparser library that is most appropriate for your toolset. Find the\nnearest certificate to the root that contains the attestation\ncertificate extension; do not assume this to be the leaf certificate.\nUse the parser to extract the attestation certificate extension data\nAttestation sample uses the ASN.1 parser from Bouncy Castle to extract an\nattestation certificate's extension data. You can use this sample as a\nFor more details about the schema of the extension data, see\nthe section about certificate extension data schema.\nCompare the extension data that you've retrieved from your ASN.1 parser\nwith the set of values that you expect the hardware-backed key to contain.\nCaution: Although you can complete this process within\nyour app directly, it's safer to check the certificate's extension data\nThe trustworthiness of the attestation depends on the root certificate of the chain.\ndevices that have passed the testing required to have the Google suite of apps, including\nGoogle Play, and which launched with Android 7.0 (API level 24) or higher should use attestation\nkeys signed by the Google Hardware Attestation Root certificate.\nMIIFYDCCA0igAwIBAgIJAOj6GWMU0voYMA0GCSqGSIb3DQEBCwUAMBsxGTAXBgNV\nBAUTEGY5MjAwOWU4NTNiNmIwNDUwHhcNMTYwNTI2MTYyODUyWhcNMjYwNTI0MTYy\nODUyWjAbMRkwFwYDVQQFExBmOTIwMDllODUzYjZiMDQ1MIICIjANBgkqhkiG9w0B\nAQEFAAOCAg8AMIICCgKCAgEAr7bHgiuxpwHsK7Qui8xUFmOr75gvMsd/dTEDDJdS\nSxtf6An7xyqpRR90PL2abxM1dEqlXnf2tqw1Ne4Xwl5jlRfdnJLmN0pTy/4lj4/7\ntv0Sk3iiKkypnEUtR6WfMgH0QZfKHM1+di+y9TFRtv6y//0rb+T+W8a9nsNL/ggj\nnar86461qO0rOs2cXjp3kOG1FEJ5MVmFmBGtnrKpa73XpXyTqRxB/M0n1n/W9nGq\nC4FSYa04T6N5RIZGBN2z2MT5IKGbFlbC8UrW0DxW7AYImQQcHtGl/m00QLVWutHQ\noVJYnFPlXTcHYvASLu+RhhsbDmxMgJJ0mcDpvsC4PjvB+TxywElgS70vE0XmLD+O\nJtvsBslHZvPBKCOdT0MS+tgSOIfga+z1Z1g7+DVagf7quvmag8jfPioyKvxnK/Eg\nsTUVi2ghzq8wm27ud/mIM7AY2qEORR8Go3TVB4HzWQgpZrt3i5MIlCaY504LzSRi\nigHCzAPlHws+W0rB5N+er5/2pJKnfBSDiCiFAVtCLOZ7gLiMm0jhO2B6tUXHI/+M\nRPjy02i59lINMRRev56GKtcd9qO/0kUJWdZTdA2XoS82ixPvZtXQpUpuL12ab+9E\naDK8Z4RHJYYfCT3Q5vNAXaiWQ+8PTWm2QgBR/bkwSWc+NpUFgNPN9PvQi8WEg5Um\nAGMCAwEAAaOBpjCBozAdBgNVHQ4EFgQUNmHhAHyIBQlRi0RsR/8aTMnqTxIwHwYD\nVR0jBBgwFoAUNmHhAHyIBQlRi0RsR/8aTMnqTxIwDwYDVR0TAQH/BAUwAwEB/zAO\nBgNVHQ8BAf8EBAMCAYYwQAYDVR0fBDkwNzA1oDOgMYYvaHR0cHM6Ly9hbmRyb2lk\nLmdvb2dsZWFwaXMuY29tL2F0dGVzdGF0aW9uL2NybC8wDQYJKoZIhvcNAQELBQAD\nggIBACDIw41L3KlXG0aMiS//cqrG+EShHUGo8HNsw30W1kJtjn6UBwRM6jnmiwfB\nPb8VA91chb2vssAtX2zbTvqBJ9+LBPGCdw/E53Rbf86qhxKaiAHOjpvAy5Y3m00m\nqC0w/Zwvju1twb4vhLaJ5NkUJYsUS7rmJKHHBnETLi8GFqiEsqTWpG/6ibYCv7rY\nDBJDcR9W62BW9jfIoBQcxUCUJouMPH25lLNcDc1ssqvC2v7iUgI9LeoM1sNovqPm\nQUiG9rHli1vXxzCyaMTjwftkJLkf6724DFhuKug2jITV0QkXvaJWF4nUaHOTNA4u\nJU9WDvZLI1j83A+/xnAJUucIv/zGJ1AMH2boHqF8CY16LpsYgBt6tKxxWH00XcyD\nCdW2KlBCeqbQPcsFmWyWugxdcekhYsAWyoSf818NUsZdBWBaR/OukXrNLfkQ79Iy\nZohZbvabO/X+MVT3rriAoKc8oE2Uws6DF+60PV7/WIPjNvXySdqspImSN78mflxD\nqwLqRBYkA3I75qppLGG9rp7UCdRjxMl8ZDBld+7yvHVgt1cVzJx9xnyGCC23Uaic\nMDSXYrB4I4WHXPGjxhZuCuPBLTdOLU8YRvMYdEvYebWHMpvwGCF6bAx3JBpIeOQ1\nMIIFHDCCAwSgAwIBAgIJANUP8luj8tazMA0GCSqGSIb3DQEBCwUAMBsxGTAXBgNV\nBAUTEGY5MjAwOWU4NTNiNmIwNDUwHhcNMTkxMTIyMjAzNzU4WhcNMzQxMTE4MjAz\nNzU4WjAbMRkwFwYDVQQFExBmOTIwMDllODUzYjZiMDQ1MIICIjANBgkqhkiG9w0B\nAQEFAAOCAg8AMIICCgKCAgEAr7bHgiuxpwHsK7Qui8xUFmOr75gvMsd/dTEDDJdS\nSxtf6An7xyqpRR90PL2abxM1dEqlXnf2tqw1Ne4Xwl5jlRfdnJLmN0pTy/4lj4/7\ntv0Sk3iiKkypnEUtR6WfMgH0QZfKHM1+di+y9TFRtv6y//0rb+T+W8a9nsNL/ggj\nnar86461qO0rOs2cXjp3kOG1FEJ5MVmFmBGtnrKpa73XpXyTqRxB/M0n1n/W9nGq\nC4FSYa04T6N5RIZGBN2z2MT5IKGbFlbC8UrW0DxW7AYImQQcHtGl/m00QLVWutHQ\noVJYnFPlXTcHYvASLu+RhhsbDmxMgJJ0mcDpvsC4PjvB+TxywElgS70vE0XmLD+O\nJtvsBslHZvPBKCOdT0MS+tgSOIfga+z1Z1g7+DVagf7quvmag8jfPioyKvxnK/Eg\nsTUVi2ghzq8wm27ud/mIM7AY2qEORR8Go3TVB4HzWQgpZrt3i5MIlCaY504LzSRi\nigHCzAPlHws+W0rB5N+er5/2pJKnfBSDiCiFAVtCLOZ7gLiMm0jhO2B6tUXHI/+M\nRPjy02i59lINMRRev56GKtcd9qO/0kUJWdZTdA2XoS82ixPvZtXQpUpuL12ab+9E\naDK8Z4RHJYYfCT3Q5vNAXaiWQ+8PTWm2QgBR/bkwSWc+NpUFgNPN9PvQi8WEg5Um\nAGMCAwEAAaNjMGEwHQYDVR0OBBYEFDZh4QB8iAUJUYtEbEf/GkzJ6k8SMB8GA1Ud\nIwQYMBaAFDZh4QB8iAUJUYtEbEf/GkzJ6k8SMA8GA1UdEwEB/wQFMAMBAf8wDgYD\nVR0PAQH/BAQDAgIEMA0GCSqGSIb3DQEBCwUAA4ICAQBOMaBc8oumXb2voc7XCWnu\nXKhBBK3e2KMGz39t7lA3XXRe2ZLLAkLM5y3J7tURkf5a1SutfdOyXAmeE6SRo83U\nh6WszodmMkxK5GM4JGrnt4pBisu5igXEydaW7qq2CdC6DOGjG+mEkN8/TA6p3cno\nL/sPyz6evdjLlSeJ8rFBH6xWyIZCbrcpYEJzXaUOEaxxXxgYz5/cTiVKN2M1G2ok\nQBUIYSY6bjEL4aUN5cfo7ogP3UvliEo3Eo0YgwuzR2v0KR6C1cZqZJSTnghIC/vA\nD32KdNQ+c3N+vl2OTsUVMC1GiWkngNx1OO1+kXW+YTnnTUOtOIswUP/Vqd5SYgAI\nmMAfY8U9/iIgkQj6T2W6FsScy94IN9fFhE1UtzmLoBIuUFsVXJMTz+Jucth+IqoW\nFua9v1R93/k98p41pjtFX+H8DslVgfP097vju4KDlqN64xV1grw3ZLl4CiOe/A91\noeLm2UHOq6wn3esB4r2EIQKb6jTVGu5sYCcdWpXr0AUVqcABPdgL+H7qJguBw09o\njm6xNIrw2OocrDKsudk/okr/AwqEyPKw9WnMlQgLIKw1rODG2NvU9oR3GVGdMkUB\nZutL8VuFkERQGt6vQ2OCw0sV47VMkuYbacK/xyZFiRcrPJPb41zgbQj9XAEyLKCH\nMIIFHDCCAwSgAwIBAgIJAMNrfES5rhgxMA0GCSqGSIb3DQEBCwUAMBsxGTAXBgNV\nBAUTEGY5MjAwOWU4NTNiNmIwNDUwHhcNMjExMTE3MjMxMDQyWhcNMzYxMTEzMjMx\nMDQyWjAbMRkwFwYDVQQFExBmOTIwMDllODUzYjZiMDQ1MIICIjANBgkqhkiG9w0B\nAQEFAAOCAg8AMIICCgKCAgEAr7bHgiuxpwHsK7Qui8xUFmOr75gvMsd/dTEDDJdS\nSxtf6An7xyqpRR90PL2abxM1dEqlXnf2tqw1Ne4Xwl5jlRfdnJLmN0pTy/4lj4/7\ntv0Sk3iiKkypnEUtR6WfMgH0QZfKHM1+di+y9TFRtv6y//0rb+T+W8a9nsNL/ggj\nnar86461qO0rOs2cXjp3kOG1FEJ5MVmFmBGtnrKpa73XpXyTqRxB/M0n1n/W9nGq\nC4FSYa04T6N5RIZGBN2z2MT5IKGbFlbC8UrW0DxW7AYImQQcHtGl/m00QLVWutHQ\noVJYnFPlXTcHYvASLu+RhhsbDmxMgJJ0mcDpvsC4PjvB+TxywElgS70vE0XmLD+O\nJtvsBslHZvPBKCOdT0MS+tgSOIfga+z1Z1g7+DVagf7quvmag8jfPioyKvxnK/Eg\nsTUVi2ghzq8wm27ud/mIM7AY2qEORR8Go3TVB4HzWQgpZrt3i5MIlCaY504LzSRi\nigHCzAPlHws+W0rB5N+er5/2pJKnfBSDiCiFAVtCLOZ7gLiMm0jhO2B6tUXHI/+M\nRPjy02i59lINMRRev56GKtcd9qO/0kUJWdZTdA2XoS82ixPvZtXQpUpuL12ab+9E\naDK8Z4RHJYYfCT3Q5vNAXaiWQ+8PTWm2QgBR/bkwSWc+NpUFgNPN9PvQi8WEg5Um\nAGMCAwEAAaNjMGEwHQYDVR0OBBYEFDZh4QB8iAUJUYtEbEf/GkzJ6k8SMB8GA1Ud\nIwQYMBaAFDZh4QB8iAUJUYtEbEf/GkzJ6k8SMA8GA1UdEwEB/wQFMAMBAf8wDgYD\nVR0PAQH/BAQDAgIEMA0GCSqGSIb3DQEBCwUAA4ICAQBTNNZe5cuf8oiq+jV0itTG\nzWVhSTjOBEk2FQvh11J3o3lna0o7rd8RFHnN00q4hi6TapFhh4qaw/iG6Xg+xOan\n63niLWIC5GOPFgPeYXM9+nBb3zZzC8ABypYuCusWCmt6Tn3+Pjbz3MTVhRGXuT/T\nQH4KGFY4PhvzAyXwdjTOCXID+aHud4RLcSySr0Fq/L+R8TWalvM1wJJPhyRjqRCJ\nerGtfBagiALzvhnmY7U1qFcS0NCnKjoO7oFedKdWlZz0YAfu3aGCJd4KHT0MsGiL\nZez9WP81xYSrKMNEsDK+zK5fVzw6jA7cxmpXcARTnmAuGUeI7VVDhDzKeVOctf3a\n0qQLwC+d0+xrETZ4r2fRGNw2YEs2W8Qj6oDcfPvq9JySe7pJ6wcHnl5EZ0lwc4xH\n7Y4Dx9RA1JlfooLMw3tOdJZH0enxPXaydfAD3YifeZpFaUzicHeLzVJLt9dvGB0b\nHQLE4+EqKFgOZv2EoP686DQqbVS1u+9k0p2xbMA105TBIk7npraa8VM0fnrRKi7w\nlZKwdH+aNAyhbXRW9xsnODJ+g8eF452zvbiKKngEKirK5LGieoXBX7tZ9D1GNBH2\nOb3bKOwwIWdEFle/YF/h6zWgdeoaNGDqVBrLr2+0DtWoiB1aDEjLWl9FmyIUyUm7\nMIIFHDCCAwSgAwIBAgIJAPHBcqaZ6vUdMA0GCSqGSIb3DQEBCwUAMBsxGTAXBgNV\nBAUTEGY5MjAwOWU4NTNiNmIwNDUwHhcNMjIwMzIwMTgwNzQ4WhcNNDIwMzE1MTgw\nNzQ4WjAbMRkwFwYDVQQFExBmOTIwMDllODUzYjZiMDQ1MIICIjANBgkqhkiG9w0B\nAQEFAAOCAg8AMIICCgKCAgEAr7bHgiuxpwHsK7Qui8xUFmOr75gvMsd/dTEDDJdS\nSxtf6An7xyqpRR90PL2abxM1dEqlXnf2tqw1Ne4Xwl5jlRfdnJLmN0pTy/4lj4/7\ntv0Sk3iiKkypnEUtR6WfMgH0QZfKHM1+di+y9TFRtv6y//0rb+T+W8a9nsNL/ggj\nnar86461qO0rOs2cXjp3kOG1FEJ5MVmFmBGtnrKpa73XpXyTqRxB/M0n1n/W9nGq\nC4FSYa04T6N5RIZGBN2z2MT5IKGbFlbC8UrW0DxW7AYImQQcHtGl/m00QLVWutHQ\noVJYnFPlXTcHYvASLu+RhhsbDmxMgJJ0mcDpvsC4PjvB+TxywElgS70vE0XmLD+O\nJtvsBslHZvPBKCOdT0MS+tgSOIfga+z1Z1g7+DVagf7quvmag8jfPioyKvxnK/Eg\nsTUVi2ghzq8wm27ud/mIM7AY2qEORR8Go3TVB4HzWQgpZrt3i5MIlCaY504LzSRi\nigHCzAPlHws+W0rB5N+er5/2pJKnfBSDiCiFAVtCLOZ7gLiMm0jhO2B6tUXHI/+M\nRPjy02i59lINMRRev56GKtcd9qO/0kUJWdZTdA2XoS82ixPvZtXQpUpuL12ab+9E\naDK8Z4RHJYYfCT3Q5vNAXaiWQ+8PTWm2QgBR/bkwSWc+NpUFgNPN9PvQi8WEg5Um\nAGMCAwEAAaNjMGEwHQYDVR0OBBYEFDZh4QB8iAUJUYtEbEf/GkzJ6k8SMB8GA1Ud\nIwQYMBaAFDZh4QB8iAUJUYtEbEf/GkzJ6k8SMA8GA1UdEwEB/wQFMAMBAf8wDgYD\nVR0PAQH/BAQDAgIEMA0GCSqGSIb3DQEBCwUAA4ICAQB8cMqTllHc8U+qCrOlg3H7\n174lmaCsbo/bJ0C17JEgMLb4kvrqsXZs01U3mB/qABg/1t5Pd5AORHARs1hhqGIC\nW/nKMav574f9rZN4PC2ZlufGXb7sIdJpGiO9ctRhiLuYuly10JccUZGEHpHSYM2G\ntkgYbZba6lsCPYAAP83cyDV+1aOkTf1RCp/lM0PKvmxYN10RYsK631jrleGdcdkx\noSK//mSQbgcWnmAEZrzHoF1/0gso1HZgIn0YLzVhLSA/iXCX4QT2h3J5z3znluKG\n1nv8NQdxei2DIIhASWfu804CA96cQKTTlaae2fweqXjdN1/v2nqOhngNyz1361mF\nmr4XmaKH/ItTwOe72NI9ZcwS1lVaCvsIkTDCEXdm9rCNPAY10iTunIHFXRh+7KPz\nlHGewCq/8TOohBRn0/NNfh7uRslOSZ/xKbN9tMBtw37Z8d2vvnXq/YWdsm1+JLVw\nn6yYD/yacNJBlwpddla8eaVMjsF6nBnIgQOf9zKSe06nSTqvgwUHosgOECZJZ1Eu\nzbH4yswbt02tKtKEFhx+v+OTge/06V+jGsqTWLsfrOCNLuA8H++z+pUENmpqnnHo\nvaI47gC+TNpkgYGkkBT6B/m/U01BuOBBTzhIlMEZq9qkDWuM2cA5kW5V3FJUcfHn\nIf the root certificate in the attestation chain you receive is one of these certificates and\nnone of the certificates in the chain has been revoked, you\nYour key is in hardware that Google believes to be secure; and\nIt has the properties described in the attestation certificate.\nIf the attestation chain has any other root certificate, then Google does not make any claims\nabout the security of the hardware. This doesn't mean that your key is compromised, only that the\nattestation doesn't prove that the key is in secure hardware. Adjust your\nIf the root certificate isn't in the list on this page, there are two likely reasons:\nMost likely, the device launched with an Android version less than 7.0 and it doesn't support\nhardware attestation. In this case, Android has a software implementation of attestation that\nproduces the same sort of attestation certificate, but signed with a key\nattestation might have been created by an attacker pretending to provide secure hardware.\nThe other likely reason is that the device isn't a Google Play device. In that case, the device\nmaker is free to create their own root and to make whatever claims they like about what the\nattestation means. Refer to the device maker's documentation. Note that as of this writing\nGoogle isn't aware of any device makers who have done this.\nAlternatively, instead of validating directly against the bytes of the certificate, consider\nvalidating against just the root public key alone and then verifying the self-signed\nroot certificate. The root public key is formatted as follows:\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAr7bHgiuxpwHsK7Qui8xU\nFmOr75gvMsd/dTEDDJdSSxtf6An7xyqpRR90PL2abxM1dEqlXnf2tqw1Ne4Xwl5j\nlRfdnJLmN0pTy/4lj4/7tv0Sk3iiKkypnEUtR6WfMgH0QZfKHM1+di+y9TFRtv6y\n//0rb+T+W8a9nsNL/ggjnar86461qO0rOs2cXjp3kOG1FEJ5MVmFmBGtnrKpa73X\npXyTqRxB/M0n1n/W9nGqC4FSYa04T6N5RIZGBN2z2MT5IKGbFlbC8UrW0DxW7AYI\nmQQcHtGl/m00QLVWutHQoVJYnFPlXTcHYvASLu+RhhsbDmxMgJJ0mcDpvsC4PjvB\n+TxywElgS70vE0XmLD+OJtvsBslHZvPBKCOdT0MS+tgSOIfga+z1Z1g7+DVagf7q\nuvmag8jfPioyKvxnK/EgsTUVi2ghzq8wm27ud/mIM7AY2qEORR8Go3TVB4HzWQgp\nZrt3i5MIlCaY504LzSRiigHCzAPlHws+W0rB5N+er5/2pJKnfBSDiCiFAVtCLOZ7\ngLiMm0jhO2B6tUXHI/+MRPjy02i59lINMRRev56GKtcd9qO/0kUJWdZTdA2XoS82\nixPvZtXQpUpuL12ab+9EaDK8Z4RHJYYfCT3Q5vNAXaiWQ+8PTWm2QgBR/bkwSWc+\nAttestation keys can be revoked for a number of reasons, including mishandling or suspected\nextraction by an attacker. Therefore, it's critical that the status of each certificate in\nan attestation chain be checked against the official certificate revocation status list (CRL).\nThis list is maintained by Google and published at:\nThis URL returns a JSON file containing the revocation status for any certificates that don't\nhave a normal valid status. The format of the JSON file adheres to the following JSON Schema\n\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\"description\" : \"Each entry represents the status of an attestation key. The dictionary-key is the certificate serial number in lowercase hex.\",\n\"description\": \"[REQUIRED] Current status of the key.\",\n\"description\": \"[OPTIONAL] UTC date when certificate expires in ISO8601 format (YYYY-MM-DD). Can be used to clear expired certificates from the status list.\",\n\"description\": \"[OPTIONAL] Reason for the current status.\",\n\"enum\": [\"UNSPECIFIED\", \"KEY_COMPROMISE\", \"CA_COMPROMISE\", \"SUPERSEDED\", \"SOFTWARE_FLAW\"]\n\"description\": \"[OPTIONAL] Free form comment about the key status.\",\n\"comment\": \"Bug in keystore causes this key malfunction b/555555\"\nThe CRL URLs embedded in legacy attestation certificates continue to operate. New\nattestation certificates no longer contain a CRL URL extension. The status of legacy\ncertificates is also included in the attestation status list, so developers can safely\nswitch to using the attestation status list for both new and legacy certificates. An example of\nhow to correctly verify Android attestation keys is included in the Key Attestation sample.\nKey attestation verifies the extension data that appears in the first\ncertificate within the chain in a device's hardware-backed keystore. The\ncertificate stores the information according to an ASN.1 schema. To view the\nschema corresponding to the attestation version that you're using, select the\nThe following list presents a description of each element within the schema:\nThis sequence of values presents general information about the key pair being\nverified through key attestation and provides easy access to additional\nWarning: Although it is possible to attest keys that are\nstored in the Android systemthat is, if the value of\ncan't trust these attestations if the Android system becomes compromised.\nThe version of the Keymaster or KeyMint hardware abstraction layer (HAL).\nContains the challenge that was provided at key creation time. Check whether this value matches\nTag::ATTESTATION_CHALLENGE authorization tag. Otherwise, your service might be\nvulnerable to replaying of old attestation certificates.\nThis value identifies the devicebut only for a limited period of time. It is\nis only used by system apps. In all other apps, uniqueId is empty.\nis enforced by the Android system, not by the device's Trusted Execution Environment (TEE).\nThis data structure indicates the extent to which a software feature, such as\na key pair, is protected based on its location within the device.\nBecause the data structure is an enumeration, it takes on exactly one of the\nThe logic for creating and managing the feature is implemented in the\nAndroid system. For the purposes of creating and storing key pairs, this\nlocation is less secure than the TEE but is more secure than your app's\nThe logic for creating and managing the feature is implemented in secure\nhardware, such as a TEE. For the purposes of creating and storing key pairs,\nthis location is more secure because secure hardware is highly resistant to\nThe logic for creating and managing the feature is implemented in a\nsecurity module. For the purposes of creating and storing key\npairs, this location is more secure because it is highly resistant to remote\ncompromise and hardware attacks against the module.\nThis data structure contains the key pair's properties themselves, as defined\nin the Keymaster or KeyMint hardware abstraction layer (HAL). You compare these values\nto the device's current state or to a set of expected values to verify that a\nEach field name corresponds to a similarly-named Keymaster / KeyMint authorization tag.\nFor example, the keySize field in an authorization list\nIn an attestation AuthorizationList object, the algorithm\nThe set of parameters used to generate an elliptic curve (EC) key pair,\nwhich uses ECDSA for signing and verification, within the Android system\nCorresponds to the Tag::RSA_OAEP_MGF_DIGEST KeyMint authorization\nTag::ORIGINATION_EXPIRE_DATETIME Keymaster authorization\nAllows the key to be used after its authentication timeout period if the\nuser is still wearing the device on their body. Note that a secure\non-body sensor determines whether the device is being worn on the user's\nCorresponds to the Tag::TRUSTED_USER_PRESENCE_REQUIRED\nauthorization tag, which uses a tag ID value of 507.\nSpecifies that this key is usable only if the user has provided proof of\nphysical presence. Several examples include the following:\nFor a StrongBox key, a hardware button hardwired to a pin on the\nFor a TEE key, fingerprint authentication provides proof of presence\nas long as the TEE has exclusive control of the scanner and performs the\nCorresponds to the Tag::TRUSTED_CONFIRMATION_REQUIRED\nauthorization tag, which uses a tag ID value of 508.\nSpecifies that the key is usable only if the user provides confirmation of\nthe data to be signed using an approval token. For more information about\nauthorization tag, which uses a tag ID value of 509.\nIndicates whether all apps on a device can access the key pair.\nThe version of the Android operating system associated with the\nKeymaster, specified as a six-digit integer. For example, version 8.1.0\nOnly Keymaster version 1.0 or higher includes this value in the\nThe month and year associated with the security patch that is being used\nwithin the Keymaster, specified as a six-digit integer. For example, the\nOnly Keymaster version 1.0 or higher includes this value in the\nTag::ATTESTATION_APPLICATION_ID Keymaster authorization\nTag::ATTESTATION_ID_BRAND Keymaster tag, which uses a tag\nTag::ATTESTATION_ID_DEVICE Keymaster tag, which uses a tag\nTag::ATTESTATION_ID_PRODUCT Keymaster tag, which uses a tag\nTag::ATTESTATION_ID_SERIAL Keymaster tag, which uses a tag\nTag::ATTESTATION_ID_MODEL authorization tag, which uses a tag\nauthorization tag, which uses a tag ID value of 718.\nSpecifies the vendor image security patch level that must be\ninstalled on the device for this key to be used. The value appears in the\nform YYYYMMDD, representing the date of the vendor security patch. For\nexample, if a key were generated on an Android device with the vendor's\nAugust 1, 2018 security patch installed, this value would be 20180801.\nauthorization tag, which uses a tag ID value of 719.\nSpecifies the kernel image security patch level that must be\ninstalled on the device for this key to be used. The value appears in the\nform YYYYMMDD, representing the date of the system security patch. For\nexample, if a key were generated on an Android device with the system's\nAugust 5, 2018 security patch installed, this value would be 20180805.\nauthorization tag, which uses a tag ID value of 720.\nThis collection of values defines key information about the device's status.\nA secure hash of the key that verifies the system image. It is recommended\nTrue if the device's bootloader is locked, which enables Verified Boot\nchecking and prevents an unsigned device image from being flashed onto the\ndevice. For more information about this feature, see the\nstate of the device, according to the Verified Boot feature.\nA digest of all data protected by Verified Boot. For devices that use\nthe Android Verified Boot implementation of Verified Boot, this value\ncontains the digest of the VBMeta struct, or the Verified Boot\nTo learn more about how to calculate this value, see\nThis data structure provides the device's current boot state, which\nrepresents the level of protection provided to the user and to apps after the\ndevice finishes booting. For more information about this feature, see the\nBoot State section within the Verifying Boot documentation.\nThis data structure is an enumeration, so it takes on exactly one of the\nIndicates a full chain of trust, which includes the bootloader, the boot\nWhen the device is in this boot state, the verifiedBootKey is\nthe hash of the device-embedded certificate, which the device manufacturer\nIndicates that the device-embedded certificate has verified the device's\nWhen the device is in this boot state, the verifiedBootKey is\nthe hash of a user-installed certificate, which signs a boot partition\nthat the user adds to the device in place of the original,\nIndicates that the user can modify the device freely. Therefore, the user is\nIndicates that the device has failed verification. The attestation\ncertificate should never use this value for VerifiedBootState.\nThis data structure reflects the Android platform's belief as to which apps\nare allowed to use the secret key material under attestation. The ID can\ncomprise multiple packages if and only if multiple packages share the same\nUID. The octet string is itself formatted according to the following ASN.1\nA set of AttestationPackageInfo objects, each providing a\nA set of SHA-256 digests of the app's signing certificates. An app can\nhave multiple signing key certificate chains. For each, the \"leaf\"\ncertificate is digested and placed in the signature_digests\nfield. The field name is misleading, since the digested data is the app's\nsigning certificates, not the app signatures, because it is named for the\n{SHA256(PackageInfo.signature[0]), SHA256(PackageInfo.signature[1]), ...}\nContent and code samples on this page are subject to the licenses described in the Content License. Java and OpenJDK are trademarks or registered trademarks of Oracle and/or its affiliates.\nConnect with the Android Developers community on LinkedIn",
    "[2107.10536] Improving the Authentication with Built-in Camera Protocol Using Built-in Motion Sensors: A Deep Learning Solution\nWe gratefully acknowledge support fromthe Simons Foundation and member institutions.\n[Submitted on 22 Jul 2021 (v1), last revised 27 Jul 2021 (this version, v3)]\nTitle:Improving the Authentication with Built-in Camera Protocol Using Built-in Motion Sensors: A Deep Learning Solution\nDownload a PDF of the paper titled Improving the Authentication with Built-in Camera Protocol Using Built-in Motion Sensors: A Deep Learning Solution, by Cezara Benegui and 1 other authors\nWe propose an enhanced version of the Authentication with Built-in Camera\n(ABC) protocol by employing a deep learning solution based on built-in motion\nsensors. The standard ABC protocol identifies mobile devices based on the\nphoto-response non-uniformity (PRNU) of the camera sensor, while also\nconsidering QR-code-based meta-information. During authentication, the user is\nrequired to take two photos that contain two QR codes presented on a screen.\nThe presented QR code images also contain a unique probe signal, similar to a\ncamera fingerprint, generated by the protocol. During verification, the server\ncomputes the fingerprint of the received photos and authenticates the user if\n(i) the probe signal is present, (ii) the metadata embedded in the QR codes is\ncorrect and (iii) the camera fingerprint is identified correctly. However, the\nprotocol is vulnerable to forgery attacks when the attacker can compute the\ncamera fingerprint from external photos, as shown in our preliminary work. In\nthis context, we propose an enhancement for the ABC protocol based on motion\nsensor data, as an additional and passive authentication layer. Smartphones can\nbe identified through their motion sensor data, which, unlike photos, is never\nposted by users on social media platforms, thus being more secure than using\nphotographs alone. To this end, we transform motion signals into embedding\nvectors produced by deep neural networks, applying Support Vector Machines for\nthe smartphone identification task. Our change to the ABC protocol results in a\nmulti-modal protocol that lowers the false acceptance rate for the attack\nproposed in our previous work to a percentage as low as 0.07%.\nCryptography and Security (cs.CR); Machine Learning (cs.LG)\nSubmission history From: Radu Tudor Ionescu [view email]\nDownload a PDF of the paper titled Improving the Authentication with Built-in Camera Protocol Using Built-in Motion Sensors: A Deep Learning Solution, by Cezara Benegui and 1 other authors\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."
  ],
  "attested camera sensor technology": [
    "Skip to contentMeet us at Embedded World 2023 during 14-16th March at Nuremberg, Germany\nHardware Design Turnkey Product Development FPGA Design\nFPGA DesignWe provide end-to-end FPGA solutions that help our customers reduce development time and increase final product confidence with proven technology.Let Logic Fruit Technologies help you succeed.Explore More  Embedded Software Development\nEmbedded Software DevelopmentWe offer our customers turn-key embedded software design and development services, creating, testing, debugging, and validating solutions.Let Logic Fruit Technologies help you succeed.Explore More  Hardware Design Hardware DesignA complete embedded hardware design and development house that helps you to build innovative solutions from prototype to final product/IP to Production.Let Logic Fruit Technologies help you succeed.Explore More  Turnkey Product Development Turnkey Product DevelopmentAgile and flexible research and development house that supports you throughout the lifecycle of product design and development from idea to prototype to actual product.Let Logic Fruit Technologies help you succeed.Explore More  SolutionsDefence and Aerospace\nArtificial Intelligence Semiconductor Image Processing & Computer Vision Defence and Aerospace\nDefence and AerospaceA trusted and recognised R&D house in the design and development of telecommunications and data communications products worldwide.Let Logic Fruit Technologies help you succeed.Explore More  Telecom\nTelecomA trusted and recognised R&D house in the design and development of telecommunications and data communications products worldwide.Let Logic Fruit Technologies help you succeed.Read More  Test and Measurement\nTest and MeasurementA trusted global R&D partner to develop test and measurement solutions for various organizations to meet rigorous requirements of the industry.Let Logic Fruit Technologies help you succeed.Explore More  Artificial Intelligence Artificial IntelligenceLogic Fruit has graduated from doing traditional FPGA/DSP based system to developing FPGA uses in new era of On-Edge processing for AI/ML applications and Hardware Acceleration using Heterogeneous system.Let Logic Fruit Technologies help you succeed.Explore More  Semiconductor SemiconductorLogic fruit enables you to bring innovations to your end customer faster with its design and verification expertise in multiple domains while optimizing for performance and high quality.Let Logic Fruit Technologies help you succeed.Explore More  Image Processing & Computer Vision Image Processing & Computer VisionOur Vision Systems design services help organizations designing superior algorithms for achieving maximum achievable accuracy and run-time performance.Let Logic Fruit Technologies help you succeed.Explore More  ProjectsIP\nProjectsInnovative end to end products and IP Core .Logic Fruit Technologies is helping organizations to build the latest technologies and end-to-end products with a wide range of specializations. To each project, we bring experience in a variety of high-speed protocols, Interfaces 1G/10G/40G/100G Ethernet, PCIe(Gen1-Gen6), USB3.0/4.0, CPRI/ORAN, DisplayPort, ARINC818, real-time signal processing, as well as signal generation, data analysis.Let Logic Fruit Technologies help you succeed.View All Projects IP\nScalable Data Acquisition System. Smart Multi-functional Display (SMFD) IF Recorder and Playback System.\nArbitrary Waveform Generator (AWG) ARINC 818 Video Analyzer & Multiformat Video Generator (AVAG)\nCase Studies Infographics Application Notes Videos Blog\nBlogsRead here all about latest trends and insights on innovative technologies around the world and also thought leadership content on multiple domains. We share our expertise and opinions on multiple topics and areas of technology.Explore More  Glossary\nGlossaryRead here all about latest trends and insights on innovative technologies around the world and also thought leadership content on multiple domains. We share our expertise and opinions on multiple topics and areas of technology.Explore More  Whitepaper\nWhitepaperCheckout all the latest innovations and technological developments whitepapers that guide you to understand the processes and help you to build new innovative solutions. We collaborate with multiple departments to cover all technical terms in simple to understand and a practical guides to implement new technologies in prospects journey of growth.Explore More  Case Studies Case Studies.Logic Fruit Capabilities Extend From Specification To FPGA To System, With Expertise Spanning RTL Design, Verification, Floor Planning, Timing Closure, Design Validation And System-Level Validation. We Are Experts On Working With High Density & High Speed Design.Explore More  Infographics InfographicsGraphics are the better way to communicate ideas and information. Checkout all the infographics from Logic Fruit Technologies here and step up your journey towards innovation in technology to the next step with learning from industry experts.Explore More  Application Notes Application NotesLogic Fruit Capabilities Extend From Specification To FPGA To System, With Expertise Spanning RTL Design, Verification, Floor Planning, Timing Closure, Design Validation And System-Level Validation. We Are Experts On Working With High Density & High Speed Design.Explore More  Videos VideosFind out the latest videos from Logic Fruit Technologies about demos of products, and awareness videos about the latest technologies like PCIe, ARINC, FPGA, etc.Explore More  About UsCompany Overview\nCompany OverviewLogic Fruit specializes in high-quality real-time high throughput FPGA/SoC embedded solutions and proof-of-concept designs that require FPGA/SOC based, computation, real-time data generation, acquisition, and analysis.Explore More  Mission & Vision\nMission & VisionWe strive to create value for our customers by becoming their trusted and reliable R&D partner for accelerated development and create opportunities for our engineers to work on the latest technologies in electronics.Explore More  Leadership\nLeadershipWe are proud to have industry leaders in our core team who not only build innovative solutions but also help new talent to grow and learn in this space.Explore More  News\nNewsCheckout here all about the latest coverage and news from Logic Fruit Technologies. As an innovative R&D house we keep publishing our new offering and company updates here to keep all our stakeholders informed and keep them up to date. Explore More  Events\nEventsWe always keep attending all major events and exhibit all around the world to showcase our innovative projects and solutions. Lets catchup at next event!Explore More  Career CareerJoin Logic Fruit Technology and work with industry experts to taste the success and put your hand in building innovative technical solutions globally. We are Logicians and believe in building new leaders in the space of Technology.Join Us  Partners PartnersWe partner with industry leaders to understand the full implications of each technical challenge, creating a plan with our expertise and reduce the timeline to market new innovationsLearn More  Contact Us Contact UsWe are the industry leader in multiple domains and have a worldwide presence. Get in touch with us to know more about how we can help your organization navigate its next level of innovation in technology.Get in Touch\nProduct EngineeringFPGA DesignEmbedded Software DevelopmentHardware DesignTurnkey Product DevelopmentSolutionsDefence and AerospaceTelecomTest and MeasurementArtificial Intelligence and Data CenterSemiconductorVisionProjectsIP\nProjectsInnovative end to end products and IP Core .Logic Fruit Technologies is helping organizations to build the latest technologies and end-to-end products with a wide range of specializations. To each project, we bring experience in a variety of high-speed protocols, Interfaces 1G/10G/40G/100G Ethernet, PCIe(Gen1-Gen6), USB3.0/4.0, CPRI/ORAN, DisplayPort, ARINC818, real-time signal processing, as well as signal generation, data analysis.Let Logic Fruit Technologies help you succeed.View All Projects IP\nScalable Data Acquisition System. Smart Multi-functional Display (SMFD) IF Recorder and Playback System.\nArbitrary Waveform Generator (AWG) ARINC 818 Video Analyzer & Multiformat Video Generator (AVAG)\nView All Projects ResourcesBlogWhitepaper Datasheet Case Studies Infographics About UsCompany Overview Mission & Vision leadership News Career Partners Contact Us\nProduct EngineeringFPGA DesignEmbedded Software DevelopmentHardware DesignTurnkey Product Development\nSolutionsDefence and AerospaceTelecomTest and MeasurementArtificial Intelligence and Data CenterSemiconductorImage Processing & Computer Vision\nProjectsIPARINC 818 IP COREARINC 429 IP COREJESD204B Transmitter and Receiver IPJESD204C Transmitter and receiver IPDISPLAY PORT IPsCPRI Master and Slave RTL IP CoreHardwareAVPSMSMFDIF RECORDERAWGSCALABLE DASAVAGMulti-Format Video Display/Monitor\nResourcesBlogGlossaryWhitepaper Application Notes Case Studies Infographics Videos\nAbout UsCompany Overview Mission & Vision leadership News Events Career Partners Contact Us\nAn ISO 9001: 2008 Certified CompanyVideo ProcessingCamera Sensor Technologies  An Overview [2023]\nIntroductionWith the development and spread of consumer electronics with imaging capabilities, camera image processing is becoming increasingly important Camera Sensor integration, image sensor integration, and camera image processing methods are widely utilized in various applications, from consumer to computer vision to industrial, defense, multimedia, sensor networks, surveillance, automotive, and astronomy.For machine vision applications, such as the inspection of flat panel displays, printed circuit boards (PCBs), and semiconductors, as well as warehouse logistics, intelligent transportation systems, crop monitoring, and digital pathology, the development of industrial camera sensor technology and image sensors has resulted in new demands for compact cameras and image sensors.What is Camera Image ProcessingCustomers building next-generation camera sensorproducts for various applications may rely on Camera Image Processing to provide the best solutions.The current generation of intelligent devices, which represent a quantum jump in sophistication, is made possible by camera competence, including camera integration, camera image processing, CMOS image sensor tuning, and other related capabilities.We ensure IS and IQ tuning addressing enhances HDR, minimizes Low light, optimizes AE, AWB, color accuracy, tone curves, contrast higher resolution, objective metrics, subjective testing, and field and drive testing as part of our experience in camera sensor technology tuning and camera image processing.Working of Camera Sensor IntegrationCamera sensor Integration is the period of time when the cameras clocks are configured to trap and hold a charge. The behavior of the readout electronics serves as the integrations boundary, which is totally unrelated to the shutters exposure.The incident light (photons) are focused by a lens or other optics and are received by the image sensor in a camera system. The sensors ability to transmit data as either a voltage or a digital signal to the following stage will depend on whether it is CCD or CMOS.Detailed explanation of the Camera/Image sensorThe choice of the appropriate camera sensor has become extremely important and varies from product to product because cameras have a wide variety of uses in many industries.Therefore, in addition to having a lot of pixels, factors like drive technology, quantum efficiency, and pixel size structure all impact imaging performance in different ways. Nowadays, charge-coupled devices(CCD cameras) and complementary metal oxide semiconductor technology (CMOS) imagers make up the majority of sensors.Due to inadequate illumination and other environmental factors, the captured image may contain some unnecessary elements despite each specific sensor, processor, and lens combination.As a result, the raw image would need a lot of processing to produce a high-quality image.The image processing sector is currently one of the global businesses with the fastest growth rates, and as a result, it is a crucial area of engineering study. ISP, which is either externally integrated or embedded within the video processor, does image processing. An image signal processor (ISP) is a processor that receives a raw image from the image sensor and outputs a processed version of that picture (or some data associated with it).To deliver a high-quality image for a specific camera sensor and use case, an ISP could carry out several procedures, including black level reduction, noise reduction, AWB, tone mapping, color interpolation, autofocus, etc.Obtaining the ideal image or video quality is tricky for each use scenario. A lot of filtering and iterations are necessary to attain a desirable outcome.To evaluate the camera sensors tuning, image quality, and image resolution, various types of labs tools are required, such as;ISO Resolution Charts & Color Tests under controlled lightingTest Charts  Lens Distortion, Lens Shading & AWBLight Booth  to create different light conditions for sharpness & contrast.Chroma Meter & IR Source and IR Power MeterGreyscale & Color ChartOverview of Types of Camera SensorsFor sensitive, quick imaging of a range of samples for several applications, quantitative scientific cameras are essential. Since the invention of the first cameras, camera technology has developed significantly.Todays cameras can push the boundaries of scientific imaging and enable us to view previously invisible things.The cameras beating heart is the photons, electrons, and grey levels used to create an image on the sensor.The various camera sensor types and their features are covered here, includingCharge-coupled device (CCD)Electron-multiplying charge-coupled device (EMCCD)Complementary metal-oxide-semiconductor (CMOS)Sensor FundamentalsThe transformation of light photons into electrons is the first process for a sensor (known as photoelectrons). Quantum efficiency (QE), which is displayed as a percentage, is the efficiency of this conversion.All electrons have a negative charge that underlies the operation of all the sensor types covered here (the electron symbol being e-).This implies that positive voltages can attract electrons, making it possible to move electrons across a sensor by applying a voltage to particular sensor regions.The Above Image explains, how an electron charge is moved through a sensors pixels. A pixel (blue squares) is struck by photons (black arrows), which are then transformed into electrons (e-), which are then stored in a pixel well (yellow).These electrons can be moved pixel by pixel anywhere on a sensor by employing a positive voltage (orange) to transfer them to another pixel.On a sensor, electrons can be carried in any direction in this way, and they are often moved to a location where they can be amplified and turned into a digital signal, which can then be presented as an image.Every type of camera sensor, though, experiences this process differently.CCDThe first digital cameras were CCDs, which have been used in scientific imaging since the 1970s. For years, CCDs were actively used, ideal for high-light applications like cell documentation or imaging fixed samples.This technologys lack of sensitivity and speed constrained the number of samples that could be scanned at acceptable levels.CCD FundamentalsAfter being exposed to light and changing from photons to photoelectrons in a CCD, the electrons are transported down the sensor row by row until they reach the readout register, which is not exposed to light.Photoelectrons are transferred simultaneously into the readout register and the output node. They are then delivered to the computer via the imaging software in this node after being amplified into a readable voltage and transformed using an analog to digital converter (ADC) into a digital grey level.The above image explains, How a CCD sensor works. Photons hit a pixel and are converted to electrons, which are then shut down the sensor to the readout register, then to the output node, where they are converted to a voltage, then grey levels, and then displayed with a PC.The above image explains the Different types of CCD sensors. The full-frame sensor is also displayed. Grey areas are masked and not exposed to light.The frame-transfer sensor has an active image array (white) and a masked storage array (grey), while the interline-transfer sensor has a portion of each pixel masked (grey).The camera technology can be quantitative since the a linear relationship between the number of electrons and photons. A full-frame CCD sensor is a kind shown in Figure 2, although there are also additional designs known as frame-transfer CCD and interline-transfer CCD.EMCCDThe Cascade 650 from Photometrics introduced EMCCDs to the scientific imaging market for the first time in 2000. EMCCDs provided quicker and more sensitive imaging than CCDs, making them ideal for photon counting or low-light imaging devices.This was accomplished in several ways via EMCCDs. The cameras back illumination (which raises the QE to over 90%) and massive pixels (16-24 m) significantly raise their sensitivity. The EM in the EMCCD, or electron multiplication, is the most important addition.EMCCD FundamentalsElectrons go from the image array to the masked array, then onto the readout register in a manner that is remarkably similar to frame-transfer CCDs.The EM Gain register now becomes the primary point of distinction. Impact ionization is a technique EMCCDs to drive more electrons out of the silicon sensor size, doubling the signal.Users can select a number between 1 and 1000 to have their signal multiplied that many times in the EM Gain register as part of this step-by-step EM process. When an EMCCD receives a signal of 5 electrons, and the EM Gain is set to 200, the output node will receive a signal of 1000 electrons.Due to its ability to be multiplied up above the noise floor as many times as needed, EMCCDs can now detect tiny signals.The above image explains, How an EMCCD sensor works. Photons hit a pixel and are converted to electrons, which are then shuttled down the sensor integration to the readout register.They are amplified using the EM Gain register, sent to the output node, converted to a voltage, grey levels, and then displayed with a PC.EMCCDs are far more sensitive than CCDs thanks to the combination of large pixels, back illumination, and electron multiplication.EMCCDs are quicker than CCDs as well. Because the speed at which electrons are moved around a sensor increases read noise, CCDs move electrons much slower than their maximum potential speed.Every signal has a set +/- a value called read noise. For example, if a CCD detects a signal of 10 electrons with a read noise of 5 electrons, the signal could be read out at any value between 5 and 15 electrons, depending on the read noise.As a result, CCDs transport electrons more slowly to lessen read noise, significantly impacting sensitivity and speed.CMOSAlthough MOS and CMOS technology has been around since the 1950s, well before the development of CCD, it wasnt until 2009 that CMOS cameras reached a quantitative level sufficient for scientific imaging. For this reason, CMOS cameras for science are sometimes referred to as scientific CMOS or sCMOS.The fundamental difference between CMOS technology and CCD and EMCCD is parallelization; because CMOS sensors work in parallel, substantially greater speeds are possible.CMOS FundamentalsEvery pixel in a CMOS sensor contains miniature electronics, including an amplifier and a capacitor.This implies that the pixel converts a photon into an electron and that the electron is then instantly changed into a readable voltage while still on the pixel.Additionally, each ADC has to read out considerably fewer data than a CCD/EMCCD ADC, which must read out the complete sensor because there is an ADC for every column. Compared to CCD/EMCCD technology, this combination enables CMOS sensors to operate parallel and analyze data more quickly.As CMOS sensors have a far lower read noise than CCD/EMCCD, they can work with weak fluorescence or live cells and move electrons much slower than the projected maximum speed. This enables them to conduct low-light imaging.The above image explains, How a CMOS sensor works. Photons hit a pixel, are converted to electrons, and then to the voltage on the pixel. Each column is read out separately by individual ADCs and then displayed with a PC.ConclusionTo provide the optimum speed, sensitivity, resolution, and field of view for your sample on your application, scientific imaging technologies have continued to improve from CCD to EMCCD, sCMOS, and back-illuminated sCMOS.By selecting the best camera manufacturers technology for your imaging system, you can enhance all aspects of your studies and conduct quantitative research. While CCD and EMCCD technologies were popular for scientific imaging, sCMOS technology has emerged in recent years as the best option for imaging in the biological sciences.Related Articles\nOnly the best of the blogs delivered to you monthly\nEmailBy submitting this form, I hereby agree to receive marketing information and agree with Logic Fruit Privacy Policy.\n5Gs Impact on Telecom Industry: Advantages, Challenges, Applications, & Advancements.May 19, 2023 | By\nThe Unmanned Aerial Vehicles (UAVs) in the Defense IndustryApril 28, 2023 | By\nRole of FPGAs in High-Performance ComputingApril 21, 2023 | By\nDelivered Fortnightly. Only the Best of the Blogs for You!\n806, 8th Floor BPTP Park Centra Sector30, NH8 Gurgaon  122001 Haryana (India)\nSy. No 118, 3rd Floor, Gayathri Lakefront, Outer Ring Road, Hebbal, Bangalore - 560 024\nLogic Fruit Inc. 862 E Evelyn Ave, Sunnyvale, CA 94086, USA\nCopyright  2021 Logic Fruit Technologies. All rights reserved.\nAll product and company names are trademarks or registered trademarks of their respective holders. Use of them does not imply any\nMessageBy submitting this form, I hereby agree to receive marketing information and agree with Logic Fruit",
    "Virtual (Live) Robot Safety for Collaborative Applications Training\nDownload the A3 Artificial Intelligence Applications Whitepaper\nVirtual (Live) Robot Safety for Collaborative Applications Training\nDownload the A3 Artificial Intelligence Applications Whitepaper\nThe size of a digital cameras image sensor is one of the most important specs in determining overall performance. Even the simplest machine vision and imaging devices have these sensors; they would be useless otherwise. But what are image sensors, exactly?\nWhat Makes a Digital Camera a Camera? The Image Sensor is King\nAn image sensor is a device that allows the camera to convert photons  that is, light  into electrical signals that can be interpreted by the device. The first digital cameras used charge-coupled devices, facilitating movement of the electrical charge through the device so it could be modulated. They were invented in 1969 at Bell Laboratories, an unexpected result of semiconductor research.\nUsing similar technology, the first commercial digital camera came about in 1973. It was a Kodak product, but never reached the mass market at the time. Nonetheless, the development of more sophisticated image sensors continued apace. Todays small, yet powerful silicon-based image sensors show up in all kinds of devices: Scanners, mobile phones, webcams, and more.\nA Look Under the Hood at the Inner Workings of the Image Sensor\nThe bigger your sensor, the better your image  but how does it work?\nMost digital cameras now use a standard configuration: A 2D Bayer arrangement of RGB color filters sits atop a pixel array. Each pixel has a photodetector and absorbs filtered light for one of the primary colors. In addition to RGB, there are specialized or legacy filter arrangements like CYGM.Related Blog Articles\nThe Latest Machine Vision Standards Promote the Growth of Machine Vision Technology\nAfter light strikes the photodiodes in the image sensor, the picture is converted into electrical signals and pushed on to several internal devices in turn: A serial shift register, capacitor, and amplifier. Finally, the analog-to-digital conversion takes place. This transforms voltage signals into binary values that can be processed and stored.\nAlthough there are many kinds of image sensors, CMOS sensors have captured the imagination of todays vision system engineers. CMOS  Complementary Metal Oxide Semiconductor  allows the data of each pixel to be read individually. This provides the most sophisticated and granular level of control over the image. These image sensors can perform well at very small sizes, so they appear in camcorders, smartphones, and many other portable applications.\nThere are several specialized image sensors, but until recently, most have been variations on the CMOS. Now, innovative technologies such as Back-Illuminated Sensors are starting to appear. These enhance an image sensor's performance by increasing the amount of light it can capture.\nMany components play a role in how an imaging system works, but the image sensor is perhaps the most important. With sensors growing more precise daily, the standards of quality for consumer and industrial imaging have never been higher.\nUpskilling the Workforce in Trained AI Expertise with General Motors' Jeff Abell\nEnd-of-Arm Tool Innovations in Industrial Automation\nMachine Vision Trends and Advancements in Industrial Automation\nAI in the Factory of the Future: 5 Ways Machine Learning and AI Can Accelerate Manufacturing Outcomes to Scale\nCopyright  2023 Association for Advancing Automation\n900 Victors Way, Suite 140, Ann Arbor, Michigan, USA 48108\nWebsite Design & Development by Amplify Industrial Marketing + Guidance",
    "Camera and Image Sensor Technology Fundamentals - Part 1\nVirtual (Live) Robot Safety for Collaborative Applications Training\nDownload the A3 Artificial Intelligence Applications Whitepaper\nVirtual (Live) Robot Safety for Collaborative Applications Training\nDownload the A3 Artificial Intelligence Applications Whitepaper\nCamera and Image Sensor Technology Fundamentals - Part 1\nCamera and Image Sensor Technology Fundamentals - Part 1\nPart of the AIA Certified Vision Professional-Basic program, Steve Kinney, Director of Technical Pre-Sales and Support at JAI, Inc., teaches the fundamentals of camera and image sensor technology. You'll gain an understanding of camera design including CCD and CMOS sensor technology.\nSubscribe for access to Advanced Certified Vision Professional Training\nLocate a Certified Vision Professional exam location and test your vision training knowledge.\nIm Steve Kinney Dir. technical sales and support for JAI Im also KIA camera link Committee Chairman Im electrical engineer who began my career in the United States Air Force as an avionic inertial navigation specialist Ive done several years in beginning in 1990 back to Silicon Valley in product design and development and began my career in machine vision with the camera company in 1997 my years in the Air Force along with product development to be a broad perspective on all types of customer situations equipment Weatherby pneumatic or electrical and can be broad base of experience for helping customers and occasions today Ill be talking to you about the fundamentals of Cameron image sensor technology teaching the basic course for the EIA CVP may speak in four segments today beginning with light basics and CCD CMOS imager fundamentals continuing to digital camera principles interfaces and finally camera types and when to use them to begin with the light basics today light is a piece of the electromagnetic spectrum which includes everything from longwave radio through your TV microwaves finally moving into the light bulb infrared visible and ultraviolet and continuing on through gamma rays and x-rays I was quite this out because a lot of the students dont recognize that flatlined is in fact an electromagnetic wave in the same as radio and one of the key points to be made here is light is actually a very narrow band we look at the total spectrum of RF energy here light is a very narrow band and we talk about applications where she talk about dividing the light up into on finer detail from near infrared to the visible section to the ultraviolet and rectory slicing the narrowband into even knocked narrow pieces to work on her applications here were primarily interested in wavelengths of light from 200 to 1100 nm with that the bulk about being visible light from 400 to 750 nm this is the energy that your lights that your IC as visible light and that most cameras focus on for imaging but for machine vision we also see applications in the near UV from 200 to 400 nm and sometimes near infrared light is also use from seven 52,000 or 1100 nm light is represented is both a particle and electromagnetic way like particles called a photon photons have some energy in the amount of energy in the photon determines the wavelength so since the speed of light is a constant if there is more energy and light vibrates faster and it becomes a blue wavelength wears up as well-written energy in it it will vibrate slower and be a red wavelength the wavelength corresponds to the colors I just described and intensity light is represented by the number of photons being emitted by a surface in the color light is the wavelength of so all said sensors in the cameras then worked on was called photoelectric effect photons are converted to electrons like hitting a silicon surface world this large electron when the light hits it feels at all the valence orbit and the electrons dislodged and is held on the imager in some fashion counted in some fashion which will discuss shortly the number electrons then released you can on the intensity and the wavelength of light for sensors we often see the quantum efficiency home represented on the data sheets quantum efficiency is the ratio that the racial light that the sensor converts into a charge so 60% quantum efficiency means that for every 10 photons hitting a pixel surface the six are converted into charge QB is sensor specific camera design does not affect you recur so these curs that you see here are given to us by the sensor manufacturers that we use in the cameras and while we want to do good things electronically in the camera to maintain the best from the sensor nothing we do in the camera can change the quantum efficiency or the response lessons are too white bomb she really is also given in absolute or relative terms the sensors that are showing here youll notice a saint absolute quantum efficiency this means they are absolute in that effect they just talked about so if you have 60% QB then that means six out of every 10 photons are hitting the surface and being registered on the sensor some sensor manufacturers give quantum efficiency and was called relative terms relative quantification efficiency simply means that the manufacturer takes the peak of the curve whatever that peak may be and they normalize a 200% so and these curves there is the sensors slightly over 50% so this manufacturer would take a multiple of just under to multiply this peak talking to 100% and then redraw the curve multiplied by that factor relative QE is good for that sensor only so that you can tell as I changed wavelength like coming in or if I were using monochromatic sources how much response I get from that since only however because its been normalized by a factor with unknown to the user you can never used relative quantum efficiency for comparing one sensor to another even if theyre from the same manufacturer the full well capacity of the sensor or of the pixel is a number electrons that register any specific in a pixel larger pixels have a higher well capacity which also tends to Lynn Lynn lead to higher sensitivity better signal-to-noise and increased dynamic range so these are typical these are not absolute but if we look around it. Sensors that we find in the industry very small pixel sensors helpful will capacities around 4000 electrons and more medium-sized pixel women have well capacities more like 10,000 electrons in larger pixels can hold 50,000 or more electrons so that what I talk about applying somebodys basics and talk specifically about CCD and CMOS sensors whats going on with them how their converting the light to the charge and what they can do for us in imaging the main difference between CCD and CMOS sensors is how they chat transfer charge out of the pixel and read it out of the camera into the machine vision system CCDs recharge and call off the face CMOS imagers click charge in the pixel in an readout palm circuitry in the axle lets it read it all the camera when we think of the CCD sensor we can think of it as a bucket brigade we can imagine a CCD pixel is a bucket collecting on why much as a bucket out in the open would collect rainwater when it rains and one satellite is collected a CCD type sensor then shifts that light on the surface of the sensor and out of the sensor into the camera body so CCD stands for charge coupled device thats important because charge coupled device means a CCD imager is our current driven devices the charges physically collected as a number of electrons in those electrons are collected in pixels and moved on the sensor surface the charges physically shifted around by various voltage barriers and as the errors are dropped charge to be moved from pixels and the shift registers and then clocked out the camera so when the cameras beginning to read a frame drops all the charge vertically into a horizontal shift register at the bottom of the imager reads out the horizontal shift registers and a line in the image and increase another vertical shift from all the charges down one more into the horizontal shift register reading those out and so forth the last line is made to the horizontal shift register and is clocked out again much like the bucket brigade on the previous slide dispersed in first out type situation the CCD output pin is an analog holes with the charges proportional to light intensity so when you readout this sensor a CCD is a quasi-analog device the doublet produces actually analog blip even those being controlled and read-out with digital pulses to control the timing and the readout CCD spy nature quasi-analog micro lenses increase the photon collection and of the area of the pixel focus of photons into the photosensitive area of the pixel so well talk about this even more and CMOS to both CCD and CMOS imagers you have a pixel area but the pixel area has to include other home structures readout charge all the pixel and controlled the pixel on shutter time and read on all the other things to get the information from the pixel into the camera that means by definition that the whole pixel cannot be photosensitive to electrons means theres only a small area usually call the photodiode that is sensitive to the photons coming in and the rest of the structure is then not photosensitive so to increase this photosensitive area and to increase the quantum efficiency overall per the curves I showed CCDs tend to use a micro lens over the surface the micro lens can be thought of is just a big magnifying glass over that the pixel area essentially the manufacturers able to create this over every pixel and all thats happening is the wife is coming in at oblique angles that would not normally strike photosensitive area would come into maybe strike any a shift register something are bent through the magnifying glass to be focused down into the pixel area almost all modern CCDs use micro lenses theres very few exceptions there are some application exceptions especially in the UV work micro lens is undesirable but home is a starting place almost all visible CCDs use micro lenses today the Pro is a effectively increase the quantum efficiency of the pixel the con is that they create an angular sensitivity to the incident light right so even though the thing the liking come in at a steep angle be bent in the photosensitive area is not perfect and we can see it roll off in the response of the pixel to light with the angle of incidence of the light to the pixel itself to what you see in this diagram is that if this is a Lanza and this is a CCD service year raise coming on axis through the center lands and being focused on the pixel are then hitting the pixel essentially on axis and code went here and the micro lens can very effectively been all the raise into the center of the photosensitive area on the pixel however if we look at a peripheral way coming in at a steep angle and will see that that the pixel in this area of the sensor has to bend all the raise coming in and that one can do this efficiently is some of the steepest raise coming steep angle even through the mic lines will be bent in a way that it cant quite make the sinner photosensitive area this means that the quantum efficiency is then expected by the angular incidence of the rate itself so we can see this is actual chart for a Kodak key hi 340 CCD will see a special end of vertical that the CCD has a very high efficiency with angle it is angle increases beyond about 10 that the efficiency begins to fall off the wall also notice they give this to us in terms of a vertical efficiency and that horizontally there is a different efficiency and that in fact that the this particular imager is more sensitive the horizontal shading across the horizontal on of its surface and this is due to the fact that the photosensitive area is not always in the center of the pixel and the fact that it may not be square fact most cases is rectangle and thus you see that there is a larger vertical collection area than a horizontal collection area and that we talk about these micro lens of fact it affects these differently so applications that are very dependent on uniformity and shading need to pay attention to this because the sensor cell will have a uniformity response but assuming all the raise on on axis and that if we had a lens to the system and it brings raisin out of acts off axis the.in fact will affect the uniformity and in general the wider the angle of field of view of the lens steep results raise on their peripheral and more vertical and horizontal shading we are going to see micro lenses increase the pixel effective area this is often called Phil factor and they produce a high Phil factor horizontal lines are shifted down the surface of the sensors we described pixels are read-out horizontal shift register through a common circuit including an amplifier to give us a pixel readout so the main advantages of CCDs are sensitivity the main disadvantage is then speed this comes back to what I was saying about this the charge being rolled on the surface of the device because the CCD is a charge coupled device were physically collecting those photons in the pixel dropping the bear and actually moving those that charge along the phase of the CCD for those of you that have electronic engineering background to remember certain number electrons make a coulomb 1 C flowing in one second is one hamper current and thus we talk about CCD sensors that are holding 50,000 electrons in a pixel and may have the 10 million pixels there is actually a large current flowing on the surface of the CCD Ellie of course trying to the resistance alone keep that power down but by nature you are moving current on the piece of the CCD and that is thus developing power on the CCD face itself for something like the Kodak four megapixel sensor theres roughly half an amp flowing to readout 4 million pixels 15 times a second and a half an amp again and then makes a little over a want on the face of the surface of the sensor and again besides the powered the disadvantage that is theres a speed at which you can move those electrons around on the CCD surface other issues for CCD is begins with blooming blooming is spread charge to adjacent pixels due to saturation of pixels especially in cases where you have a bright light on in this case the sunlight in the image and you may be using a high-speed shutter theres a limit to how many photons hit the surface in the CCD can either readout through the shutter. Wars discharging those electrons not using the shutter. But theres a limit to the amount of photons that can be discharged and when they begin to oversaturated and spread to adjacent pixels and and make blooming issues such as you see in this image similar to blooming theres smearing or vertical streaking and this is also caused by the pixels being saturated light spilling over into the next column of pixels so essentially whats happening here is like spilling over but as I described is vertical and horizontal shift registers a light spilling into a vertical shift register so even though that this is the hot pixel this is hot pixel last year on these other pixels that have discharged when a role because the whole images being shifted vertically down the phase then it rolled into shift registers that were used by other pixels and continued to streak in there this is why you get a vertical shift are the streak and youll notice even on the bright side is called as lupus smearing going on here as well from the daylight now a CMOS sensor then works a little bit differently CMOS stands for complementary metal oxide; conductor CMOS technology usher came from the development RAM chips so in the early days of RAM for computers and even nowadays sometimes you still see him erasable EEPROMs and those the problems on were sensitive to UV light so engineers can program around that they wanted to use with certain instructions load them in the computer and they wanted to change it they would expose an area in the RAM to UV light he race it and start over and someone early on recognized that hey these these registers arranged in array on on a chip ended if this thing were looking at why I might actually be able to create an image so theres been deviation of course CMOS imagers are optimized for imaging not storage RAM data and vice a versa but the technology really came from the development in the early days of the computer CMOS imagers are voltage driven devices and this is important so a CCD we talked about collecting charge enrolling the charger all surface CMOS imager light striking the pixel creates a voltage for portions of the proportional to the intensity so the physical effect is the same they light is still knocking a electron off in the silicon but in the case of the CMOS imager that that electron is making a voltage in the well as opposed to a number electrons are being read out which is the current in the well the voltage is sampled directly at the pixel digitized on the image imager and cleared for the next frame is never completely rolled around on here there are issues stated to clearly imagers to dump the ground but essentially there not moving the charge round in the same way its a voltage driven device consequently the CMOS imager unlike a CCD imager has a totally digital output by time the images coming off of the imager itself is read-out is a a binary number which will talk about and is a digital sample coming off the sensor so see CMOS cameras are totally digital devices were the CCD camera contains a quasi-analog section and in on the manufacture may have more or less analog circuitry and some CMOS works by having the ability to have multiple layers as a comp literate complementary metal oxide semi conductor which means they can have complementary layers building devices on on the sensor cell this is called stack up as they build these layers what is typically happened then is CMOS imagers because the stack up we have a photosensitive well down in the bottom of the active area but because its built with layers is literally a photosensitive area and a well which has some depth and because of this structure then CMOS imagers typically have not use micro lenses youll notice in the latest generation is CMOS imagers and some of the Maxi advertise now that they have a micro lens and that there increasing the sensitivity do this might lens again helps increase the quantum efficiency which in turn helps increase the sensitivity but these are newer developments is CMOS and coming from manufacturers who have actually made work on thin metal films to make these layers thinner on so you can see today on CMOS imagers both with and without micro lens starting point for CMOS as it usually does not have micro lens and Les is something designed for higher in damaging which the manufacturers taken special steps to all limit the depth of these layers and install microloans so between not having a micro lens and some physics involved in the in the silicon on CMOS usually has a lower sensitivity than CCD just buy that the nature of this is going on the bike went help schedule backup but theres still some other physics that that usually make it a low but lower national charge converter CMOS also because the images digitize on the sensor tends to have an active amplifier either on every pixel or least on on columns are multiple columns and the structures and repeated on throughout the sensor in order to get the active readout not have to flow the charge around the downfall to that in will see some examples of this coming up with the downfall to that is every a duty amplifier has a little bit of variation is Wellstone random thermal effects and that fixed variation between all these massive parallel port of ADD circuits and amplifiers causes a fixed pattern noise in the amateur because is not changing the gain of this AV circuits always little higher than the one next to it and thus not changing and despite manufacturers working to keep these uniform that are never perfect and so typically AAC one of the key noise sources and is CMOS imagers fixed pattern was in the background however because again there not rolling the charge around CMOS imagers tend to be more resistant to smearing her blooming than a CCD so voltage sampling is faster than rolling charge a CCD this again is akin to just taking her cell scope probative or high impedance of reading and checking what the voltages instead of rolling the charge out trying to roll through a correlated double sampling circuit and read what the charges are literally count electrons in a CCD West lower charge means less power on voltage sampling versus Royal charge round means more speed so the main advantages of a CMOS sensor over a CCD sensor then tend to be speed and power consumption and you can see this even in the consumer digital camera market of the most early digital cameras tend to be CCD they tended to be lower pixel counts and lower quality on what we have today is the market was evolving an in order get sufficient quality that typically use CCD that meant battery life and other things as these consumer manufacturers of pushed on the CMOS development they got sensors which are more than good enough for the consumer market and then the advantage of the battery life especially with flash and other than comic pictures I can take takeover and today you see almost exclusively in the consumer market for CMOS imager in the machine vision market were concerned with a very high quality and often dynamic range of Wallys other minor effects compared to a consumer camera and you see the CCD stop the place for quality and high dynamic range and very low sensitivity in these types applications that we see CMOS imagers been use more more today on because of the advantage of architecture and because of on advancements in the technology itself making them were more equal but there still trade-offs between CCDs and CMOS imagers to be considered by the user theres no one clear answer the main disadvantages then as I mentioned a CMOS tend to be sensitivity and pattern noise worth X related to the so among one of the other issues for CMOS imagers is also rolling shutter and I have a diagram which shows will more clearly but essentially in a CCD as I showed on everything is done on the surface of an airline transfer CCD which means the second substrate below that can be an entire groundlings a CCD imager generates a a electronic shutter by simply being able to take all those pixel wells and ground them to substrate all once the substrate is large capacity compared to the pixel capacity enabled them very quickly discharge a very large amount of current and make a complete on charge efficient discharge of all the pixels very uniform in CMOS then they have to get rid of the charge the same way but all they have to do little bit differently the starting point for CMOS is than rolling shutter this means that its only good a readout expose one line read-out line that exposes the\nreadout outline so if these images were equal resolution and they both had 1000 vertical lines with the CCD and electronic shutter all 1000 lines are exposed simultaneously in the motion is frozen simultaneously from the top of the image the bottom there is no temporal time difference in the exposure from the first line to the last line in a fact this motorcycles moving at over 160 miles an hour and we can see the spokes in the holes in the CalPERS and everything is perfectly frozen here the CMOS imager if this hat frame had 1000 lines theres 1000 different exposures that took place in the frame kinds of the of the frame time is relatively facile say this was 100 frame per second fast CMOS camera is still 100th of a second to take the frame and within the hundreds of the second is still literally 1000 exposures that occurred now when we look at this this camera was taken out of a moving car were someone like the scene is to picture out of moving car Im a people really think that this railing was built slanting forward in the direction of movement of the car it of course was not and of course is the fact where this book of the pixels made bar read-out at one time and is the car was moving forward in the\ntends to make everything shift in the slanted direction the other thing I will point out what we have this image up here is that the excellent rolling shutter proportional to the velocity in pixels per second and of course velocity pixels per second for the pixels are close to moving cars much higher than the background so you notice the rolling shutter effect is very high on something close to the car where the Lord perspective movement and something like this rail is compared to the island in the background is relatively unaffected for this because its a very wide field of view and there is very few pixels per out there would be pixels for hundreds of feet this is a diagram showing largest described so this is a CMOS issue and essentially whats happening is this is an integration. So it integrates work. In the readout outline that comes down integrates\nthe readout outline and as you can see it in their timing diagram the society of the Bilbo timing diagram and this is exactly what happens to your image distortion was the only caveat is that if you can have in a machine vision application if you can have an entirely black background was no external light leakage and he could set off the strobe is a chance to freeze the motion in a rolling shutter type of scenario with a strobe but again you have to have a very controlled circumstances but no wine kit in other parts of the area where the line is photosensitive but you do not want to see the integration of that life so coming back to our example if we take an image of an object moving horizontally through our field of view with a CCD camera and global shutter for CMOS camera global shutter whether CCD electronic shutter CMOS as global shutter the object will be frozen that will be one exposure for the entire frame but is CMOS imagers with a rolling shutter then again there are multiple home exposures within the frame and we see a displacement where the displacement is proportional to the horizontal velocity and pixels for time through the frame and again this is showing the same image coming back to the beginning and we can see the effect in the bars because they are moving very fast passed a car that is very close to them and they again in the distance you dont see it because velocity in pixels per second is much less when we talk about CCD and CMOS imagers and this comes back to mostly CCDs in the days of TV interlaced scanning but there is progressive and interlaced scanning methods for readout and cameras there are some interlaced CMOS imagers on the work developed early on to intentionally mimic the TV interlaced formats but for the most part interlaced scanning tends to be a a nature of his CCD device so the image from the cameras for my sequence of pixel line scanned and displayed in one of two different ways from regard or progressive scanning means all the lines are scanned and exposed at the same time and then read out for interlaced scanning it means there is exposure and then a readout of the odd-numbered lines and then correspondingly theres a second exposure and readout of the even-numbered lines and the odd in the even-numbered lines and fit together to make a complete frame so this again comes back to and you see several things come back to the early days of TV scanning here so interlaced scanning was used in normal TV systems which were developed all the way back late 30s early 40s and whats essentially happening here is at the time the technology was developed and they wanted to broadcast it over radio frequencies we didnt have HDTV and satellite at the time this technology was developed they literally couldnt capture all the pixels broadcast them and re-create them would take too much bandwidth so the what what the book essentially the Americans came up with in the early days of TV and the NTSC systems was interlaced sees step back and get everything into a high-bandwidth signal and read the mouth and then re-create it with high speed scanning on a CRT when your TV in 1940 was disbanded on the large magnetic devices inside they simply slowed the scanning rate in half read-out half the number of pixels so your TV camera within capture as I described the odd lines shown in gray here and were captured lines 1357 911 a M, shutter freeze them but then it would readout windows locked in a would capture her second exposure shown by the black line 0 the even-numbered lines 246 and a and so forth through the imager freeze them and then read those out and this would give your TV to scanning time to scan in the same matters in the TV I have the scan apparatus on up the odd-numbered lines of what they said oh now I go back and think lines in between so this helped on with the transmission and an the creation of the imager in the early days of broadcast that actually had one advantage to that and that was the fields that though the frame rate itself was only 31st the fields are treated as 60 Hz aware I have high-speed motion then that the field nature of this at 60 fields per second lettuce capture motion accurately because interlacing to feel the 60th of a second into a frame authority of the second floor watching on TV are eyes were relatively immune to this surprise could see both the motion and find the resolution even though there was some blurring and youll see the blurring occurs this is nice for TV and for eyes but in machine vision system were usually capturing the image because we wanted to some analysis on it and then the kinds of boring and effects a come from this interlaced or than bad to us in the machine vision sons so this is an example of the soccer ball that is been kicked with interlaced blurring in the sexual capture off the TV type of card and youll see the reason is spiky is because what I said have to lines are read out and then the other half are read and what you see is the leading edge lines here youll notice on the edges LOreal here on the sewer lines that were captured first and then the ball moved actually I should say this way the trailing edges captured first in the ball move forward this way and in the leading edge was capture and youre actually seeing the physical separation of what was a time together frame is now separated by the difference in the exposure and the movement of the ball so again for eyes we once and I had the ball that was kicked by a soccer player we would never see the pattern on the ball anyway we would discuss see the ball and Lucys movement but it were machine vision system and were starting to capture frames and do analysis on this is ugly from the machine vision if the machine vision were trying to do an object line and say put a line around the ball maybe test pattern on the ball to be having a hard time right now in progressive scan also called not nonelderly scanning one field equals one frame the solves this on in in progressive scanning there is only one exposures common to all the lines the lines within read-out as I described on the first pixels coming off the imager and sequentially all the way back to the top of the imager last pixels read-out the benefit to this then is that the full frame is available as a result of a single shutter event the downfall to this is that some nonstandard scanning systems and no longer does this progressive scan source plug-ins your TV or VCR or as any TV format camera plugged into any VCR any other device and not always a not plug-in but it means that if I use a frame grabber whatever it capture that nonstandard device with SB configured because once I leave the TV standard format there is no standard Gleason analog realm for the image that is created will talk a little bit about digital standards and whats happening later but as far as that of the video format of the coming off the can be anything if it says progressive scan you can tell what is happening if this is TV interlaced you know that there is just under half 1 million pixels therein in NTSC scanning in the US of the 768 x 494 active pixels and that there built into 21 interlacing we can build something if I tell you I have a one megapixel progressive scan you dont know anything more than theres 1 million pixels there I have to tell you that its a one-to-one aspect ratio is 0.125 x 1 K or that it might be 1200 x 800 and somewhat lines can be anything that nonstandard format of the so again this is just reiterating on how progressive and interlaced scanning this is what comes out the camera progressive and this is what the two fields of an interlaced scan look like in the same scene so far refrained grabbing on this will be captured from progressive scan camera and these two fields would each be captured from a frame grabber is fields in joint back to a frame so with interlaced scanning spatial resolution is reduced temporal resolution resolution over time is improved twice as many full images are presented per second with progressive scanning sharper images are formed with interlaced sulci smoother motion but also no centerline images in the image just like described on watching TV there I told you several things came back to the old days of TV and this is on image format is one of the things that we get a lot of questions on and the truth is comes back to the old days at TV a lot of people were confused by the fact that that optically whether were talking to the optics people about lenses of whether were talking to the camera people about the optical format of sensor in the camera lot of people confused by the fact that I wanted sensor has no bearing to 1 inch in physical size happened since her has no bearing to half-inch in physical size or doesnt the truth is a comes back from the old days when there were neither CCD or CMOS sensors on TVs were action TDs rushing made into just like the tubes that used to protect the TV on to your display on these three collected and they had a photosensitive area N/A electronic tube with a magnetic deflection yoke around it so what happened is what became 1 inch format was literally based on the physical dimensions of the 1 inch two and this was how much form photosensitive area fit inside the deflection yoke of a 1 inch tube in 1942 deaths we got 1 inch format and released two thirds half-inch one third even one quarters they came along and so when we say 1 inch format all this machine vision people at the distance of 43 aspect ratio being 12.8 mm x 9.3 or 16 mm on the diagonal and again lot of users and are confused as they think 1 inch format there expecting 25 mm diagonal are 25 horizontal or some something related in fact again this is the photosensitive area fit in the 1 inch to be the other thing thats interesting to note here that OPI p.m. and I began trying say that on which is the metric equivalent in Europe recommended use metric designation so when you see that the IPM designation youll note it actually matches the diagonal the format and like a lot of things that makes more common sense than the metric system villas that will talk real quick about is when smelt seamount in CS mount for machine vision cameras on we do see other amounts nowadays there are large amounts of balance in 42 and somethings come along with the very large imagers with the most common male on machine vision cameras is still by far seamount and within that the substandard tackle CS mount and this is a diagram showing whats happening with wins as a standard seamount camera so you have a focal plane and a focal plane is inside the camera separated by some distance to the female threads on the front the camera to which the lens threads into that distance for standard seamount is 17.526 mm CS mount then simply means that it is exactly 5 mm shorter on that back flange and it is only 12.526 mm CS mount taxi stands for was always told that it was security in the reason this is important is in the reason that it comes around for back flange is when you want a very wide angle lens than if we place laid the back of the lens closer to the focal plane makes it much easier to create it so this is why still CCS mount lenses out there and very white applicant wide-angle applications especially security then they abate for this type amount but we also find that couple calls a lot of confusion for customers guzzle of a CS mount camera and a seamount lens or vice versa the CS mount lenses seamount camera not sure what to do since its only physical difference of 5 mm then you have to think of is only in a physical sense so we can had a 5 mm we can take a 5 mm adapter ring any seamount lens and we can put it on a seam CS mount camera CS mount camera expected only 12  mm too short lenses designed by 17 so we can have a ring and get the right distance however what gets people is you cannot do the reverse of this because you cannot take it 5 mm adapter ring any CS mount lens and use in a seamount camera is a seamount camera was designed for 17 mm and you would need to remove not had 5 mm to make the CS mount lens work unless youre taking your camera to a machine shop youre not when you use the CS mount lens on your seamount camera the other thing one mention is these distances that we give our call our given in free air: effective distance this is the physical distance if it were a single lens in free air the cameras often have glass filters in between the perk protective windows on the imagers sell all you have filters for the manufactures color filters in color cameras and sometimes users actually screw filters in the bottom of seamount present but between the lens and the bottom of the threat on this is okay because you can adjust back to that 17 on .526 ideal primer that means again did the effective distance is always 17.526 physical distance maybe longer if I put piece of glass in there because a glass slows down the light relative to the speed of light in error and that causes the distance the lengthens slightly that all the calculations would still be based on 17 526 so in summary the number of photons hitting a pixel during exposure time creates a number of electrons in the pixel well which formally charge is converted by passer into voltage or read down the face of the CCD TV amplifies and then digitized resulting a digital gray value for that pixel CCD then is high image quality is typically lower speed CMOS lenses the higher speed the lower image quality and again pay attention to whether you have a global war rolling shutter situation and dont use rolling shutter for motion this concludes section 1 of the presentation\nCopyright  2023 Association for Advancing Automation\n900 Victors Way, Suite 140, Ann Arbor, Michigan, USA 48108\nWebsite Design & Development by Amplify Industrial Marketing + Guidance",
    "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\nHelpLearn to editCommunity portalRecent changesUpload file\nLanguage links are at the top of the page across from the title.\nAfrikaansCataletinaDanskDeutschEestiEspaolEuskaraFranaisItalianoNederlandsNorsk bokmlOzbekcha / PolskiPortugusSimple EnglishSloveninaSuomiSvenskaTrkeTing Vit\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item\nDevice that converts images into electronic signals\nAn American Microsystems, Inc., (AMI) 1-kilobit DRAM chip (center chip with glass window) used as an image sensor by the Cromemco Cyclops\nAn image sensor or imager is a sensor that detects and conveys information used to form an image. It does so by converting the variable attenuation of light waves (as they pass through or reflect off objects) into signals, small bursts of current that convey the information. The waves can be light or other electromagnetic radiation. Image sensors are used in electronic imaging devices of both analog and digital types, which include digital cameras, camera modules, camera phones, optical mouse devices,[1][2][3] medical imaging equipment, night vision equipment such as thermal imaging devices, radar, sonar, and others. As technology changes, electronic and digital imaging tends to replace chemical and analog imaging.\nThe two main types of electronic image sensors are the charge-coupled device (CCD) and the active-pixel sensor (CMOS sensor). Both CCD and CMOS sensors are based on metaloxidesemiconductor (MOS) technology, with CCDs based on MOS capacitors and CMOS sensors based on MOSFET (MOS field-effect transistor) amplifiers. Analog sensors for invisible radiation tend to involve vacuum tubes of various kinds, while digital sensors include flat-panel detectors.\nA micrograph of the corner of the photosensor array of a webcam digital camera\nImage sensor (upper left) on the motherboard of a Nikon Coolpix L2 6 MP\nThe two main types of digital image sensors are the charge-coupled device (CCD) and the active-pixel sensor (CMOS sensor), fabricated in complementary MOS (CMOS) or N-type MOS (NMOS or Live MOS) technologies. Both CCD and CMOS sensors are based on the MOS technology,[4] with MOS capacitors being the building blocks of a CCD,[5] and MOSFET amplifiers being the building blocks of a CMOS sensor.[6][7]\nCameras integrated in small consumer products generally use CMOS sensors, which are usually cheaper and have lower power consumption in battery powered devices than CCDs.[8] CCD sensors are used for high end broadcast quality video cameras, and CMOS sensors dominate in still photography and consumer goods where overall cost is a major concern. Both types of sensor accomplish the same task of capturing light and converting it into electrical signals.[citation needed]\nEach cell of a CCD image sensor is an analog device. When light strikes the chip it is held as a small electrical charge in each photo sensor. The charges in the line of pixels nearest to the (one or more) output amplifiers are amplified and output, then each line of pixels shifts its charges one line closer to the amplifiers, filling the empty line closest to the amplifiers. This process is then repeated until all the lines of pixels have had their charge amplified and output.[9]\nA CMOS image sensor has an amplifier for each pixel compared to the few amplifiers of a CCD. This results in less area for the capture of photons than a CCD, but this problem has been overcome by using microlenses in front of each photodiode, which focus light into the photodiode that would have otherwise hit the amplifier and not been detected.[9]\nSome CMOS imaging sensors also use Back-side illumination to increase the number of photons that hit the photodiode.[10] CMOS sensors can potentially be implemented with fewer components, use less power, and/or provide faster readout than CCD sensors.[11] They are also less vulnerable to static electricity discharges.\nAnother design, a hybrid CCD/CMOS architecture (sold under the name \"sCMOS\") consists of CMOS readout integrated circuits (ROICs) that are bump bonded to a CCD imaging substrate  a technology that was developed for infrared staring arrays and has been adapted to silicon-based detector technology.[12] Another approach is to utilize the very fine dimensions available in modern CMOS technology to implement a CCD like structure entirely in CMOS technology: such structures can be achieved by separating individual poly-silicon gates by a very small gap; though still a product of research hybrid sensors can potentially harness the benefits of both CCD and CMOS imagers.[13]\nThere are many parameters that can be used to evaluate the performance of an image sensor, including dynamic range, signal-to-noise ratio, and low-light sensitivity. For sensors of comparable types, the signal-to-noise ratio and dynamic range improve as the size increases. Its because in a given integration (exposure) time, more photons hit the pixel with larger area.\nExposure time of image sensors is generally controlled by either a conventional mechanical shutter, as in film cameras, or by an electronic shutter.\nElectronic shuttering can be \"global,\" in which case the entire image sensor area's accumulation of photoelectrons starts and stops simultaneously, or \"rolling\" in which case the exposure interval of each row immediate precedes that row's readout, in a process that \"rolls\" across the image frame (typically from top to bottom in landscape format).\nGlobal electronic shuttering is less common, as it requires \"storage\" circuits to hold charge from the end of the exposure interval until the readout process gets there, typically a few milliseconds later.[14]\nFoveon's scheme of vertical filtering for color sensing\nThere are several main types of color image sensors, differing by the type of color-separation mechanism:\nIntegral color sensors[15] use a color filter array fabricated on top of a single monochrome CCD or CMOS image sensor.\nThe most common color filter array pattern, the Bayer pattern, uses a checkerboard arrangement of two green pixels for each red and blue pixel, although many other color filter patterns have been developed, including patterns using cyan, magenta, yellow, and white pixels.[16]\nIntegral color sensors were initially manufactured by transferring colored dyes through photoresist windows onto a polymer receiving layer coated on top of a monochrome CCD sensor.[17]\nSince each pixel provides only a single color (such as green), the \"missing\" color values (such as red and blue) for the pixel are interpolated using neighboring pixels.[18]\nThis processing is also referred to as demosaicing or de-bayering.\nFoveon X3 sensor, using an array of layered pixel sensors, separating light via the inherent wavelength-dependent absorption property of silicon, such that every location senses all three color channels. This method is similar to how color film for photography works.\n3CCD, using three discrete image sensors, with the color separation done by a dichroic prism.\nThe dichroic elements provide a sharper color separation, thus improving color quality. Because each sensor is equally sensitive within its passband, and at full resolution, 3-CCD sensors produce better color quality and better low light performance. 3-CCD sensors produce a full 4:4:4 signal, which is preferred in television broadcasting, video editing and chroma key visual effects.\nInfrared view of the Orion Nebula taken by ESO's HAWK-I, a cryogenic wide-field imager[19]\nSpecial sensors are used in various applications such as thermography, creation of multi-spectral images, video laryngoscopes, gamma cameras, sensor arrays for x-rays, and other highly sensitive arrays for astronomy.[20]\nWhile in general, digital cameras use a flat sensor, Sony prototyped a curved sensor in 2014 to reduce/eliminate Petzval field curvature that occurs with a flat sensor. Use of a curved sensor allows a shorter and smaller diameter of the lens with reduced elements and components with greater aperture and reduced light fall-off at the edge of the photo.[21]\nEarly analog sensors for visible light were video camera tubes. They date back to the 1930s, and several types were developed up until the 1980s. By the early 1990s, they had been replaced by modern solid-state CCD image sensors.[22]\nThe basis for modern solid-state image sensors is MOS technology,[23][24] which originates from the invention of the MOSFET by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.[25] Later research on MOS technology led to the development of solid-state semiconductor image sensors, including the charge-coupled device (CCD) and later the active-pixel sensor (CMOS sensor).[23][24]\nThe passive-pixel sensor (PPS) was the precursor to the active-pixel sensor (APS).[7] A PPS consists of passive pixels which are read out without amplification, with each pixel consisting of a photodiode and a MOSFET switch.[26] It is a type of photodiode array, with pixels containing a p-n junction, integrated capacitor, and MOSFETs as selection transistors. A photodiode array was proposed by G. Weckler in 1968.[6] This was the basis for the PPS.[7] These early photodiode arrays were complex and impractical, requiring selection transistors to be fabricated within each pixel, along with on-chip multiplexer circuits. The noise of photodiode arrays was also a limitation to performance, as the photodiode readout bus capacitance resulted in increased noise level. Correlated double sampling (CDS) could also not be used with a photodiode array without external memory.[6] However, in 1914 Deputy Consul General Carl R. Loop, reported to the state department in a Consular Report on Archibald M. Low's Televista system that \"It is stated that the selenium in the transmitting screen may be replaced by any diamagnetic material\".[27]\nIn June 2022, Samsung Electronics announced that it had created a 200 million pixel image sensor. The 200MP ISOCELL HP3 has 0.56 micrometer pixels with Samsung reporting that previous sensors had 0.64 micrometer pixels, a 12% decrease since 2019. The new sensor contains 200 million pixels in a 2 x 1.4 inch lens.[28]\nThe charge-coupled device (CCD) was invented by Willard S. Boyle and George E. Smith at Bell Labs in 1969.[29] While researching MOS technology, they realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straightforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next.[23] The CCD is a semiconductor circuit that was later used in the first digital video cameras for television broadcasting.[30]\nEarly CCD sensors suffered from shutter lag. This was largely resolved with the invention of the pinned photodiode (PPD).[7] It was invented by Nobukazu Teranishi, Hiromitsu Shiraki and Yasuo Ishihara at NEC in 1980.[7][31] It was a photodetector structure with low lag, low noise, high quantum efficiency and low dark current.[7] In 1987, the PPD began to be incorporated into most CCD devices, becoming a fixture in consumer electronic video cameras and then digital still cameras. Since then, the PPD has been used in nearly all CCD sensors and then CMOS sensors.[7]\nThe NMOS active-pixel sensor (APS) was invented by Olympus in Japan during the mid-1980s. This was enabled by advances in MOS semiconductor device fabrication, with MOSFET scaling reaching smaller micron and then sub-micron levels.[6][32] The first NMOS APS was fabricated by Tsutomu Nakamura's team at Olympus in 1985.[33] The CMOS active-pixel sensor (CMOS sensor) was later improved by a group of scientists at the NASA Jet Propulsion Laboratory in 1993.[7] By 2007, sales of CMOS sensors had surpassed CCD sensors.[34] By the 2010s, CMOS sensors largely displaced CCD sensors in all new applications.\nThe first commercial digital camera, the Cromemco Cyclops in 1975, used a 3232 MOS image sensor. It was a modified MOS dynamic RAM (DRAM) memory chip.[35]\nMOS image sensors are widely used in optical mouse technology. The first optical mouse, invented by Richard F. Lyon at Xerox in 1980, used a 5m NMOS integrated circuit sensor chip.[36][37] Since the first commercial optical mouse, the IntelliMouse introduced in 1999, most optical mouse devices use CMOS sensors.[38]\nIn February 2018, researchers at Dartmouth College announced a new image sensing technology that the researchers call QIS, for Quanta Image Sensor. Instead of pixels, QIS chips have what the researchers call \"jots.\" Each jot can detect a single particle of light, called a photon.[39]\nImage sensor format, the sizes and shapes of common image sensors\nColor filter array, mosaic of tiny color filters over color image sensors\nSensitometry, the scientific study of light-sensitive materials\nHistory of television, the development of electronic imaging technology since the 1880s\nList of large sensor interchangeable-lens video cameras\n^ Lyon, Richard F. (August 1981). \"The Optical Mouse, and an Architectural Methodology for Smart Digital Sensors\" (PDF).\nIn H. T. Kung; Robert F. Sproull; Guy L. Steele (eds.). VLSI Systems and Computations. Computer Science Press. pp.119. doi:10.1007/978-3-642-68402-9_1. ISBN978-3-642-68404-3.\n^ Lyon, Richard F. (2014). \"The Optical Mouse: Early Biomimetic Embedded Vision\". Advances in Embedded Computer Vision. Springer. pp.3-22 (3). ISBN9783319093871.\n^ Brain, Marshall; Carmack, Carmen (24 April 2000). \"How Computer Mice Work\". HowStuffWorks. Retrieved 9 October 2019.\n^ Cressler, John D. (2017). \"Let There Be Light: The Bright World of Photonics\". Silicon Earth: Introduction to Microelectronics and Nanotechnology, Second Edition. CRC Press. p.29. ISBN978-1-351-83020-1.\n^ Sze, Simon Min; Lee, Ming-Kwei (May 2012). \"MOS Capacitor and MOSFET\". Semiconductor Devices: Physics and Technology: International Student Version. John Wiley & Sons. ISBN9780470537947. Retrieved 6 October 2019.\nBlouke, Morley M. (ed.). \"Active pixel sensors: are CCDs dinosaurs?\". SPIE Proceedings Vol. 1900: Charge-Coupled Devices and Solid State Optical Sensors III. International Society for Optics and Photonics. 1900: 214. Bibcode:1993SPIE.1900....2F. CiteSeerX10.1.1.408.6558. doi:10.1117/12.148585. S2CID10556755.\n^ a b c d e f g h Fossum, Eric R.; Hondongwa, D. B. (2014). \"A Review of the Pinned Photodiode for CCD and CMOS Image Sensors\". IEEE Journal of the Electron Devices Society. 2 (3): 3343. doi:10.1109/JEDS.2014.2306412.\n^ \"CMOS Is Winning the Camera Sensor Battle, and Here's Why\". techhive.com. 2011-12-29. Archived from the original on 2017-05-01. Retrieved 2017-04-27.\n^ a b \"CCD and CMOS sensors\". Canon Professional Network. Archived from the original on 28 April 2018. Retrieved 28 April 2018.\n^ \"What is a backlit CMOS sensor?\". techradar.com. 2012-07-02. Archived from the original on 2017-05-06. Retrieved 2017-04-27.\n^ Moynihan, Tom (29 December 2011). \"CMOS Is Winning the Camera Sensor Battle, and Here's Why\". Archived from the original on 25 September 2015. Retrieved 10 April 2015.\n^ scmos.com Archived 2012-06-03 at the Wayback Machine, home page\n^ ieee.org - CCD in CMOS Archived 2015-06-22 at the Wayback Machine Padmakumar R. Rao et al., \"CCD structures implemented in standard 0.18 m CMOS technology\"\n^ Nakamura, Junichi (2005). Image Sensors and Signal Processing for Digital Still Cameras. CRC Press. pp.169172. ISBN9781420026856.\n^ Dillon, Peter (Dec 1976). \"Integral color filter arrays for solid-state imagers\". Technical Digest International Electron Device Meeting (IEDM), Washington, DC, Dec 1976: 400403. doi:10.1109/IEDM.1976.189067  via IEEE.\n^ Parulski, Kenneth (August 1985). \"Color Filters and Processing Alternatives for One-chip Cameras\". IEEE Transactions on Electron Devices. 32 (8): 13811389. doi:10.1109/T-ED.1985.22133.\n^ Dillon, Peter (February 1978). \"Fabrication and performance of color filter arrays for solid-state imagers\". IEEE Transactions on Electron Devices. 25 (2): 97101. doi:10.1109/T-ED.1978.19045.\n^ Dillon, Peter (February 1978). \"Color imaging system using a single CCD area array\". IEEE Transactions on Electron Devices. 25 (2): 102107. doi:10.1109/T-ED.1978.19046.\n^ \"Deepest Ever Look into Orion\". Archived from the original on 13 July 2016. Retrieved 13 July 2016.\n^ Gitto, Simone (2020). Arduino with MATLAB in the thermography: From the sensor to the thermal camera (Arduino and Beyond). Independently published. ISBN979-8698999171.\n^ Dent, Steve. \"Sony's first 'curved sensor' photo may herald better images, cheaper lenses\". Archived from the original on July 11, 2014. Retrieved July 8, 2014.\n^ Musburger, Robert B.; Ogden, Michael R. (2014). Single-Camera Video Production. CRC Press. p.64. ISBN9781136778445.\n^ a b c Williams, J. B. (2017). The Electronics Revolution: Inventing the Future. Springer. pp.2458. ISBN9783319490885.\n^ a b Ohta, Jun (2017). Smart CMOS Image Sensors and Applications. CRC Press. p.2. ISBN9781420019155.\n^ \"1960: Metal Oxide Semiconductor (MOS) Transistor Demonstrated\". The Silicon Engine. Computer History Museum. Retrieved August 31, 2019.\n^ Kozlowski, L. J.; Luo, J.; Kleinhans, W. E.; Liu, T. (14 September 1998). \"Comparison of passive and active pixel schemes for CMOS visible imagers\". Infrared Readout Electronics IV. International Society for Optics and Photonics. 3360: 101110. Bibcode:1998SPIE.3360..101K. doi:10.1117/12.584474. S2CID123351913.\nPage 1731 https://books.google.com/books?id=6VE_AQAAMAAJ\n^ Web, Desk (2022-06-25). \"Samsung Electronics releases a sensor with 200 million pixels\". BOL News. Retrieved 2022-06-25.\n^ Janesick, James R. (2001). Scientific charge-coupled devices. SPIE Press. pp.34. ISBN978-0-8194-3698-6.\n^ Boyle, William S; Smith, George E. (1970). \"Charge Coupled Semiconductor Devices\". Bell Syst. Tech. J. 49 (4): 587593. doi:10.1002/j.1538-7305.1970.tb01790.x.\n^ U.S. Patent 4,484,210: Solid-state imaging device having a reduced image lag\n^ Fossum, Eric R. (2007). \"Active Pixel Sensors\" (PDF). Semantic Scholar. S2CID18831792. Archived from the original (PDF) on 9 March 2019. Retrieved 8 October 2019.\netal. (1985). \"A new MOS phototransistor operating in a non-destructive readout mode\". Japanese Journal of Applied Physics. 24 (5A): L323. Bibcode:1985JaJAP..24L.323M. doi:10.1143/JJAP.24.L323. S2CID108450116.\n^ \"CMOS Image Sensor Sales Stay on Record-Breaking Pace\". IC Insights. May 8, 2018. Retrieved 6 October 2019.\n^ Benchoff, Brian (17 April 2016). \"Building the First Digital Camera\". Hackaday. Retrieved 30 April 2016. the Cyclops was the first digital camera\n^ Lyon, Richard F. (2014). \"The Optical Mouse: Early Biomimetic Embedded Vision\". Advances in Embedded Computer Vision. Springer. pp.3-22 (3). ISBN9783319093871.\n^ Lyon, Richard F. (August 1981). \"The Optical Mouse, and an Architectural Methodology for Smart Digital Sensors\" (PDF).\nIn H. T. Kung; Robert F. Sproull; Guy L. Steele (eds.). VLSI Systems and Computations. Computer Science Press. pp.119. doi:10.1007/978-3-642-68402-9_1. ISBN978-3-642-68404-3.\n^ Brain, Marshall; Carmack, Carmen (24 April 2000). \"How Computer Mice Work\". HowStuffWorks. Retrieved 9 October 2019.\n^ \"Super Sensitive Sensor Sees What You Can't\". npr.org. Archived from the original on 24 March 2018. Retrieved 28 April 2018.\nDigital Camera Sensor Performance Summary by Roger Clark\nClark, Roger. \"Does Pixel Size Matter?\". clarkvision.com. (with graphical buckets and rainwater analogies)\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Image_sensor&oldid=1154151270\"\nCategories: Image sensorsDigital photographyMOSFETsHidden categories: Webarchive template wayback linksArticles with short descriptionShort description is different from WikidataAll articles with unsourced statementsArticles with unsourced statements from May 2023Articles with GND identifiers\nThis page was last edited on 10 May 2023, at 15:50(UTC).\nText is available under the Creative Commons Attribution-ShareAlike License 3.0;\nBy using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.",
    "ABOUT USThe CST TeamTerms & Conditions SaleCode of ConductVehicle SystemsLocal Situational Awareness SystemHDCitadel Quattuor IICortex InterceptorCitadel Sentinel MROUCitadel CompactCitadel ThermalCitadel DualMonitor 15IR-IlluminatorAnalogCitadel QuattuorCitadel ConnexCitadel PanoramicSprinkler tankCitadel CompactCitadel ThermalCitadel DualIR IlluminatorMonitor 15Driver Vision EnhancementCortex EnhanceCitadel CompactCitadel ThermalCitadel DualMonitor 15Remote Weapon SystemsRemote Weapon System ArmySpectrel 12120/336Spectrel Dual HD 12220-1250/340Spectrel 12120/340 HDSmall RWS S-RWSRemote Weapon System NavySpectrel Naval 13165/336Spectrel Naval 12210/340 HDFire Control SystemsArmySPECTREL DUAL-CAM HD SP12220-1250-340NavySPECTREL DUAL-CAM HD SP12220-1250-340Spectrel Naval 13165/336Spectrel Naval 12210/340Surveillance SystemsArmySpectrel 12120/340 HDSpectrel 12210/340 HDSpectrel12750/340 HDSpectrel 121000/340 HD SDIOpen Frame 121000/340W HDSPECTREL DUAL-CAM HD SP12220-1250-340Spectrel PTZI-1000 HDNavySpectrel Naval 13165/336Spectrel Naval 12210/340 HDCitadel Compact NavalOpen Frame SystemsOEM 5-50/336Open frame 13165/336Open frame 12210/340Open frame 12750/340Open frame 121000/340Open frame Dual/340\nhigh precision zoom optics for any combat and surveillance situation in all environments All CSTs rugged cameras  even the thermal cameras, have optimized performance with specially designed and adapted lens, filter and sensor. CST delivers clear images and high performance in one optics and camera solution.Rugged camera delivering full situational awareness in all terrains and any challenging environment\nCST has a 20-years track record in delivering thermal sensor cameras performing in the most challenging environments and guarantees a lifespan of up to 20 years.\nCSTs camera with flexible interface between analog and HD makes it very cost effective and reduces need for maintenance and retrofit time.\nIn international military missions under challenging and extreme conditions CSTs thermal cameras demonstrate the highest reliability.\nCSTs sensor delivers best stay on target with high boresight accuracy.Rugged Cameras - Higher degree of\nfor the soldiers CSTs rugged cameras with surrounding panoramic view ensure full situational awareness whenever the soldiers leave the vehicle.CST delivers perfectly standardized solutions where no further development is required..READ MORELong range camera - High\noptronics solutions that provide a significant benefit to the end userHighly customized systems based on experience and knowhow from long-term collaboration with partners.CST partners with major suppliers of weapon systems such as Thales, BAE Systems Bofors and Kongsberg.READ MOREThermal sensor camera - High\nprecision zoom optics in any naval combat situation and tough maritime environmentCSTs sensor delivers best stay on target with high boresight accuracy for remote weapon stations.CSTs rugged camera system is bore-sighted to achieve a precision of 0.1-0.2 mrad boresight retention @NFOV at a temperature range from -40C to +70C.READ MOREOUR REFERENCESCST has become a key supplier of both analog and digital solutions in one camera.CST has a 20-year track record in delivering cameras performing in the most challenging environments and guarantees a lifespan of up to 20 years.With the impressive development effort to deliver new electro-optical sensors for our Goalkeeper and Mirador systems, CST has demonstrated great expertise in the design and development of electro-optical sensorsGerben Edelijn, CEO of Thales in the NetherlandsLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamcoLorem ipsum, CEO lorem ipsum dolorSign up for newsletter\nYes, thank you, I would like to receive a newsletter with information about Copenhagen Sensor Technology A/S products, offers, competitions and events via e-mail and I accept the terms and I am familiar with the privacy policy.\nCopyright  2019 Copenhagen Sensor Technology | info@copst.com\nThis website uses cookies to improve your experience. We'll assume you're ok with this, but you can opt-out if you wish. Privacy PolicyACCEPT Privacy & Cookies Policy\nPrivacy OverviewThis website uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "The most important things to consider when choosing your CCTV solution for maritime application\nWhether constructing a cargo ship, tanker, ferry or cruise ship, the first and foremost priority of any shipyard should always be safety.\nSmart buildings are designed to collect more and more information to provide a better understanding of the users and enhance the energy consumption, the usability, and the security. The use of contextual data together with the control and management system is the way to make the building smart. Data collection helps to control the building and optimize it to create a safe, energy and user-friendly environment.\nWith todays technology a smart building can, for example, have control of the amount of people in the facility and adjust the light and ventilation accordingly. With the help of deep learning, artificial intelligence, and analytics. It is possible to route people to the most efficient fire exit, control ambient light and air flow and create contextual information about the use of the building. It can also facilitate meeting room bookings, report safety or issue and take corresponding actions.\nTo get a comprehensive behavior of a building it is extremely important to get the right information and have it processed accordingly. The building will get smarter if a correlation between information flow is created. For example, counting people in the building at the entrance will give a general indication but will not be accurate enough to control light or ventilation in the whole facility. It will increase the granularity from building to room, and provide an exact picture of the density and provide information to control the energy consumption. With deep learning and behavior understanding the building can learn how the building is used and make management decision based on historical data and facts.\nTo understand the behavior of the building, user data must be collected. Data can be accumulated in 3 different ways:\nIs when a person is entering the data. It can be badging at the entrance, taking fingerprints, or that the receptionist is providing an input periodically or based on an event. Manual input requires the interaction of a person to validate the data input.\nA data related to the building behavior is deducted from 2 or more different sources and provide value that can be used to act or modify the facility. For example, the amount of people entering the building deducted from the amount for people leaving the building per hour will provide the occupancy flow and can be used as a future indicator to create an occupancy trend for the short term.\nThe field sensors provides real time data, each type of sensors has a purpose to fullfill, such as safety or environement monitoring. It can be to measure the concentration of certain type of gas gas, detect smoke, heat or fire. Smart cameras has the ability to detect object, line crossing or movement detection and can be use as a complementary solution for the security and safety system.\nIn every case a sensor provides a real time value in a form of an electric signal, then this signal must be processed in the edge to transfer a comprehensive data that can be analyzed. A sensor on its own does not possess any processing. It is the computing power attached to this sensor and the way it is integrated that will make the system smarter.\nCamera systems has evolved from analogue to digital over the last decade. This transformation has enabled a set of possibilities where digital images can be processed and understood by computers, enabling digital image processing and analytics.\nAt first, when those system started to be integrated, the computing power was centralized, now the trend is moving towards edge processing and close to source computing.\nModern IP cameras are now embedded with a microprocessor providing the ability to process data at the source and combining sensorics technology with analytics. High quality analytics is possible due to the high image resolution, thermal imaging, or extreme light sensitive chipset. With modern IP cameras it is now possible to understand and create trends by analyzing the video stream. This evolution is moving the use of the camera from security to facility management tools.\nComplex image processing will require powerful graphical processing, however, with today's technology cameras are able to detect and trigger actions based on the following:\nPeople counting: Multi camera systems can count people located in a room, floor or building. Communication between camera allow the system to know exactly how many people are entering or exiting an area.\nCrowd management: Camera systems that can detect if a crowd is gathering around a certain location, which can detect unsafe situations and create a building that is situation aware.\nFace recognition: Entrance systems can be use as a double authentication method. Ensure that the card/key user is the rightful owner and avoid identity thief by associating the face to the user number. (Disclaimer: this function has to be used according to the GDPR)\nObject tracking: Triggers alarm if an object or person has been in a predefined area for a period of time. This can help prevent crime such as vandalism and theft. The application is ideal for low-traffic areas and after business hours on semi-public properties such as office parking lots and schoolyards, as well as near fences.\nPerimeter detection: Outdoor- & Indoor Zones, Buildings and premises, in general, are secured with an intelligent perimeter protection. Early alarms are triggered by unwanted intruders detected within a predefined are that is tagged in the camera.\nLPR: License plate recognition makes it easy to have an automated vehicle access system in areas such as parking lots or can be used to restrict area to a whitelist of vehicle. The analytics application installed on an IP camera, using the Input output relay to control the facility barrier and can report to the management system of the building.\nOccupancy estimator: provides real-time data on how many people are present in the building or in a certain area at a certain time. This insight can help understand visitor flow and occupancy trends giving the opportunity to improve space usage. It also gets an indication of revenue opportunity, optimizing workforce planning and opening hours. The system can be used to take measures if occupancy exceeds your set threshold (example with social distancing). It also offers the possibility to inform visitors about peak times or when your premises are at full capacity.\nObject detection and identification: Analytics and historical data, allows the system to detect the type of object and create action about possible risks, such as blocking emergency exit, protects indoor areas against theft, and detects left objects.\nAlarm and Security: Video motion detection application detects and triggers the alarm whenever an object, such as a person or vehicle, moves within predefined areas in the cameras field of view. It is ideal for low-traffic areas such as after-hours monitoring of office buildings, retail stores, industrial properties, vehicle depots and other unattended areas in non-critical applications.\nAccess control: The combination of a camera and an access control system will provide better safety and better control of personnel. Cameras can detect tailgating and enhance security by knowing exactly who is using the facility and when.\nIn addition to being able to detect and analyze behavior based on a video stream, it is also possible to connect the video system to a speaker and microphone and play a message for security purpose, but also music or general information based on real time assessment. The microphone applications can detect sounds such as broken glass or gun shot to increase security.\nIn building such as museum where the air quality is important for the preservation of the artifact, an automated people count can be linked to the light and air flow control management ensuring that the heat and humidity stays within limitation. Also, the audio system can politely ask the people to move to the next room or wait at the entrance if the amount of people is off limit or guide the flow of visitors in a less populated area.\nIP camera as an integrated part of a smart building\nUsing IP camera as an integrated part of a smart building provides valuable information that can be used not only for security purpose but also for facility management and energy saving. In the end, the insight modern IP camera system provides are a constant eye on your building and combining this tool with deep learning or artificial intelligence will provide facility manager a deeper insight on their building, a better control, and a better user experience.\nHatteland Technology  your ideal smart building partner\nHatteland Technology has over 30 years of experience in designing security CCTV systems.\nOur extensive knowledge of CCTV cameras, camera placement, lighting conditions, suppliers, softwares, and video management systems, combined with our long-lasting relationships with the top suppliers in the market, makes us an ideal partner when choosing your new smart or AI camera system  or updating your existing one.\nWe can help you find your perfect camera for smart building! Contact one of our surveillance experts to review your options.\nCCTV is fundamental to tunnel safety. And theres more to it than you might picture.\nRask og nyaktig gjenkjenning av kjrety er avgjrende for  sikre god ytelse, plitelig videoanalyse og kt..\nPlease complete the form below. Submissions will be responded to within two business days.\nHatteland Technology is a provider of advanced technology solutions within industrial computing, security & surveillance and industrial networking ranging from standard off-the-shelf products to customized solutions and services. With in-depth industry knowledge of the segments we operate in, we offer specialized, tailored solutions in the design, engineering and manufacture of precision technology, built for tough conditions."
  ],
  "image verification techniques": [
    "Kill switch: reporting on and during internet shutdowns\nHow to create data visualisations that serve the public\nUncovering the truth: Exploring the benefits of federated databases for policing records\nHow data can power public health investigations  through collaboration\nIts time to rethink how we report election results\nThe latest edition explores innovative ways that data is analysed, created, and used in the context of journalism.\n1.1. Separating Rumor From Fact in a Nigerian Conflict Zone\n3.1. Monitoring and Verifying During the Ukrainian Parliamentary Election\n4.2. Verifying Two Suspicious Street Sharks During Hurricane Sandy\n5.2. Investigating a Reported Massacre in Ivory Coast\n5.3. Confirming the Location and Content of a Video\n7.1. How OpenStreetMap Used Humans and Ma- chines to Map Affected Areas After Typhoon Haiyan\n8.1. How NHK News Covered, and Learned From, the 2011 Japan Earthquake\n9. Creating a Verification Process and Checklist(s)\nVISUALIZE JUSTICE: A Field Guide to Enhanc- ing the Evidentiary Value of Video for Human Rights\nTracking Back a Text Message: Collaborative Verification with Checkdesk\nThe Story of Jasmine Tridevil: Getting around Roadblocks to Verification\nStolen Batmobile: How to Evaluate the Veracity of a Rumor\nRussian Bear Attack: Tracking Back the Suspect Origin of a Viral Story\nThat was certainly the case for BBC News User Generated Content hub in the beginning of July 2005. It had been one week since the initial pilot team was set up to help collate the content being sent to BBC News by its audiences, and help get the best of it shown across TV, radio and online.\nThat morning, as the BBC and other news organizations reported a power surge on the London Underground, the UGC team started seeing a very different story emerging via content sent to BBC News directly from its audience.\nThis was one of the first images the team received. Before it was broadcast, the image was examined closely and the originator was contacted to verify his story and the details of what he saw. The photo inadvertently became one of the first examples of the UGC image verification process that has since moved toward standard practice across the industry.\nThat image, and others like it, showed the terror and chaos in London during the moments immediately after the attacks. As a result, it ensured that the reporting of the story quickly changed. It was the first significant example of UGCs proving critical to helping BBC News tell a major story more accurately, better and faster.\nToday, the UGC team is embedded within the heart of the BBC newsroom. Its 20 journalists work across TV, radio, online and social media platforms to produce content sourced either directly from the BBCs audiences or from the wider Web.\nVerification is critical to the success of what the UGC team produces. Technology has moved on considerably since 2005, bringing an exponential rise in the use of social networks and the power of mobile phones. These changes offer great benefits in our newsgathering processes, particularly on breaking news; they also bring great challenges.\nWhether a trusted global news organization like the BBC or a humanitarian professional on the ground, the need to be fast at collecting and disseminating key images on a breaking news story has to be balanced with the need to be sure the images are credible and genuine. We also have to ensure copyright is protected and appropriate permissions are sought.\nSince that day in 2005, the UGC team has developed a number of approaches to help in this process. While the technology will continue to change - as will the tools we use - the basic principles of image verification remain the same:\nEstablish the author/originator of the image.Corroborate the location, date and approximate time the image was taken.Confirm the image is what it is labeled/suggested to be showing.Obtain permission from the author/originator to use the image.\nThe obvious - and usually most effective - way of doing this is to contact the uploader and ask him directly if he is indeed the person who took the image.\nReaching out to the uploader via the social network account or email address the image was shared from is a first step, but its also important to try to ascertain as much about the uploaders identity as possible. These details can help in determining whether he is in fact the original source of the image.\nAs outlined in the previous chapter, in many instances, people may try to be helpful by repost- ing images they have seen elsewhere. This happens frequently to news organizations - images are sent in by well-meaning members of the public to help report a story. Just by asking the sender to confirm if its his image or not can save a lot of time in the verification process.\nWhile tracking down the source of an image begins with the person who uploaded it, it often ends with a different person  the one who actually captured the image.\nAs referenced in an earlier chapter, an important step is to use a service like Google Reverse Image Search or TinEye. Paste the image URL or a copy of the image into either and they will scan the web to see if there are any matches. If several links to the same image pop up, click on view other sizes to investigate further.\nUsually, the image with the highest resolution/size should take you to the original source. (On Google Images, the resolution for each image result is listed just next to the image itself.) You can then check it against the image you have and see if the source appears authentic.\nQuite often on a breaking news event, there will be no images of specific people that you want to illustrate the story with (particularly if they involve ordinary members of the public). Alternatively, you might want to confirm that an image you have of someone is actually them and not someone else with the same name.\nIve found Pipl.com to be particularly helpful here as it allows you to cross-reference names, usernames, email address and phone numbers against online profiles of people. For interna- tional searches, WebMii is an additional resource that can help. LinkedIn is also proving to be a great way of verifying individuals and often provides additional leads for being able to track them down (through companies/organizations they are currently or previously associated with).\n2. Corroborate the location, date and approximate time the image was taken\nThere are some useful journalistic and technical ways of establishing information such as date, location and other important details. One core way of gathering this information is when you speak to the creator/uploader of the image. These five questions continue to stand the test of time:\nWho are they?Where are they?When did they get there?What can they see (and what does their photo show)?Why are they there?\nOne important aspect to note here: If the image is from a dangerous location, always check that the person you are talking to is safe to speak to you. Also be aware of any issues about identifying the source through any details you broadcast about him or his images.\nFrom our experience at the BBC, people who were really there will give visual answers, often describing the details in the present tense. (Im in the middle of X Street; I can see and hear Y.) The more vague the answer, the more caution you should exercise about what the source is telling you.\nAnother useful technique is to ask the person to send any additional images shot at the same time. Its rare that someone takes only one picture in a newsworthy situation. Having more than one image helps you learn more about how the events in question unfolded.\nOnce youve gathered the sources account of how the image was taken, work to corroborate the information further. Two primary methods can be used to investigate the contents of the photo itself and triangulate that with what you were told by the source.\nFirst, check if the image has any metadata. Metadata, also referred to as EXIF data when it comes to digital images, refers to information embedded in an image. If the image is an original, theres a good chance you will see information about the make and model of the camera, the timestamp of the image (be careful though - if there is one, it could still be set to the manufacturers factory setting or another time zone), and the dimensions of the original image, among other details. You can use software like Photoshop (look at the file informa- tion) or look for or free online tools like Fotoforensics.com or Findexif.com to generate an EXIF report.\nUpload the image and the EXIF reader will return out whatever information is contained on the image. Some of the information is useful to those who have a more technical understanding of digital photography. But for the average person, data such as the date the photo was originally taken or the type of camera that took the image can sometimes help expose a lying source.\nOne note of caution here: The majority of social media image sites such as Twitter, Facebook and Instagram strip out most of the original metadata from images when they are uploaded onto their platforms, if not all. (Flickr seems to be an exception to this.)\nSecond, cross-reference the image with other sources. Awaken your inner investigator by examining the image closely. Quite often there will be clues that can help you verify the location and time it was taken:\nLicense/number plates on vehiclesWeather conditionsLandmarksType of clothingSignage/letteringIs there an identifiable shop or building?What is the type of terrain/environment in the shot?\n3. Confirm the image is what it is labeled/suggested to be showing\nAn image may be authentic, but it could be inaccurately labeled. For example, during Hurricane Sandy, this image spread widely on Twitter and was described as being a shot of three soldiers standing guard at the Tomb of the Unknown Soldier during the storm:\nThe image was accurate in that it did show soldiers at the Tomb. But it had been taken a month earlier, not during Sandy. The picture had been posted on the Facebook page of the First Army Division East.\nAs part of verifying the date, time and approximate location of an image, its also important you confirm that the image is what it purports to be. An authentic image can still be placed in a false context.\nUse Google Maps, Bing Maps or Wikimapia to help you verify locations. UGC images are increasingly being tagged on these services now, and they can also provide useful leads to follow up on, as well as different angles to locations you are investigating. (Learn more about using these mapping services for verification in Chapter 5: Verifying Video.)\nUse weather sites that can give you accurate reports of conditions at that location on that date to confirm if the weather in the image matched. As noted in the previous chapter, Wolfram Alpha is very good at searching for weather reports at specific times and places.\nIf there is lettering (e.g. on a sign) in a different language within the image, use Google Translate to see if it can give you another clue to the location. The optical character reading tool free-ocr.com can also be helpful if you want to extract text from an image -which you can then run through an online translation.\nSocial media location services like Geofeedia and Ban.jo can also help establish the loca- tion from which an image was uploaded. These services use the GPS data from the mobile device that uploaded the image. While they currently capture only a small percentage of the social media content uploaded from a given location, they do provide a useful initial filter. The image below is an example of some of the photos captured by Geofeedia in the immedi- ate aftermath of the Boston marathon bombings:\nAlong with those tools and techniques, for images its also useful to check to see if similar images are being distributed by official news organizations or agencies. Are there any images from that location being uploaded on social media by others? If they show a similar scene from a different angle, that will also help establish credibility of the image.\nFinally, on a big story, its always worth double checking if a particularly strong image you come across appears on Snopes, which specializes in debunking urban legends and misin- formation on the Internet.\n4. Obtain permission from the author/originator for use of the image\nIt is always best practice to seek permission from the copyright holder of images. Adding to this, copyright laws in many countries are increasingly clear that damages can be sought by the originator if permission isnt asked for or granted.\nThe terms and conditions with regards to the copyright of content uploaded on social media sites vary from service to service. Some, like Flickr, show clearly alongside the image if the photographer has retained all copyright, or if he allows Creative Commons usage. (Its a good idea to read up on Creative Commons licenses so you are familiar with how they differ.)\nWhen seeking permission, its important to keep a few details in mind:\nBe clear about the image(s) you wish to use.Explain how the image(s) will be used.Clarify how the photographer wishes to be credited (name, username, etc., keeping in mind that in some cases they may wish to remain anonymous).\nMost importantly, remember that if youve gone through the above checks and processes and youre still in doubt - dont use the image!\nJoin 10.000 data journalism enthusiasts and receive a bi-weekly newsletter or access our newsletter archive here.\nI agree that my data will be processed for sending me this newsletter.\nAll processing will happen according to the EJC Privacy Policy*\nHead over to your inbox and click the confirmation link in the email to complete your subscription.\nIf you experience any other problems, feel free to contact us at [emailprotected]\nReview your cookie settings for the optimal site experience.\nSocial Features: allows us to show embedded Tweets Usage Insights: helps us improve the website",
    "9 tools for verifying images | International Journalists' Network\nEnglishEspaolPortugusFranais\nEnglishEspaolPortugusFranais\nAs technology advances, fake images and video become harder to spot. A University of Warwickstudyfound participants identified fake images only 60 percent of the time. But these tools can help us figure out if what were seeing is actually real.\nThis website allows users to record, convert and stream any kind of audio or video. Users can also resize video quickly using a polyphase filter or watch video in slow motion.\nThis tool provides an in-depth analysis of images on the internet. Although FotoForensics doesnt simply state whether an image is real or fake, it can identify hidden pixels, error level analysis and metadata details.\nWith this tool, you can Google search an image, and the results will display similar images and websites that contain these images.\nInVID is a plugin available on Chrome and Firefox. Upload an image or video to the system and it will show you its original location, date of creation, thumbnails and keyframes. It allows you to reverse search on Google and Twitter, magnify images without losing clarity and analyze metadata with an Image Verification Assistant. Watch a tutorialhere.\nUsers can upload images to this free app and it will perform a series of tests to conclude whether or not the image is doctored. It also shows which part of the image has been modified. Serelay does not keep an inventory of photos in attempts to protect users privacy. But it does store a digital fingerprint of each image that can detect even a single-pixel edit.Learn more about Serelay and Truepic here.\nSunCalc is an application shows the suns movement throughout the day on any map area. This helps users verify information by matching the shadows in videos to the time of day the video was taken.\nWhen users upload an image to TinEye, the site reverse image searches to find duplicates and scours the internet for other sites where the image occurs.\nMedia consumers can use this free app to validate photos and videos they encounter on the internet. It uses computational techniques to determine whether pixels or metadata seem altered. Truepic stores all photo and video in a server using Blockchain.\nThis tool shows users the upload time of a video after they copy and paste the link into the search bar. With this website, users can also view thumbnails and a link to reverse image search the thumbnails.\nThisarticlewas originally published by theNews Co/Lab at Arizona State University. It was republished on IJNet with permission.\nMain image CC-licensed byUnsplashviaCole Keister.\nIJNet provides the latest tips, trends and training opportunities in eight languages. Sign up here for our weekly newsletter:\n750 17th Street, NW, Suite 300, Washington, DC 20006 USA",
    "Digital Image Forensics-Image Verification Techniques | SpringerLink\nIntelligent Computing and Applications pp 221234Cite as\nDigital Image Forensics-Image Verification Techniques\nPart of the Advances in Intelligent Systems and Computing book series (AISC,volume 1172)\nAbstractIn the present scenario of the digital era, we are on the brink of marginal transformation in digital imaging technology. Developments of high computational and artificial intelligence techniques have produced wonderful image editing techniques that create imposing results in stint frames. And, doing this will affect the original artifacts present in digital images. This refers to the manipulation or forgery of digital data with the help of image editing applications and this also erodes our trust in digital images. To rely on the semantics of digital images, there is significant research contribution in terms of various image forgery detection techniques. These techniques help to re-evaluate the authenticity of digital images. In this article, an examination of various research contributions is conducted. The primary goal of this work is to give a glimpse of various current existing techniques related to digital image forensics. These techniques are assessed as per their proficiencies and boundaries. Whenever possible, a comparison between similar domain techniques is presented. The main focus is on pixel and physics-based techniques. Our analysis discusses the challenges that are to be addressed in this area of forensic science and provides insight into various datasets available for researchers to develop and test new approaches. Finally, the work recommends possible future research directions.KeywordsContent authenticationForgery detectionImage forensicsPassive techniques\nThis is a preview of subscription content, access via your institution.\nReferencesM. Kumar, S. Srivastava, Image forgery detection based on physics and pixels: a study. Aust. J. Forensic Sci. 51(2), 119134 (2019)CrossRef\nS. Walia, K. Kumar, Digital image forgery detection: a systematic scrutiny. Aust. J. Forensic Sci. (2018). https://doi.org/10.1080/00450618.2018.1424241H. Farid, Image forgery detection. IEEE Signal Process Mag. 26(2), 1625 (2009)CrossRef\nM. Dilshad, S. Ghrera, V. Tyagi, Pixel based image forgery detection: a review. IETE J. Educ. 55(1), 4044 (2014)CrossRef\nE. Silva, T. Carvalho, A. Ferreira, A. Rocha, Going deeper into copy-move forgery detection: exploring image telltales via multi-scale analysis and voting processes. J. Vis. Commun. Image Represent. 26, 1632 (2015)CrossRef\nD. Cozzolino, G. Poggi, L. Verdoliva, Efficient dense-field copy-move forgery detection. IEEE Trans. Inf. Forensics Secur. 10(11), 22842297 (2015)CrossRef\nL. Yu, Q. Han, X. Niu, Feature point-based copy-move forgery detection: covering the non-textured areas. Multimedia Tools Appl. 75(2), 11591176 (2016)CrossRef\nM. Zandi, A. Mahmoudi-Aznaveh, A. Talebpour, Iterative copy-move forgery detection based on a new interest point detector. IEEE Trans. Inf. Forensics Secur. 11(11), 24992512 (2016)CrossRef\nX.-Y. Wang, S.L.Y.-N. Liu, Y. Niu, H.-Y. Yang, Z.-L. Zhou, A new keypoint-based copy-move forgery detection for small smooth regions. Multimedia Tools and Applications 76(22), 2335323382 (2017)CrossRef\nJ.-C. Lee, C.-P. Chang, W.-K. Chen, Detection of copymove image forgery using histogram of orientated gradients. Inf. Sci. 321, 250262 (2015)\nZ. Liang, G. Yang, X. Ding, L. Li, An efficient forgery detection algorithm for object removal by exemplar-based image inpainting. J. Vis. Commun. Image Represent. 30(2015), 7585 (2015)CrossRef\nD.-Y. Huang, C.-N. Huang, W.-C. Hu, C.-H. Chou, Robustness of copy-move forgery detection under high JPEG compression artifacts. Multimedia Tools Appl. 76(1), 15091530 (2017)CrossRef\nH. Zhou, Y. Shen, X. Zhu, B. Liu, Z.F.N. Fan, Digital image modification detection using color information and its histograms. Forensic Sci. Int. 266, 379388 (2016)CrossRef\nX. Zhao, S. Wang, S. Li, J. Li, Passive image-splicing detection by a 2-D noncausal markov model. IEEE Trans. Circuits Syst. Video Technol. 25(2), 185199 (2015)CrossRef\nK. Bahrami, A.C. Kot, L. Li, H. Li, Blurred image splicing localization by exposing blur type inconsistency. IEEE Trans. Inf. Forensics Secur. 10(5), 9991009 (2015)CrossRef\nL. Zhan, Y. Zhu, Z. Mo, An image splicing detection method based on PCA minimum eigenvalues. J. Inf. Hiding Multimedia Signal Process. 7(3), 610619 (2016)\nD. Cozzolino, L. Verdoliva, Single-image splicing localization through autoencoder-based anomaly detection, in IEEE International Workshop on Information Forensics and Security (WIFS), Abu Dhabi, United Arab Emirates, 2016\nA.R. Abrahim, M.S.M. Rahim, G.B. Sulong, Splicing image forgery identification based on artificial neural network approach and texture features. Cluster Comput. 22(S1), 647660 (2019)CrossRef\nC. Song, P. Zeng, Z. Wang, T. Li, L. Qiao, L. Shen, Image forgery detection based on motion blur estimated using convolutional neural network. IEEE Sens. J. (2019). https://doi.org/10.1109/jsen.2019.2928480H. Li, W. Luo, X. Qiu, J. Huang, Image forgery localization via integrating tampering possibility maps. IEEE Trans. Inf. Forensics Secur. 12(5), 12401252 (2017)CrossRef\nP. Korus, J. Huang, Multi-scale fusion for improved localization of malicious tampering in digital images. IEEE Trans. Image Process. 25(3), 13121326 (2016)CrossRef\nF. Akhtar, H. Qayyum, Two fold image forgery detection system using combined keypoint based method and block based method. J. Inf. Commun. Technol. Robotic Appl. 9(2), 6270 (2018)\nX.-Y. Wang, L.-X. Jiao, X.-B. Wang, H.-Y. Yang, P.-P. Niu, A new keypoint-based copy-move forgery detection for color image. Appl. Intell. 48(10), 36303652 (2018)CrossRef\nX.-Y. Wang, Y.-N. Liu, H. Xu, P. Wang, H.-Y. Yang, Robust copymove forgery detection using quaternion exponent moments. Pattern Anal. Appl. 21(2), 451467 (2018)CrossRef\nH.-Y. Yang, S.-R. Qi, Y. Niu, P.-P. Niu, X.-Y. Wang, Copy-move forgery detection based on adaptive keypoints extraction and matching. Multimedia Tools Appl. 128 (2019). https://doi.org/10.1007/s11042-019-08169-wP. Srivastava, M. Kumar, V. Deep, P. Sharma, A technique to detect copy-move forgery using enhanced SURF. Int. J. Eng. Adv. Technol. 8(6S), 676680 (2019)CrossRef\nK.S. Choi, E.Y. Lam, K.K. Wong, Source camera identification using footprints from lens aberration, in Proceedings of SPIE-IS&T Electronic Imaging, SPIE, vol. 6069, 2006\nJ. Lukas, J. Fridrich, M. Goljan, Digital camera identification from sensor pattern noise. IEEE Trans. Inf. Forensics Secur. 1(2), 205214 (2006)CrossRef\nL.-H. Chan, N.-F. Law, W.-C. Siu, A confidence map and pixel-based weighted correlation for PRNU-based camera identification. Digit. Invest. 10(2013), 215225 (2013)CrossRef\nS. Taspinar, M. Mohanty, N. Memon, PRNU-based camera attribution from multiple seam-carved images. IEEE Trans. Inf. Forensics Secur. 12(12), 30653080 (2017)CrossRef\nP. Yang, R. Ni, Y. Zhao, W. Zhao, Source camera identification based on content-adaptive fusion residual networks. Pattern Recogn. Lett. 119, 195204 (2019)CrossRef\nH. C. Nguyen, S. Katzenbeisser, in Robust resampling detection in digital images, ed. by B. De Decker, D.W. Chadwick. Communications and multimedia security. CMS 2012. Lecture Notes in Computer Science, vol. 7394 (Springer, Berlin, Heidelberg, 2012)\nM. Kirchner, T. Gloe, On resampling detection in re-compressed images, in First IEEE International Workshop on Information Forensics and Security (WIFS), London, UK, 2009\nY. Su, X. Jin, C. Zhang, Y. Chen, Hierarchical image resampling detection based on blind deconvolution. J. Vis. Commun. Image Represent. 48, 480490 (2017)CrossRef\nF. Zach, C. Riess, E. Angelopoulou, in Automated image forgery detection through classification of JPEG ghosts, ed. by A. Pinz, T. Pock, H. Bischof, F. Leberl. Pattern Recognition. DAGM/OAGM 2012. Lecture Notes in Computer Science, vol. 7476 (Springer, Berlin, Heidelberg, 2012)\nQ. Wang, R. Zhang, Double JPEG compression forensics based on a convolutional neural network. EURASIP J. Inf. Secur. 23(2016), 112 (2016)\nC. Pasquini, G. Boato, F. Prez-Gonzlez, Statistical detection of JPEG traces in digital images in uncompressed formats. IEEE Trans. Inf. Forensics Secur. 12(12), 28902905 (2017)CrossRef\nP. Korus, Digital image integritya survey of protection and verification techniques. Digit. Signal Proc. 71(2017), 126 (2017)MathSciNet\nS. Mandelli, P. Bestagini, S. Tubaro, D. Cozzolino, L. Verdoliva, Blind detection and localization of video temporal splicing exploiting sensor-based footprints, in 26th European Signal Processing Conference (EUSIPCO), Rome, Italy, 2018\nB. Gupta, M. Tiwari, Improving source camera identification performance using DCT based image frequency components dependent sensor pattern noise extraction method. Digit. Invest. 24(2018), 121127 (2018)CrossRef\nM. K. Johnson, H. Farid, Exposing digital forgeries by detecting inconsistencies in lighting, in ACM Multimedia and Security Workshop, New York USA, 2005\nM. K. Johnson, H. Farid, Exposing digital forgeries in complex lighting environments, IEEE Trans. Inf. Forensics Secur. 2(3), 450461 (2007)\nE. Kee, H. Farid, Exposing digital forgeries from 3-D lighting environments, in IEEE international workshop on information forensics and security, Seattle, WA, USA, 2010\nB. Peng, W. Wang, J. Dong, T. Tan, Improved 3D lighting environment estimation for image forgery detection, in IEEE International Workshop on Information Forensics and Security (WIFS), 2015\nM. Kumar, S. Srivastava, Identifying photo forgery using lighting elements. Indian J. Sci. Technol. 9(48), 15 (2016)\nC. Riess, in Illumination analysis in physics-based image forensics: a joint discussion of illumination direction and color, ed. by A. Piva, I. Tinnirello, S. Morosi. Digital Communication. Towards a Smart and Secure Future Internet. TIWDC 2017. Communications in Computer and Information Science, vol. 766 (Springer, Cham, 2017)\nM. Kumar, S. Srivastava, Image authentication by assessing manipulations using illumination. Multimedia Tools Appl. 78(9), 1245112463 (2019)CrossRef\nM. Kumar, S. Srivastava, in Image tampering detection based on inherent lighting fingerprints, ed. by D. Hemanth, S. Smys. Computational Vision and Bio Inspired Computing. Lecture Notes in Computational Vision and Biomechanics, vol. 28 (Springer, Cham, 2018)\nM. Kumar, A. Rani, S. Srivastava, Image forensics based on lighting estimation. Int. J. Image Graph. 19(3):195004:1195004:14 (2019)\nJ. Lopez-Moreno, E. Garces, S. Hadap, E. Reinhard, D. Gutierre, Multiple light source estimation in a single image. Comput. Graph Forum 32(8), 170182 (2013)CrossRef\nS. Xu, A. Wallace, Recovering Surface reflectance and multiple light locations and intensities from image data. Pattern Recogn. Lett. 29(11), 16391647 (2008)CrossRef\nY. Wang, D. Samaras, Estimation of multiple directional light sources for synthesis of mixed reality images, in 10th Pacific Conference on Computer Graphics and Applications, Beijing, China, 2002\nM. Kumar, S. Srivastava, N. Uddin, Forgery detection using multiple light sources for synthetic images. Aust. J.Forensic Sci. 51(3), 243250 (2019)\nA. Mazumdar, P.K. Bora, Estimation of lighting environment for exposing image splicing forgeries. Multimedia Tools Appl. 78(14), 1983919860 (2019)CrossRef\nDownload references Author informationAuthors and AffiliationsDepartment of Computer Science, G L Bajaj Institute of Technology and Management, Greater Noida, Utter Pradesh, IndiaAnuj RaniDepartment of Computer Science, Banasthali Vidyapith, Tonk, Rajasthan, 304022, IndiaAjit JainAuthorsAnuj RaniView author publicationsYou can also search for this author in\nPubMedGoogle ScholarAjit JainView author publicationsYou can also search for this author in\nPubMedGoogle ScholarCorresponding authorCorrespondence to\nAnuj Rani . Editor informationEditors and AffiliationsDpt of Electrical and Electronics Engg, Government College of Engineering, Keonjhar, IndiaDr. Subhransu Sekhar DashElectronics and Communication Sciences, Indian Statistical Institute, Kolkata, West Bengal, IndiaProf. Dr. Swagatam DasIndian Institute of Technology, New Delhi, Delhi, IndiaDr. Bijaya Ketan Panigrahi Rights and permissionsReprints and Permissions Copyright information 2021 Springer Nature Singapore Pte Ltd. About this paperCite this paperRani, A., Jain, A. (2021).\nDigital Image Forensics-Image Verification Techniques.\nIn: Dash, S.S., Das, S., Panigrahi, B.K. (eds) Intelligent Computing and Applications. Advances in Intelligent Systems and Computing, vol 1172. Springer, Singapore. https://doi.org/10.1007/978-981-15-5566-4_19Download citation.RIS.ENW.BIBDOI: https://doi.org/10.1007/978-981-15-5566-4_19Published: 30 September 2020\nOnline ISBN: 978-981-15-5566-4eBook Packages: Intelligent Technologies and RoboticsIntelligent Technologies and Robotics (R0)Share this paperAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard\nProvided by the Springer Nature SharedIt content-sharing initiative\nOver 10 million scientific documents at your fingertips\n 2023 Springer Nature Switzerland AG. Part of Springer Nature.",
    "Home - Journalism - LibGuides at University of South Carolina Upstate\nHomeGeneral Journalism SourcesPeriodicals - Print MediaInternetFilm, Television & Radio SourcesPrimary SourcesFake, Bias & Satirical NewsFact-Checking Your ReportingVerifying Images & Social Media\nJournalism is the activity of gathering, assessing, creating, and presenting news and information. It is also the product of these activities.\nJournalism can be distinguished from other activities and products by certain identifiable characteristics and practices. These elements not only separate journalism from other forms of communication, they are what make it indispensable to democratic societies. History reveals that the more democratic a society, the more news and information it tends to have.\nWhat is journalism? Definition and meaning of the craft.Journalism essentials, American Press Institute, 9 Oct. 2013, www.americanpressinstitute.org/journalism-essentials/what-is-journalism/.",
    "skip navigation Telerik Test Studio Product BundlesDevCraftAll Telerik .NET tools and Kendo UI JavaScript components in one package. Now enhanced with:NEW: Design Kits for FigmaOnline TrainingDocument Processing LibraryEmbedded Reporting for web and desktopWebKendo UI UI for jQuery UI for Angular UI for React UI for Vue UI for Blazor UI for ASP.NET Core UI for ASP.NET MVC UI for ASP.NET AJAX UI for Silverlight UI for PHP UI for JSPMobileUI for .NET MAUI UI for XamarinDocument ManagementTelerik Document ProcessingDesktopUI for .NET MAUI UI for WinUI UI for WinForms UI for WPF UI for UWPReporting & MockingTelerik Reporting Telerik Report Server Telerik JustMockAutomated TestingTest Studio Test Studio Dev EditionCMSSitefinityUI/UX ToolsThemeBuilder Design System KitDebuggingFiddler Fiddler Everywhere Fiddler Classic Fiddler Jam FiddlerCap FiddlerCoreExtended RealityUI for Unity XRFree ToolsJustAssembly JustDecompile VB.NET to C# Converter Testing FrameworkView all products OverviewSolutions Functional UI TestingRESTful API TestingLoad TestingRemote Test ExecutionAgile TeamsWeb Test AutomationDesktop Test AutomationResponsive UI TestingContinuous TestingServices Test Studio TrainingsTest Studio ConsultingResources Docs & SupportDocumentationVideosBlogsWebinarsWhitepapersCase StudiesWhats NewRoadmapRelease HistoryChangelogReport a bugSuggest a featurePricing Search Shopping cart Your Account Account OverviewYour LicensesSupport CenterForum ProfilePayment MethodsEdit ProfileLog out Login Contact UsRequest a demo Try nowclose mobile menu\nFeatures / Recorder / Advanced Recording Tools / Element Steps Tab /\nYou can build an image verification against specific elements for pixel-by-pixel visual verifications in tests. The image verification feature is based on an elements visual rendering rather than the properties or attributes of that element. An application with rich graphic rendering can leverage this functionality to automate some of its test scenarios that have always needed manual visual inspection to verify. The image verification in Test Studio allows you to refine your verification area down to the pixel level within an element and also assign a threshold for the matching.\nTest Studio is a test automation platform for web, WPF, and responsive web applications, supporting UI, functional, load, and RESTful API testing. Sign up for a free 30-day trial!\nThere are few things that could ensure more stable and reliable test execution:\nUse Image Verification when you need to verify an exact and static image, such as a logo, button, or icon.\nDo not use Image Verification to verify a specific color, an image with text content, or a dynamic slide show.\n2. Navigate to the application under test, for example www.Telerik.com.\n4. Hover over the image, for example the Progress Telerik logo in the header, and select Build Step... from the context menu.\n5. In the Advanced Recording Tools click Verifications > Image.\nBy default, Verify Entire Image is checked and the Threshold is set to 90%.\nWe recommend keeping the Threshold around 90%. A setting of 100% equals an exact match and the verification will fail, if it is off by a single pixel.\nBy default, Verify Entire Image is checked and the Tolerance is set to 1%.\nWe recommend keeping the Tolerance around 1% to 9%. A setting of 0% equals an exact match and increasing the tolerance means the verification is more forgiving.\n6. Uncheck Verify Entire Image to refine the comparison area. Either enter coordinates or drag the desired selection area within the image.\n7. Click Add Step and notice the Verify image step is added to the test.\nYou can configure the threshold and scrolling options that will be used during test execution.\nAfter the image verification step is recorded, you can make changes to the captured image and even recapture it. To do that, you need to select the image verification step and click on the image. This image will be associated with the test step, but all features and options for modifying element image are available. Only the Image Recognition Settings are not available, because the threshold is a test step option.\nIf you receive a failure on an Image Verification step, you can explore the step failure details for more details. In addition to the failure details, you can analyze the expected and actual image under the Images tab.\nCopyright  2023 Progress Software Corporation and/or its subsidiaries or affiliates. All Rights Reserved.\nProgress, Telerik, and certain product names used herein are trademarks or registered trademarks of Progress Software Corporation and/or one of its subsidiaries or affiliates in the U.S. and/or other countries. See Trademarks for appropriate markings."
  ],
  "metadata tracking image edits": [
    "7 Free Tools To Change Photo's Exif Data, Remove Metadata And Hide Dates\nTemplatesFree PowerPoint TemplatesFree Creative Resume / CV TemplatesFree Instagram Stories TemplatesMoney12 Robo-Advisor  Invest Stocks Guarantee Returns28 Warren Buffets Advice On Wealth7 Affiliate Programs For Making Money OnlineeFaxMicrosoft Windows Fax2 No Credit Card Online Fax Services8 Android iOS Faxing AppSarcastic43 Sarcastic Quotes For Your Boss33 Anonying Birthday Wishes For Friends71 Insults To Use Before Un-FriendingAntivirusDownload Norton 360 and Internet SecurityTop 8 Free 90 days Full Version AntivirusFree Kaspersky Internet SecurityPranks4 Broken Screen Smartphone WallpapersWhat is Wrong With Her?Weeping Angel Desktop PrankCats40 Funny Cat Comic Drawings10 Cat Optical Illusions12 Tombstone Quotes For Pets\n40 Funny Doodles For Cat Lovers and Your Cat Crazy Lady Friend\n120 Free Airport WiFi Passwords From Around The World\n4 Ways To Boost And Optimize Wireless WiFi Signal Strength And Speed\n6 Virtual SIM Phone Number App For iOS And Android Smartphones\n6 Best VPN for Gaming  No Lags, 0% Packet Loss and Reduce Ping ms\n7 Free Apps To Find, Spy And Track Stolen Android Smartphone\n10 Best Free WordPress Hosting With Own Domain And Secure Security\n10 GPS Tracker For Smartphones In Locating Missing, Abducted And Kidnapped Child\n7 Laptop Theft Recovering Software with GPS Location Tracking and SpyCam\nDownload the New Norton Antivirus, Internet Security\nTop 8 Free 90 days Full Version Antivirus Software Trial for Norton, Kaspersky, AVG, Trend Micro and more\nDownload Free Norton 360 Version 7.0 OEM for 90 Days Trial\nDownload Free AVG Internet Security With 1 Year Serial License Code\nMicrosoft Fax Software, How to Send Free Fax Online via Computer and Email\n10 Best Mac OS X Anti-Spyware and Anti-Virus Software for Free\nDownload Free Kaspersky Internet Security Antivirus for Windows 8\n6 Months Free Genuine Serial Number For Panda Internet Security Antivirus 7 Free Tools To Change Photos Exif Data, Remove Metadata And Hide DatesUpdated: January 29, 2023 / Home  Freeware and Software ReviewsHow to change the data and properties on a Jpeg, Tiff or RAW image file format? If youre searching for a freeware that does simple exif editing, look no further. Microsoft Windows Explorer has the capability to edit exif data. You can change the description, tags, authors, copyright information, data modified and many more.\n 4 Free FBI & CSI Digital Photo Forensic Tools To Analyze Fake PhotosHowever, for those looking for a more powerful metadata editor, this article has a simple list of freeware that enables you to change exif data of photos. Just for your information, photo metadata allows information to be transported with an image file, in a way that can be understood by other software, hardware, and end users, regardless of the format. By modifying these metadata, it doesnt mean you have successfully fooled the police, there will always be digital breadcrumbs that are traceable. 01  AnalogExif | Windows | macOS\nAnalogExif is a free metadata editor for the scanned films and DSC-captured digital images. Modification of the most EXIF, IPTC and XMP metadata tags for JPEG and TIFF files.Modification of the most EXIF, IPTC and XMP metadata tags for JPEG and TIFF filesEquipment library to store metadata properties of the film cameras and other analog equipmentCustom XMP schema for film camera properties (e.g. film name, exposure number etc.) and user-defined XMP schema for extra flexibilityBatch operations (copy metadata from another file, auto-fill exposure number)Customizable set of the supported metadata tags 02  ExifTool GUI | WindowsThis is perhaps one of the most powerful Exif editors. Runs on both Microsoft Windows and macOS, ExifTool is a powerful editor that reads, writes and edits meta information in a wide variety of files.ExifTool supports many different metadata formats including EXIF, GPS, IPTC, XMP, JFIF, GeoTIFF, ICC Profile, Photoshop IRB, FlashPix, AFCP and ID3, as well as the maker notes of many digital cameras by Canon, Casio, FLIR, FujiFilm, GE, HP, JVC/Victor, Kodak, Leaf, Minolta/Konica-Minolta, Nikon, Olympus/Epson, Panasonic/Leica, Pentax/Asahi, Phase One, Reconyx, Ricoh, Samsung, Sanyo, Sigma/Foveon and Sony.See More  FREE Adobe Photoshop Express Online Editor App For Android, iOS and Windows 03  Exif Pilot | WindowsExif Pilot is a Free EXIF Editor that allows you to manipulate METAdata within image files. The free version doesnt come with batch processing, youll be required to pay a small fee for the addon plugin.View EXIF, EXIF GPS, IPTC, and XMP data.Edit, create EXIF, EXIF GPS, IPTC, and XMP.Remove EXIF, IPTC tags, and clean up all metadataAdd, edit, and remove keywords.Import/export EXIF and IPTC from/to XML files.Import/export EXIF and IPTC from/to MS Excel files.Import/export EXIF and IPTC from/to Text files in CSV format. 04  EXIFManager | WindowsEXIFManager is a free software that allows users to quickly enter image description / user comment / author on a set of pictures, inside the EXIF data and also to rename pictures files using EXIF data and a powerful format editor. 05  Metadata++ | WindowsMetadata++ is a powerful yet easy to use freeware tool to view, edit, modify, extract, copy metadata of various file formats. Metadata++ is designed to be as fast as possible: quick navigation among directories, fast directories reading, fast and smooth painting of panels and thumbnails. Metadata++ is portable, it is configured to store all configuration and settings in the same folder that it is run from. 06  EXIF Date Changer Lite | WindowsAlmost all digital cameras record and store various camera settings, scene information, date/time the photo was taken etc as Exchangeable Image File Format (EXIF) data. This metadata is stored within the image file and wont get lost or overwritten when copying or transferring your photos to other computers. EXIF Date Changer Lite enables you to quickly and easily adjust the date/time taken on your photos and scanned images.Adjust or set the  date time taken of your digital photos  Compensate for incorrect camera settings or different time zones within seconds across an entire folder of images.Process selected images or batch process entire folders of images  Quickly and easily select the required images by either selecting a folder or individual images.See More  5 Free Photoshop Alternatives - Best Photo & Image Editor Online 07  GeoSetter | WindowsGeoSetter is a freeware tool for Windows for showing and changing geo data and other metadata (IPTC/XMP/Exif) of image files (e.g. images taken by digital cameras). The embedded map of GeoSetter doesnt work on Windows, please see my short explanation here.Reads and writes the formats JPEG and TIFF as well as camera RAW formats DNG (Adobe), CRW, CR2 and THM (Canon), NEF and NRW (Nikon), MRW (Konica Minolta), PEF (Pentax), ORF (Olympus), ARW, SR2, SRF (Sony) and RAF (Fujifilm), RW2 and RAW (Panasonic), RWL (Leica)Uses ExifTool by Phil Harvey for writing dataShows existing geo coordinates, image directions and tracks on embedded Google Maps map (requires internet connection)Setting geo data by using embedded Google Maps map (requires internet connection) or by entering known values for coordinates and altitude directlyAutomatic filling of location IPTC fields and altitude values (requires internet connection)Editable IPTC data (IPTC-NAA/XMP)Possibility to change taken date of imagesSynchronization with track files (NMEA, GPX, PLT, Sony LOG, IGC and others)Synchronization with already geo tagged images with buddy images (e.g. between RAW images and their corresponding JPEG images)Google Earth exportRecommended for you:\n100 Free Ransomware Decryption Tools To Remove And Unlock Encrypted Files\n4 Free Tools To Remove Stubborn AdWare Toolbars, Pop-Up Ads Or Browser Bundler\n4 Free Bootable Tools To 100% Securely Wipe Clean SSD HDD  Making Data Unrecoverable Updated On January 29, 2023 / Category: Photoshop & Image Editing / Author:\n]Why to fake camera details? I believe its better to delete them with the help of tools such as EXIFCleaner by Superutils, instead of putting inaccurate data.\n]Exif data typically includes the date and time the picture was  right clicking on it and selecting Properties from the menu\n]Having scanned photos that are about 60 years old, I dont want the scanned date (ie today) as date taken.\n]If you have an older version of photoshop you can open a raw file created from a\nno need to spend money in a new photoshop version. So it has its advantage to change the exif camera info, saves you money\nSure, I could do it in Lightroom, but if the Lightroom catalog becomes corrupt, I lose all that data.\n]When I put photos into my girlfriends photoframe, the orientation is wrong, despite rotating on the computer. I need to get into the metadata to change the orientation there.\n]Because files on old PhotoCDs (from the early 2000s) ALL have the same EXACT date and timestamp, and only valid on the day the film negative was processed and scanned.\n]because if you are an activist and want to disseminate information, then you dont want the FBI tracing you through metadata. The head of the FBI actually stated we kill people with metadata\n]I use a tool called MetaCleaner, easily the best I have used.\n]If you shoot film and scan the negatives, the EXIF data will be for the scanner not the camera. Tools like this are then vital, you arent faking anything, you are providing the true information about the camera, lens and film used to take the picture.\n]why not Download EXIF Date Changer 3? it is simple and easier\n]Hi. If you are using windows, Quick Exif Editor. I am now trying to find a decent one for my Mac.\n]Hello Mates, Try the MetaEditor by Picvario, its online free program, so you can edit metadata from any device.\n]are there any more Tools for Reading and Editing Exif Data of Photographs apart from this? thanks!\n]How to change the data and properties on a Jpeg, Tiff or RAW file format?\n]EXIF Date Changer will update JPEG and RAW file formats but for batch updating of many files at once, it does require the Pro version.\n]great tutorial on how Can I Edit My Photo Metadata? thanks mate\n]this can be done easily with the View EXIF Digital Photo Data in Mac OS X\n]You can try other 3rd party software, they are:Google Picasa  http://picasa.google.com.my/ GeoSetter  http://www.geosetter.de/en/Adobe Photoshop CS and Element can perform similar task too\n]I know free online tool for editing EXIF, IPTC data: https://www.imgonline.com.ua/eng/exif-editor.php Or here you can delete all metadata from jpeg picture without loss of quality: https://www.imgonline.com.ua/eng/delete-exif.php\n]If you want to process only a few photos, I suggest using an online tool. For example you do not need to install anything or for a long time to understand the options. Fix can not only Exif :-)\n]You can also use KrojamSoft PhotoViewerPro.You can choose from wide range of eye-catching effects! And control the intensity of Looks..\n]You are a group of gentlemen and are of good will.\nSome men are not and take photos to prove they are somewhere (at a store) when they are in fact cheating in some way.\nIf I have received 100 photos, all with exif data, and then all of a sudden receive a random shots while hes grocery shopping or working out that are stripped?\nAnalogExif was a real life saver for me, as Id spent the last couple of hours searching for something that did all those settings (ISO, aperture, etc.) and I either ended up stubling upon someones recommendation of ExifTools (which, though a great tool, didnt have anything in its documentation saying it could do just that) or some defunct hybrid Android/iOs-Windows app.\n]Im very suspicious of a particular sample image on the Canon website,\n50 megapixel camera. There is an image taken from (most likely a helicopter) , of Yokohama, Japanits clarity and resolution is beyond even what an 810 film camera could do. Ive worked with large format field cameras for over 30 years.\nIts simply impossible resolution. I have printed this image out at 5436 inches. Its not to be believed. Thus, i dont trust the exif data in this image. Could the exif metadata have been altered? I do not want to spend $5,000 on this camera, only to be duped in the end.\n]Analog Exif just never installed. Ran through super fast and left two unusable files on its folder. Win 7 HP 64\n]What is this? ExifTool, when I followed the instructions, flashed a command window for 1/10th second and that was it. Useless. Thats 2 of 3 now.\n]ExifTool, as is, is a command line tool, so, its not accessible for everybody but really impressive and practical for batch processing. A GUI exists for ExifTool, called ExifToolGUI, and I think you have to try this to change your opinion about this software\n]Is there any way to recover pictures which were regained by recovery tools after previously deleted. Now pictures are unreadable. I expected that internal data structures are partially deleted. Since they have a quite large size so I think it may contains most of the HEX data.\n]Best program hands down is the GEOSETTER!Forgot link to site! http://www.geosetter.de/en/Totally free but you are welcome to donate. I use it to help people with massive photo libraries that need EXIF data updated with information on photos taken before this information was embedded.Then when importing to Google Photos, Google Photos is able to read location data etc.Bulk editing and ability to create favorites to apply to large quantities of photos!\na better tool, for managing metadata . Free for personnal use.\n]Im looking for a plugin that lets me edit/remove EXIF data into WordPress.Is there a plugin available for that?Thank you\n]There are plugins that remove exif data when you upload, as for after you upload, I am not sure.\n]Metadata+++ the best program to fix your metadata mistakes like forgetting to change the clock to your camera when traveling to another time Zone or when changing from winter time to summer time and also fixing GMT difference with your local time with your JPG and MP4 videos (Canon Cameras base there time with GMT) you can fix everything with this APP, FIVE STARS to this APP\n]You can also change EXIF data on your images using Namexif.\n]Try to use Exifer. IMHO its the best solution for edit and manage EXIF/ITPC metadata. And its free! Sad, but it doesnt work perfectly under Windows 10. Latest version is 2.15 (build 263) from year 2002 and will no more updated :(\n]Is there a way to edit EXIF data inside photoshop? Im trying to change gps data.\n]Is there a way to edit EXIF data in RAW (ARW) pictures ? I would like to fill the lens informations data (brand, model & max. aperture) when I use not recognised lenses (old stuff from other brand) on my camera. And if possible, for a lot of pictures at once.\n]Hi, I think u can take a photo from your camera. After that, export this metadata. Lastly, import them to your target photos.\n]Metadata ++ is the best for import Metadata from a photo to another. But it still have some bugs when using. Such as bug when right click on need-to-change-metadata photo and select import. You can use another way to do this.\n]Hey, I want to know about ExifRead Software, is that also help to change the metadata or other info?.\n]ExifToolGUI is best (portable windows 64 version) Metadata ++ use too exiftool.exe for editor Thanks\nOnly programs obtained via the Apple Store will be allowed.\n]Question: is there a tool that updates the file date/time to the date/time that the photo was taken (i.e from the Exif data)?\n]JpgKeywords  Specialized only for keywords in JPG files (IPTC metadata). Does only one thing, but does it well. Useful if you want to tag many personal photos.\n]I noticed just that when I edit JPEG metadata with Windows 7, example if I add tags or something text to Author place then the file size drops little bit. Is there bug in Windows??\n]I use GeoSetter for many, many years now, its very intuitive and has a lot of options\n]BatchPurifier Lite is a freeware for Windows that lets you easily remove all metadata from jpg pictures fast.Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment *Name * Email * Website\nThis site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.\nGeckoandfly grew from strength to strength to be one of the many popular blogs around the world. It started out as a hobby and one thing lead to another, here we are now. Our goal is to create simple tutorials and beautiful quotes for the average user. More at About Us page. Our top articles:\nCounter Fake News - CNN, Fox News, & CNBC 6 Tips Boost Wireless WiFi - Speed & Signals 12 Free CV Templates - Office / Google Docs 10 BitCoins Alternatives - Cryptocurrencies Mining 11 Classified Scripts - Craigslist & eBay25488 viewsFree Norton Antivirus and Internet Security 2020  90 Days Trial21548 viewsDownload FREE 30-Days Norton Security Standard 2020 With Smart Firewall12530 viewsDownload FREE Norton Security Premium 2020 With 30-Days Trial8825 viewsTop 16 Free 60, 90 & 180 Days Antivirus Trial  Norton, Kaspersky, AVG, Avast, BitDefender And More8447 views9 Best Gaming VPN  No Lags, 0% Packet Loss And Reduce Ping msSearch\nCheck out these top articles, we believe you will find them useful, use Google Translate for other languages. Do share them on Facebook, Twitter, LinkedIn, YouTube, Pinterest and Instagram. Thanks.\n6 Free Antivirus - Hotel, Business & Commercial 8 Android iOS Faxing App - eFax via Tablets Download Norton Antivirus - Free License Key 4 Live Camera App Translator - Translate English To Spanish No Credit Card Online Fax - Free Internet Fax Free Kaspersky Internet Security - Best Antivirus 14 Free VPN - Unblock Netflix & Hulu\n6 Blue Light Filter For PC & macOS - PayPal & Wikipedia 6 Virtual Number For Smartphone - Complete List 5 Anti-Malware With 60+ Scanners - Social Security 60 Ransomware Decryptor - Crack Encryption 2006 - 2023 GeckoandFly. Some of the content is copyrighted to Geckoandfly.com and may not be reproduced on other websites. If we made any mistakes, email us, we apologize in advance. Read our Privacy Policy for more information. Google serves cookies to analyze traffic to this site and for serving personalized ads, visit this link to opt out. By accessing geckoandfly.com and navigating without modifying your parameters, you accept the use of cookies or similar technologies. This is in order for us to provide you with the best services and offers adapted to your interests. But most importantly, a more secure experience on our website.",
    "Extract Metadata from an Image | Online Web App | Brandfolder\nCustom templates, on-brand and on-demand, that don't cost the creative team extra time\nAlways up-to-date, accessible anywhere brand guidelines\nLearn what Digital Asset Management is, and the many ways it can benefit your business.\nA free set of tools designed to make features of a DAM platform available to everyone.\nExplore and uncover what happens behind the scenes of some of the worlds most interesting brands.\nGet to know the Brandfolder platform! Find out why brands trust us to manage their assets.\nYour free resource library of tools to power your assets.\nExtract metadata recorded behind your files, ranging from file size and modification history to the software tools used to create them.\nThe file converter will allow you to convert jpg to png, png to jpg or another file type. Upload your files and change their file type from your browser, and then download.\nThe text extractor will allow you to extract text from any image. You may upload an image or document (.pdf) and the tool will pull text from the image. Once extracted, you can copy to your clipboard with one click.\nResize images without losing quality, while maintaining the aspect ratio to avoid skewing and distorting.\nThis simple color palette generator extracts colors from any photo or file on upload. Then just copy the HEX codes from the color palette app with a simple click!\nDiscover how AI auto-tagging technology uses image recognition to analyze and generate tags based on objects detected within images.\nWhen you're ready, we'll put you face-to-face with the #1 rated DAM based on user reviews. Let's get something on the calendar and give all your assets a happy new home.\nLearn how to extract and use metadata to organize your brand's digital asset library with these free resources from Brandfolder.\nHow to Create Content Taxonomy: 5 Steps to a Strategy\nHow to Organize Digital Photos: 8 Methods for Todays Brands\nNope! Workbench is 100% free, up to 5 uses a day per user. No license, subscription or even email addresses required.\nWill you be adding new tools to Workbench in the future?\nYes! Our team will continue to build out useful resources and release them over time. To get full access to all digital asset management features, check out Brandfolder's DAM software.\nAI auto-tagging is a feature utilized by Digital Asset Management (DAM) platforms to help users save time by eliminating manual work, encouraging organization, and making files easy to locate.\nThe technology works by analyzing objects within an image and generating a set of tags returned from a machine learning system. Based on a confidence score, the tags with the highest likelihood of accuracy will be applied to the image. When used within a DAM software like Brandfolder, metadata and auto-tagging provide a convenient method to search by. You can read more about metadata auto tagging in our blog.\nThe Workbench color palette generator extracts a series of HEX colors from an image upon upload. It counts every pixel and its color, and generates a palette of up to 6 HEX codes of the most recurring colors.\nMetadata provides information about an asset's content.\nFor example, an image may include metadata that describes how large the picture is, the color depth, the image resolution, the creation date, and other data. A text document's metadata may include information about length of document, the author, publish date, and a short summary of the document.\nDigital Asset Management (DAM) has, in recent years, become a critical system for companies of all industries and sizes. A DAM is a software platform brands use to store, edit, distribute and track their brand assets. DAMs are intended to encourage the organization of a company's digital architecture, eliminating the use of buried files and folders typically housed in Google Drive or Dropbox.\nDAM systems scale to store massive quantities of digital assets, including but not limited to: photos, audio files, graphics, logos, colors, animations, 3D video, PDF files, fonts, etc. In addition to meticulous organization within the DAMs central file system, these files are discoverable using unique identifiers such as their metadata and tags (auto and manual).\nWhen used for distribution, DAMs encourage asset permissioning and expiration, ensuring only the correct content is available to the correct recipient for a specified amount of time. Once published or distributed, DAMs can analyze how, where and by whom assets are being used.\nDigital asset management platforms are used by marketing, sales and creative teams at some of the world's largest brands. Want to learn more about how a DAM could benefit your team? Sign up for a free Brandfolder trial or schedule a demo with one of our DAM experts here.\nCopyright @2023Brandfolder Digital Asset Management",
    "Image metadata is text information pertaining to an image file that is embedded into the file or saved to a separate file that is associated with the image file.\nImage metadata includes details relevant to the image itself and to its production. Some metadata is generated automatically by the system capturing or creating the image. Additional metadata can be added manually and edited through dedicated software or general image editing software, such as GIMP or Adobe Photoshop. Metadata can also be added directly on some digital cameras.\nImage metadata is often divided into three main categories:\nTechnical metadata is mostly generated automatically by the device or software that creates the image. For example, if the image is a photograph taken by a digital camera, the camera typically generates metadata about the camera and the photo's settings, including aperture, resolution, focal length, shutter speed, ISO speed, camera brand and model, date and time when the image was created, and the GPS location where it was created. Cameras usually generate more technical data than devices or software such as scanners, screenshot tools or draw programs.\nDescriptive metadata is mostly added manually using special software such as GIMP or Affinity Photo. The individual who creates or manages the image can use the software to add or edit the descriptive metadata. The metadata might include the name of the image creator, keywords specific to the image, captions, titles, comments or other information. Effective descriptive metadata can make it easier to search for images.\nAdministrative metadata is like descriptive metadata. Most of the metadata is added manually using special software. The metadata might include usage and licensing rights, restrictions on reuse, contact information for the image owner or similar types of information.\nVarious standardized formats are used for metadata. The following four formats are the most common ones:\nExchangeable Image File (EXIF or Exif). The format is used extensively in digital cameras, smartphones and other devices when generating image metadata. EXIF data can include a range of information and often represents the bulk of metadata in an image file. It includes data such as image width and height, aperture value, exposure time, camera model and more.\nInformation Interchange Model (IIM). The metadata in IIM provides individuals and organizations with a way to add details to images such as titles, genres, instructions, owners or creators, location and contact information, copyright and attribution specifications, and similar types of information.\nInternational Color Consortium (ICC). This metadata includes details about the color profile embedded in an image. ICC is an organization that defines and publishes open standards for image color management.\nExtensible Metadata Platform (XMP). The metadata is an XML-based format that can accommodate a wide range of information. This format was created by Adobe but is now an ISO standard that has been adopted by Adobe and other vendors. XMP can incorporate both EXIF and IIM metadata.\nFigure 1. Photo open in Affinity Photo program with a panel on the right showing some of the image metadata\nMany image editing programs make it possible to view and edit an image's metadata. For example, Figure 1 shows a photograph (sunset.jpg) that was taken with an iPhone. The photo is open in Affinity Photo. Next to the photo is a panel that shows some of the image metadata. Most of the metadata was generated by the phone and is included in the EXIF section, although one of the XMP sections shows a small amount of metadata generated by Affinity Photo.\nMetadata is usually added to an image file directly, along with the bits that define the image itself. With an application such as Hex Fiend, you can pick out pieces of the metadata text from the binary data. Figure 2 shows the binary data for the sunset.jpg file. The screenshot starts at the beginning of the file, where the EXIF data has been added. The four bytes that represent the term EXIF are highlighted in the screenshot, along with the matching text.\nFigure 2. Binary data for the sunset.jpg file (the same image opened in Affinity Photo in the previous image)\nImage metadata can also be stored in a file separate from the main image file. This type of file is often called a sidecar file and must accompany the image file to ensure that the metadata is available to the image when needed. Sidecar files commonly use XMP to format the metadata. Figure 3 shows part of the contents of an XMP file that was generated for the sunset.jpg image. Notice how the data is presented in an XML-based format.\nFigure 3. Image metadata can be stored in a file called a sidecar file separate from the main image file. This shows part of the contents of an XMP file generated for the sunset.jpg image.\nImage metadata can be useful for cataloging and contextualizing visual information. Many photographers and other visual artists provide information about themselves and their images within the metadata.\nImage metadata can also help protect intellectual property. However, including copyright information in the metadata is not adequate protection as it can easily be stripped away. Also, as with other types of content, metadata security can be cumbersome, requiring extra measures to safeguard image metadata and protect it from unauthorized access.\nLearn best practices for enterprise image data storage.\nSeeking truth in crisis times shows importance of metadata\nAI-assisted image and video search is next content frontier\nHow automated metadata management improves business insights\nWhy version control is necessary in digital asset management\nCommodity hardware in computing is computers or components that are readily available, inexpensive and easily interchangeable ...\nCommon Service Center (CSC) is an initiative by the government of India to establish locations with computers that are freely ...\nMini-ITX is a compact motherboard configuration designed to support relatively low-cost computers in small spaces such as in ...\nGoogle Bard is an AI-powered chatbot tool designed by Google to simulate human conversations using natural language processing and machine learning.\nWhat is Web 3.0 (Web3)? Definition, guide and history\nSecure access service edge, also known as SASE and pronounced sassy, is a cloud architecture model that bundles network and ...\nThe Network Configuration Protocol (NETCONF) is an Internet Engineering Task Force (IETF) network management protocol that ...\nGeo-blocking is blocking something based on its location.\napplication blacklisting (application blocklisting)\nApplication blacklisting --increasingly called application blocklisting -- is a network or computer administration practice used ...\nClaims-based identity is a means of authenticating an end user, application or device to another system in a way that abstracts ...\nCertified Cloud Security Professional (CCSP) is an International Information System Security Certification Consortium, or (ISC)2,...\nGenerally Accepted Recordkeeping Principles (the Principles)\nGenerally Accepted Recordkeeping Principles is a framework for managing records in a way that supports an organization's ...\nA learning management system is a software application or web-based technology used to plan, implement and assess a specific ...\nThe Information Age is the idea that access to and the control of information is the defining characteristic of this current era ...\nA human resources generalist is an HR professional who handles the daily responsibilities of talent management, employee ...\nThe employee lifecycle is a human resources model that identifies the different stages a worker advances through in an ...\nCandidate experience reflects a person's feelings about going through a company's job application process.\nInbound marketing is a strategy that focuses on attracting customers, or leads, via company-created internet content, thereby ...\nAccount-based marketing (ABM) is a business-to-business (B2B) strategy that focuses sales and marketing resources on target ...\nSpeech analytics is the process of analyzing voice recordings or live customer calls to contact centers with speech recognition ...",
    "Top 5 ways to view and edit metadata | Daminion Blog\nBy using this site, you agree to the Privacy Policy and Terms of Use.Accept\nOrganize, centralize, secure and manage your digital assets in one place\nLearn More About Daminion Digital Asset Management Features\nAt Daminion we dont believe that the cloud is meant for every technology platform or for every creative team\nIllustrator, Photoshop, Microsoft Office, API and other\nMarketing Teams Video Production Design Teams Sales Teams GIS Specialists\nDAM tool Game Studios, Media Production, Creative Agencies, and TV Channels\nDAM solution for Educational Institutions, Universities, and Schools\nDAM platform for Architecture, Construction, Real Estate and Engineering\nSelf Hosted DAM for Government Organizations, CMAs, and Councils\nDAM software for Water Management, Funds, and Not for profit organizations\nAbout Us Whats New Supported Formats Tutorial Technical Support System Requirements Partners\nCore information about Daminion, its benefits, and more\nThis online documentation describes the commands and menus of the Daminion interface\nThis checklist will help to answer yourself about the necessary DAM solution for your team. Simple questions to make the right decision.\nMetadataisdatathat provides information about a particular file or image. We can also call it a characteristic of the file. However, metadata is not the safest place to backup your data, as it contains sensitive file information. ContentsViewing and editing file metadataHere are the steps to do it onWindows:How to View Metadata inPhotoshopHow to change metadata inInDesignTypes of metadataTop 5 programs for editing metadataAnalogExifEXIF.toolsImBatchExif PilotCan You Remove Metadata?Conclusion It is essential to understand how to work with such data (view, edit, delete). In addition, I will explain how to manage metadata using effective programs and explore the top 5 tools that can help you organize your work with metadata properly. In this article, I would like to talk about the meaning of metadata in images. Photo metadata is the information and details about a particular image or file. Thisinformationcan include:\nFilename Date created File size Image dimensions Color depth Keywords or tags Usage rights Copyrights File type, and more\nDue to its structured appearance, metadata is readable not only by people but also by computers. As a result, it can be processed by machines and used for various purposes, such as indexing, searching, merging, and automatic processing.\nViewing and editing file metadata The amera or scanner automatically generates metadata, and you can check the content in file property windows in all types of devices. Here are the steps to do it onWindows:\nSelect properties after right-clicking on the image. Select the details tab. Now, add the metadata to description, origin, author, and go through the following steps.\nAt the bottom of the properties, click on remove properties and personal information and select the data you want to delete if youre looking to remove any metadata.\nOpen the image for which you want to check the metadata. Head to theFilemenu, then clickFile info. And you can also pressCtrl + Alt + Shift + Ion Windows andCommand + Option + Shift + Ion Mac. From here, you can copy or edit the metadata. ClickOKto save your changes.\nNote:You cant edit the entire metadata, for example you cant edit information about the camera or data of file creation.\nAssets and metadata How to change metadata inInDesign Edit metadata in image files\nIn InDesign, right-click (Windows) or Control-click (Mac OS) the image, and then choose Edit Original.  In the original application, select file> File Info. Edit the metadata, and then click OK.\nIf you are an amateur and need to view and edit the metadata in some instances, we recommendPhotoscape.  If you need more capabilities to edit metadata. Here are our top 5programs. Types of metadata Before we move on to the next chapter, lets look at the basic definitions of metadata types: EXIFExchangeable image file format (officially Exif, according to JEIDA/JEITA/CIPA specifications) is a standard that specifies the formats for images, sound, and ancillary tags used by digital cameras (including smartphones), scanners and other systems handling image and sound files recorded by digital cameras.IPTCThe Information Interchange Model (IIM) is a file structure and set of metadata attributes that can be applied to text, images and other media typesXMPThe Extensible Metadata Platform (XMP) is an ISO standard, originally created by Adobe Systems Inc., for the creation, processing and interchange of standardized and custom metadata for digital documents and data sets.ID3ID3 is a metadata container most often used in conjunction with the MP3 audio file format. It allows information such as the title, artist, album, track number, and other information about the file to be stored in the file itself. Consequently, there are different types and formats for recording and storing metadata. In this article, we will not delve into the specifics of each type. However, if you or a technical specialist of your company have any questions about how to edit a specific type of metadata, dont hesitate to get in touch with Daminion support service; we will try to offer you the best solution.\nView and edit metadata in Daminion Top 5 programs for editing metadata Daminion A tool that can update metadata (EXIF / IPTC / XMP / ID3 / Office) in vector and bitmap images, video and audio files, office documents. Uses ExifTool and its libraries for working with metadata. Please feel free to read the article about metadata in Daminion:Metadata mapping Rules Daminion automatically synchronizes database information with EXIF, IPTC, XMP, MWG, and format-specific metadata in a wide range of other media files, including PDF and Camera RAW formats. Daminion stores all asset information in tags. Examples of tags are Author, Keywords, File size, Media format, People, Place, etc.\nEdit metadata in Daminion Mapping Tags to Metadata Each tag is mapped into a set of metadata. ThePlaceTagis mapped into eight metadata fields:\nIPTC:Country, IPTC: State, IPTC:City, IPTC:Location XMP:Country, XMP: State, XMP:City, XMP:Location\nAny changes to the Place Tag are reflected in the metadata. So, the Place Tag will be filled out from information extracted from the IPTC/XMP, during the import. AnalogExif AnalogExif is a free metadata editor for scanned films and DSC-captured digital images. Modify the most EXIF, IPTC, and XMP metadata tags for JPEG and TIFF files. EXIF.tools A package of utilities for the string system allows you to retrieve, enter, delete, copy, and replace metadata. One of the fullest Exif editors. So, it runs on both Microsoft Windows and macOS, ExifTool is a powerful editor that reads, writes and edits meta information in a wide variety of files. ImBatch Utility for batch processing of images. In addition to the possibility of batch processing of metadata, it has a tool for editing metadata in a separate file. Exif Pilot Exif Pilot is a Free EXIF Editor that allows you to manipulate METAdata within image files. Unfortunately, the free version doesnt come with batch processing; youll be required to pay a small fee for the addon plugin.\nHow to use DAM system Can You Remove Metadata? Yes, you can remove the metadata. Different programs and software can work with specific files or with certain information in the metadata. Metadata is an essential part of data, so that, if the image you share must have detailed data, you must edit the metadata in advance. Because if you upload an image to the site, then any user will have access to the metadata of this file.\nExample of DAM usage Conclusion In conclusion, metadata plays an increasingly important role in todays digital landscape. By properly utilizing metadata, you can greatly improve the organization and management of your files.\nAdditionally, the ability to edit or delete specific data types can help secure sensitive information when needed. However, its crucial not to neglect your work with metadata. To experience the benefits first-hand, consider booking a free trial with Daminion.\nCMO at Daminion Software. Talks about #daminion, #marketing, and #digitalassetmanagement.\nHow to tag images? AI Auto-Tagging. Magic Or Smart Solution?\nThe best architecture bureaus in the USA. What unites them?\n Sustainable Technology: Unveiling the Role of Digital Asset Management \nMarketing Automation: Future Forecast on the Trends and Triumphs of \nBoost  Your Business with Online Collaboration Tools \n Mastering Marketing Touchpoints: Unlock the Power of Customer Engagement \nGet the latest tips and tricks, expert advices, insightful interviews delivered monthly\n Daminion Corp. All Rights Reserved. Daminion Articles",
    "How to View, Edit, and Remove Location and Other EXIF Data From Your Photos | Gadgets 360\nHomeAppsApps FeaturesHow to View, Edit, and Remove Location and Other EXIF Data From Your Photos\nHow to View, Edit, and Remove Location and Other EXIF Data From Your Photos\nIt also includes information such as name of the camera, capture settings\nIt can be removed on all platforms including Windows, Android, macOS, iOS\nExchangeable Image Format, also known as EXIF, is a set of data thats attached to every image you take. Most cameras and smartphones these days add basic parameters such as the mode in which the photo was taken, the shutter speed, ISO, aperture data, and sometimes even the location of the photograph. This presents a peculiar problem  when you click a picture of a beautiful bird perched on the windowsill in your house, your camera automatically adds the location of the picture, which could reveal your home address when you share it with others online. Most social networks such as Facebook, Instagram, and Twitter remove all of this data from photos when you upload them. However, when your photo is shared via email or cloud storage services such as Google Drive or Dropbox, this data is still present on pictures. To protect your privacy, its best to remove EXIF data from images. Heres how you can do it.The Best Free Photo Editing Apps on AndroidHow to view, edit, and remove EXIF Data including location on AndroidFollow these steps to view EXIF data on your Android smartphone. Open Google Photos on the phone - install it if needed. Open any photo and tap the i icon. This will show you all the EXIF data you need.To remove EXIF data from your photos, youll need a third-party app such as EXIF Eraser. Once youve installed the app, follow these steps. Open EXIF Eraser. Tap Select Image and Remove EXIF. Select the image from your library. The app will show you all of its EXIF data and tell you that itll remove it. Tap Ok.We tried a few apps that let you edit EXIF data and offer granular control on the EXIF data you can remove, but none of them worked flawlessly for us.If for some reason, you dont want location data saved with your images not too difficult to prevent your Android phone from doing that. Follow these steps to stop your Android phone from saving location with your photos: Open the camera app on your Android device and go to Settings by tapping the Gear icon. This varies from phone to phone as theres no standard camera app on all Android devices. After that turn off Store location data to prevent geotagging of photos. This option may have a slightly different wording but its there in pretty much every Android phones camera app.How to view, edit, and remove EXIF Data including location on WindowsWindows has a very good native metadata editor that lets you quickly view or remove EXIF data from images. Heres what you need to do. Go to the folder where your image is located. Right-click the image > click Properties. Click the Details tab. Click Remove Properties and Personal Information. Then you can click Create a copy with all possible properties removed for a copy of the photo with EXIF data stripped. Alternatively you can edit metadata by clicking Remove the following properties from this file. Once youre done, click OK.For bulk removal of EXIF data, you will need a third-party app such as IrfanView. Follow these steps to get the job done. Download IrfanView and then download all IrfanView plugins. Install both. Open IrfanView and press B on the keyboard. This will open the batch conversion menu in IrfanView. On the right hand side, select all the images you want to process and then click the Add button below. On the left side, click Options and uncheck the following three options Keep original EXIF data, Keep original IPTC data and Keep original JPG-Comment. Click Start Batch after selecting the directory you want the output files in. Now all your photos will have been stripped of EXIF data.How to view and edit EXIF Data including location on macOSOn macOS, the Photos app does more than what it does on iOS. The app lets you view EXIF data and remove location data from your images. It doesnt let you edit or remove all parameters of EXIF data however. Follow these steps to view EXIF data on Photos for macOS and to remove location data too. Open Photos for macOS. Open the image you want to edit. Tap the i button on the top-right. Here you can view EXIF data in the photo and add a description and keywords if you wish. You can remove location data from a photo by clicking Image in the top bar and then clicking Location > Hide Location. You can also edit the date and time on the photo by clicking Image > Adjust date and time. Change the time and date and then click Adjust.To remove EXIF data entirely, youre going to have to rely on a third-party app such as ImageOptim. Heres how. Download ImageOptim. Click the + icon and select the images you want to strip EXIF data from. You can select multiple images too. The app will automatically remove EXIF data from the image.For greater control over this process, click the Gear icon and go through the options. These let you decide whether you want to reduce file size a lot at the cost of image quality or if you simply want to remove metadata without losing image quality.How to view and edit EXIF Data including location on iPhone and iPadOn iOS surprisingly enough you cant see much in terms of EXIF data via the Photos app. For granular control on viewing and removing EXIF data on photos from iPhone and iPad, you need to download a free app such as Metadata Remover or Photo Investigator (both free with in-app purchases). These apps let you edit and remove the EXIF data from photos, but editing EXIF data will require you to pay Rs. 249 to unlock the apps premium features. In Photo Investigator, follow these steps: Tap the gallery icon on the bottom-left. Select the picture you want to edit EXIF data for. To view EXIF data, you can tap the various icons below the image. To edit or remove EXIF data (after you pay for the app), tap Metadata. Now select Remove or Edit.You can achieve similar results from any other apps that offer this feature too, we just happened to use these two and found them to work.If you are privacy conscious, you might want to disable adding location data to your photos. Be aware that disabling geotagging of photos will result in the Places album being empty and weve found that album as a good way to search for old photos. Heres how to disable geotagging on iPhone and iPad. Go to Settings > Privacy > Location Services > Camera. Tap Never.We werent able to find any apps that reliably remove EXIF data in bulk on Android and iOS. Do you use any apps to bulk remove EXIF data on smartphones? Which apps do you use to view, edit, and remove EXIF data from your photos? Let us know via the comments. For more tutorials, visit our How To section.\nFor the latest tech news and reviews, follow Gadgets 360 on Twitter, Facebook, and Google News. For the latest videos on gadgets and tech, subscribe to our YouTube channel.\nThe resident bot. If you email me, a human will respond.\nLenovo S5 With Dual Cameras Teased Ahead of Launch, to Take on Xiaomi Redmi Note 5\nMicrosoft Phone Link for iOS With iMessage, Calling Support Rolled Out to All Windows 11 Users\nCall of Duty 2023 Reveal Set for Early August; Will Launch on November 10: Report\nThe Legend of Zelda: Tears of the Kingdom, Redfall, and More: New Games on PC, PS4, PS5, Switch, Xbox One, Xbox Series S/X in May\nWindows 10 Feature Updates Discontinued by Microsoft Ahead of End of Support in 2025\nMicrosoft Phone Link for iOS Begins Rolling Out to Windows 11 Users: How to Download\nHow to View, Edit, and Remove Location and Other EXIF Data From Your Photos\nAsus ROG Flow Z13 Acronym First Look: A Gaming Tablet From the Future\nHow To: Enable Maintenance Mode on Samsung Galaxy Smartphones\nSpam Calls on the Rise for WhatsApp Users in India: Know More\nRedmi A2, Redmi A2+    , 5,999    \n   7GB RAM  ,  7   !\nVivo S17, S17 Pro      ,   \nMotorola Edge 40  23        , 50    \nBGMI Unban: 90        BGMI!      \nVoltas 1.5 Ton 3 Star Inverter Split AC (183V Vertis Emerald 4503459)\nVoltas 1.2 Ton 3 Star Inverter Split AC (153V Vectra Prism 4503541)\nOnePlus Nord 3 5G Price in India, Launch Timeline, Live Images Leaked\nMotorola Edge 40 Price in India Revealed Ahead of May 23 Launch: See Here\nBGMI Returning to India Soon After 10-Month Ban, Krafton Confirms\nOppo Reno 10 Series to Go Official on This Date; Specifications Teased\nSamsung Galaxy F54 5G May Debut With This Price Tag\nRealme Narzo N53 Review: Sleek and Shiny, but Does It Stand Out?\nNothing Phone 2 Confirmed to Get This Snapdragon SoC: Check Here\nApple Releases iOS 16.5 With These New Features: Check Here\nHeres When the Realme 11 Pro 5G Series Will Launch in India\nRedmi A2, Redmi A2+ With 5,000mAh Battery Debut in India: Check Price\nOnePlus Fold, Oppo Find N3 Key Specifications Leaked, May Get Similar Features: All Details\nMortal Kombat 1 Announced With Skull-Krushing Trailer, Launches September 19\nApple Reality Pro Headset Will Feature External Display Showing Facial Expressions of User: Mark Gurman\nHong Kong Commences CBDC Pilot; Multiple Fintech Players Onboarded for Trial of e-HKD\nApple, Samsung Looking to Expand Production in India, Says MoS IT Rajeev Chandrasekhar\nTikTok Users in Montana, US, File Lawsuit Seeking to Block State's Ban on App\nApple Releases iOS 16.5 With New Pride Wallpaper, Sports Tab in Apple News, More: All New Features\nBitcoin, Ether Record Losses Amid US Debt Ceiling Discussions; Most Altcoins Fall\nByju's Alpha Accused of Hiding $500 Million From Lenders as Firm Battles US Lawsuit Over Control\nBGMI Returning to India Soon After 10-Month Ban, Krafton Confirms\n Copyright Red Pixels Ventures Limited 2023. All rights reserved.",
    "EXIF metadata privacy: A picture is worth a thousand data points - Comparitech\nBlogVPN & PrivacyEXIF metadata privacy: A picture is worth a thousand data points\nEXIF metadata privacy: A picture is worth a thousand data points\nAre the pictures you share online posing a threat to your privacy?\nWhen you snap a digital photo with your camera or phone, it stores more than just the pixels and colors that make up the image. Each image file also contains metadata, which includes details ranging from creation date and copyright info to the location where the photo was taken.\nThe same goes for images modified with many photo editing programs. Image editing programs often add metadata to images including modification timestamps, system info, and tracked changes.\nAn example of EXIF metadata stored with a photo, including the GPS coordinates where the photo was taken.\nMetadata can pose a privacy threat to people who share and post photos online. Although some social networks and photo storage and sharing sites scrub metadata from uploaded photos, many fail to do so, Comparitech researchers say, which could allow attackers to gather personal information from images posted online. For example, if someone posts a vacation photo with GPS coordinates and a timestamp in the metadata, an attacker could easily find when and where they traveled.\nMetadata can be categorized into three broad categories:\nSystem metadata is generated when the image is stored (i.e. when a photo is taken or edits are saved). It includes specific labeled criteria, like the date and time the image was created and details about the camera and/or editing software used\nSubstantive metadata include the contents of the actual file, such as tracked changes to an edited image\nEmbedded metadata include data entered into a document that is not normally visible, such as formulas in an Excel spreadsheet\nImage metadata can be embedded internally in common image file formats like JPEG and PNG. Such image data is usually stored in Exif (exchangeable image file format). But it can also exist outside the image file in a digital asset management (DAM) system. These are sometimes referred to as sidecar files, and are often stored in the XMP format.\nDescribing the contents of the file, including keywords, names of persons pictured, and location coordinates\nCopyright info includes the creators attribution, licensing restrictions, credits, and terms of use\nAdministrative data can include the creation date, modification date, location, and other system metadata mentioned above.\nWhich image sharing services scrub metadata and which ones dont?\nComparitech researchers analyzed the metadata scrubbing practices of 12 popular image storage and sharing services online. They uploaded an image of the Mona Lisa loaded with metadata to each of the services. After the upload, they then downloaded the image from each respective service to see if the metadata remained intact or not.\nAn image uploaded to imgbb, displayed on a web page with its original metadata.\nLets start with the most popular places to share images on the web. Imgur, Facebook and Instagram all scrub all metadata from photos upon upload. You dont have to worry about leaking metadata when uploading images to these sites. Bear in mind, however, that even though users of those sites dont have access to metadata, the sites themselves do.\nFlickr keeps all of the original metadata data and even displays a lot of it on each photos web page.\nPhotobox.co.uk tags photos in the metadata comments section to indicate that uploaded images are compressed. The rest of the metadata is intact. It was the only service that actually added or modified data.\nThe remaining image sharing and storage services we examined didnt remove or modify any metadata except for date modified timestamps:\nIf you dont want to expose EXIF metadata on those sites, youll have to scrub images beforehand. More on how to do that below.\nHow you can be tracked using EXIF metadata: research examples\nComparitech researchers proved the sensitivity of image metadata by using publicly available images to track down image subjects and creators. (Note: weve scrubbed all of the following images of their original metadata).\nLets start with a simple example. Using the GPS metadata in the above photo, we determined it was taken near Srstranda, Norway.\nThe next subject was a photo of a mans face. Using the image metadata, reverse image search, and a bit of open-source intelligence (OSINT), researchers were able to identify him as a previous game-show contestant. They found his country, date of birth, wedding date, spouses name, Facebook profile, Twitter account, LinkedIn page, Instagram account, work experience, skills, education, and interests. Researchers were also able to identify and find info about the subjects game-show teammates as well.\nAnother subject was a passport-style headshot featuring a man in what appear to be military fatigues. Researchers were able to track down the image to a site with photos of the subjects school graduation. Using the school name and graduation gallery, researchers retrieved the names of everyone in his graduating class. With the possibilities narrowed down, they found a man with a name similar to that of the image filename. Researchers went on to find the mans Facebook and Instagram profiles. Using these images, they further discovered he was indeed a soldier. They learned his division and brigade, and info about his closest relatives.\nLastly, researchers identified a Philippine national using a photo of herself posted on an image-sharing site. The subject is holding up photo identification. Such photos are often used to verify the subjects identity to a digital service, such as an online bank. Researchers were able to find out the subjects country, birth date, weight, height, blood type, address, Facebook profile, job, education, that she recently had Covid-19, and her Youtube channel.\nMetadata from images and other files has been used as evidence in courts of law and police investigations, demonstrating metadatas value from a privacy perspective. Here are a few prominent examples:\nIn 2016, two Harvard students used GPS coordinates stored in the metadata of photos posted on the dark web to identify drug dealers 229 drug dealers. Dark web drug dealers often post images of their products online to help prove their credibility, but they often forget to scrub EXIF data beforehand.\nIn 2017, an employee of Bio-Rad Laboratories filed a suit against his employer alleging he was fired for telling authorities about potential bribery in China. A performance review with a metadata timestamp dated after he was fired served as evidence in the case, resulting in a higher payout for violating laws against firing whistleblowers. This is the biggest metadata-linked payout to date at $10.8 million in damages.\nIn 2015, a judge threw out a case in which a woman accused her spouse of physical abuse. The plaintiff provided several photos as evidence of abuse, but the metadata indicated the date that the wife had claimed the abuse occurred three months after the photos were taken.\nDigital forensics company Legility published a case study (PDF) describing a lawsuit it investigated. In that case, a healthcare company acquired another business. Employees from the original company left to start their own company. The now-acquired company sued the new company, alleging it poached employees and stole trade secrets and proprietary documents including customer lists. Using the metadata of those documents as evidence of when documents were copied and transferred, the acquired company was rewarded $7 million (GDP 5.1 million).\nCameras and camera apps vary quite a bit, but many of them have an option to turn off or limit the generation of metadata. Check your camera or app settings.\nMost cameras and image editing programs store image metadata in the EXIF format. You might be able to edit EXIF data on exiting images through your camera or photo editing app.\nSome programs are specifically designed to work with metadata. ExifTool and Image Scrubber are two great open-source options.\nWindows 10 comes with a built-in option to remove metadata. However, this will only remove metadata that Windows 10 understands, which means it could leave some metadata behind. Still, it should at least help minimize the information stored in images. If youre a PC user, just follow these steps:\nRight-click the image file and selectPropertiesto open a new window\nClick the link that saysRemove Properties and Personal Informationat the bottom. Another new window will pop up.\nIn theRemove Properties window, selectRemove the following properties from this file:\nWhat's in this article?Which image sharing services scrub metadata and which ones dont?How you can be tracked using EXIF metadata: research examplesMetadata used as court evidenceHow to remove metadata from images\nThis site uses Akismet to reduce spam. Learn how your comment data is processed.\nComparitech.com is owned and operated by Comparitech Limited, a registered company in England and Wales (Company No. 09962280), Suite 3 Falcon Court Business Centre, College Road, Maidstone, Kent, ME15 6TF, United Kingdom. Telephone +44(0)333 577 0163",
    "Canto Digital Asset ManagementProductIndustriesCustomersResourcesPricingFree TrialPhoto managementFinding photo metadata: A guide for the rest of usby Casey Schmidt | June 29, 20215 min. readContentsWhat is photo metadata?How to access and view photo metadataCan I edit photo metadata? Should I?Why is metadata so valuable for photos?Closing ideasIf a picture is worth a thousand words, then metadata of that picture is worth millions. The extensive metadata attached to digital photos gives users a chance to sort, store and understand the files in their photo library.\nIts important then to be able to locate and access this information. This article explains everything you need to know about photo metadata, including the reasons why its so important to learn about.\nPhoto metadata is the information and specific details concerning a particular image file. This information often includes date created, author, file name, content, themes and more. Photo metadata offers users a better way to organize, sort and maintain image files within a system.\nWithout metadata, theres only so much information we can gather from just looking at a photo. Lets look at an example:\nWe could say that this picture is a photograph of a small dog wearing a Champion brand yellow shirt in front of a blue background. Metadata adds to that, with elements such as: Date created, geographical location, author (photographer), image title, pixels, camera type, file type and more. It may seem minor, but if the title of the picture is The Kings Dog, the metadata is crucial in understanding it.\nNow lets discover how to view all the metadata within a picture file.\nPhoto metadata is accessible for all users and doesnt take any kind of special software such as Adobe Lightroom Classic. The pathways to locating photo metadata are different on both Windows and Mac. Ive listed both methods below.\nLocate and right-click the intended digital image file\nClick the Details tab at the top of the popup window\nNow simply scroll down through the resulting window until you find the metadata section you require\nDepending on the type of image file, youll be given different metadata for each file. Generally though, things like type, name and date will all be available. Now lets look at how to see photo metadata on a Mac.\nUsing Finder, locate and open the intended digital image file\nThis will result in a new window with relevant details. Cycle through the different tabs at the top until you find the information needed\nNow that you have some clear paths to locating metadata in your photos, heres a look at how to go about editing this information when necessary.\nThere are many reasons people want to edit the metadata of their digital photos. Some do so because of an error when the metadata was processed, leaving files with the wrong name, author or date. Others may just need to remove metadata from photos.\nSometimes a user needs to edit metadata because of missing fields or unsatisfied parameters. For example, the data doesnt include things like a title (which may be known only to the user). There are also users who wish to hide key details of images before uploading them to a social media platform. Whatever the reason, chances are there will be a time when you need to switch up or add a few key data details. When this occurs, youll need to know how.\nFortunately, images are one of the easiest file types to change metadata in, because they are accessible for editing without any outside software systems.\nTo edit the metadata of a digital photo, follow this sequence:\nAs we did above, right-click it and select Properties\nClick the Details tab at the top of the popup window\nLocate the category that needs editing (name, date, author, etc.)\nUnderneath the Value tab, youll be able to input the new information (click and type)\nOf course, there may be plenty of reasons that manually editing in this manner isnt ideal. Users in need of large number of edits implement a metadata editor. Make sure that the software you use is suitable for the types of image files you work with, and can deal with the extensive editing tasks required. Lets make sure you understand the value of this information within a photo.\nThose who work extensively with metadata understand how fortunate they are that it exists in such large quantities. When it comes to photos, this is especially true. Pictures are now almost exclusively digital, and even print images tend to be copied into a digital format at some point. One way metadata proves itself valuable is as a connection between different systems that are exchanging large quantities of image files. Imagine hundreds of thousands of image files being transferred from one computer to another. Without metadata, the only way to organize and understand the new files would be by manually opening these pictures.\nWithout metadata, sorting and organizing images would be difficult. Even a small number of image files can become cluttered without a bit of organizational efforts. When the amount of images is multiplied, theres potential for disaster. Metadata gives users the power to quickly organize photos using specific data categories like topic, size, date or author. This in turn makes the images more retrievable, sorting them into more specific groups. This might not be as important for someone sorting birthday photos, but a large company keeping branded images will be very grateful.\nHopefully this article has motivated you to make the most out of your own metadata. Photo metadata is one of the most helpful ways to take control of your images. Stay ahead of the curve by understanding how and why to use it.\nProductCanto DAM PlatformMedia Delivery CloudVideo EnhancementsIntegrationsFree TrialResourcesWebinarseBooksVideosWhat is DAM?TutorialsCompanyAbout usPress releasesCareersBlogContactEnglishUnited States FlagEnglishGermany FlagGermanPrivacy PolicyDMCA PolicyCookie PolicyImprintSupportAPISitemapCopyright  2023 Canto, Inc. All rights reserved.TOP",
    "How to edit metadata on images using ExifTool - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features 2023 Google LLC",
    "How to edit metadata on images using ExifTool - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features 2023 Google LLC",
    "How to tell if someone modified the EXIF data in a photograph - Quora\nSomething went wrong. Wait a moment and try again.Try again\nPlease enable Javascript and refresh the page to continue"
  ],
  "detecting AI-generated images": [
    "AI Detection: How to Pinpoint AI Generated Text and Imagery [+ Detection Tools]\nTrusted by business builders worldwide, the HubSpot Blogs are your number-one source for education and inspiration.\nResources and ideas to put modern marketers ahead of the curve\nEverything you need to deliver top-notch customer service\nTutorials and how-tos to help you build better websites\nThe insights you need to make smarter business decisions\nAll of HubSpot's handcrafted email newsletters, tucked in one place.\nIrreverent and insightful takes on business and tech, delivered to your inbox\nBrowse our collection of educational shows and videos on YouTube.\nOur unrivaled storytelling, in video format. Subscribe for little revelations across business and tech\nLearn marketing strategies and skills straight from the HubSpot experts\nWhen it comes to brainstorming business ideas, Sam and Shaan are legends of the game\nWatch two cerebral CMOs tackle strategy, tactics, and trends\nEverything you need to know about building your business on HubSpot\nHubSpot Podcast Network is the destination for business professionals who seek the best education on how to grow a business.\nEach week, hosts Sam Parr and Shaan Puri explore new business ideas based on trends and opportunities in the market\nRedefining what success means and how you can find more joy, ease, and peace in the pursuit of your goals\nA daily dose of irreverent, offbeat, and informative takes on business and tech news\nEach week, Another Bite breaks down the latest and greatest pitches from Shark Tank\nHubSpot CMO Kipp Bodnar and Zapier CMO Kieran Flanagan share what's happening now in marketing and what's ahead\nExpand your knowledge and take control of your career with our in-depth guides, lessons, and tools.\nLearn and get certified in the latest business trends from leading experts\nInteractive documents and spreadsheets to customize for your business's needs\nIn-depth guides on dozens of topics pertaining to the marketing, sales, and customer service industries\nMulti-use content bundled into one download to inform and empower you and your team\nCustomized assets for better branding, strategy, and insights\nAll of HubSpot's marketing, sales CRM, customer service, CMS, and operations software on one platform. See pricing\nMarketing automation software. Free and premium plans\nContent management software. Free and premium plans\nOh no! We couldn't find anything like that.Try another search, and we'll give it our best shot.\nAI Detection: How to Pinpoint AI Generated Text and Imagery [+ Detection Tools]\nAI-generated content is a fascinating development, and were seeing more and more articles, stories, and images created by AI tools. (Thanks, AI, for the intro sentence.)\nBut, the rise of advanced AI generation tools has exposed potential issues, from people being unable to detect the difference between AI and human generations to AI predictions and analysis being flat-out wrong.\nThis is where AI detection comes in, as it's a way for people to uncover when text, images, and even videos are machine-generated, so they can make informed decisions on the content they consume. In this post, well cover:\nAI detection is figuring out if content is AI or human generated, usually with the help of an AI detection tool that uses machine learning and natural language processing to identify patterns. If content follows a more predictable pattern, a tool will likely classify it as AI-generated.\nAI detection tools don't know the meaning of words and use context to analyze text. To get more technical, tools use the context of what's to the left of the following word to predict the likelihood of the word to the right.\nThe more predictable the word to the right is, the more likely the text is AI-generated. On the other hand, human-written sentences vary from predictable patterns and are more creative.\nIf youre anything like me, a basic example might be helpful to understand this. Lets break it down.\nSay someone inputs the sentence, Bunnies are so fluffy.\nThe tool uses learned data and context of words to the left of fluffy to predict that fluffy is more likely to come next, more so than words like cute or soft.\nSince the sentence follows a highly predictable pattern, the tool will likely classify the text as AI-generated.\nAI detection tools work at a much larger scale with more complex sentences and paragraphs than Bunnies are so fluffy to make predictions and classifications, but this is a basic example and shows how the process works.\nSome detection tools analyze images and videos and use pixel anomalies to determine if something is AI-generated.\nThere are no set rules or guidelines for identifying AI-generated text, but here are some things to look out for:\nRepetition of words and phrases: AI knows what its talking about, but not to the extent human experts do. Its outputs might repeat the same keywords and phrases with little variation when discussing a topic.\nLack of depth: Generation tools lack depth and can't go beyond basic facts to truly analyze a topic and develop unique insight. AI-generated text might read more robotic and prescriptive than creative and have a generic tone.\nInaccurate and outdated information: The facts that content generation tools have are typically correct, but since the tools make predictions, outputs can be incorrect or unrelated to true facts. In addition, information can be outdated, like how ChatGPT is limited to information pre-September of 2021.\nFormat and structure: Generation tools follow the same sentence structure as humans, but sentences can be shorter and lack the complexity, creativity, and varied sentence structure humans produce. Content can be streamlined and uniform with little variation.\nHuman-written text is also more likely to have typos and use informal and casual language and slag.\nRoft.io is a fun game to test your detection skills and see how good you are at predicting when text is AI-generated.\nIdentifying AI generated images and videos can be a bit more challenging than detecting text. Some commonly discussed tells are:\nTextured backgrounds, images that look airbrushed, random brush strokes throughout images\nOverall image sharpness, or parts of images that are blurry while others are more clear\nSigns of artist watermarks or signatures (AI tools are trained from existing artwork)\nTools like DALL-E 2 place a watermark on image outputs, but they might not be easy to spot. OpenAI also allows people to remove a watermark. You can also reverse image search to see if there are any traces of an image on the web.\nThe challenge of detecting AI images and videos is why deepfakes are so dangerous, as videos and images that seem lifelike enough can rapidly spread misinformation.\nAt the moment, it might be easier to tell if something is AI generated because it sounds robotic, or someone's hand is missing two fingers in an image. If generation tools become more sophisticated, it might be harder for humans to find the key discrepancies.\nRegardless of future progressions, SEO AI detectors and other detection tools can be more helpful than our own deduction abilities in classifying AI-generated content, and there are various options available.\nBelow well go over some of them and rate their effectiveness using an AI-generated paragraph from HubSpots Content Assistant (which uses GPT). Heres what it gave me when I asked it to write a paragraph about dogs:\nDogs are simply amazing creatures. They are loyal, loving, and endlessly entertaining. Whether you need a furry friend to cuddle with on the couch or a loyal companion to explore the great outdoors with, dogs are always up for the task. They come in all shapes and sizes, from tiny teacup Chihuahuas to majestic Great Danes, but all dogs share one thing in common: a boundless capacity for love and affection. Whether you're a lifelong dog lover or a newcomer to the world of canine companionship, there's never been a better time to discover the joys of life with a furry friend by your side.\nNote that human writing can still trigger a tool if it follows a predictable pattern.\nZeroGPTs algorithm is trained on 10M+ articles and text to have a detection accuracy rate of 98%. It supports multilingual text and detects popular language generators like Chat GPT, GPT-4, and Google Bard. Outputs highlight sentences most likely to be written by AI.\nI entered the AI-generated paragraph about dogs, and it predicted the text is 88.57% AI/GPT generated.\nBest for: ZeroGPT was built for educators to test for AI-generated content, but it works for anyone looking to detect AI content.\nTests for: Developed in 2019 for GPT-2 text, might be unreliable on other generators\nMIT-IBM Watson AI lab and the Harvard NLP group created the Giant Language model Test Room to detect AI-generated text. It analyzes inputs based on how likely each word is to appear based on the word immediately to the left. The more predictable the word is, the more likely the text is written by AI.\nThis tool doesnt give a percentage but color codes words based on their predictability, with green meaning the word is part of the top 10 most predictable words.\nMost of my paragraph is highlighted green, so the words are part of the top 10 most predictable (based on context) and more likely to be AI-generated.\nBest for: Testing GPT-2 and learning more about predictable writing through an in-depth probability analysis.\nPrice: Free 50 credit trial, then $0.01/100 words (1 credit scans 100 words)\nOriginality.AI Chrome Extension, built by content marketing experts, detects multiple versions of GPT with 94% accuracy. It scores text on a scale of 0-100, with a higher score being a higher likelihood of being produced by AI. You can also use the tool to scan for plagiarism (beneficial for educators). It's the most accurate with more than 50 words.\nWith my test, it said that the paragraph was 99% likely to have been written by AI.\nBest for: The Chrome extension makes it perfect for anyone looking for a seamless and immediate detection process when writing and reading online. Writers, content marketers, and web publishers alike can leverage this tool; not for academics.\nContent at Scales AI Detector uses 3 AI engines and natural language processing to detect ChatGPT, all versions of GPT, and other generators. You can use it to test SEO, educational, and marketing content. The tool needs at least 25 words for reliable results, and you can enter up to 25,000 characters.\nMy test results were inconclusive because the tool couldn't say with certainty if the paragraph was AI-generated. It gave a human content score of 51% with 17% predictability.\nIt did say with certainty that the last sentence is AI-generated.\nBest for: SEO and marketing-focused content creators to get line-by-line text breakdowns and analyze longer pieces of content (up to 25,000 characters).\nWriter AIs content detector estimates how much text is AI-generated. The free and paid versions have a 300-word limit (1,500 characters), and results give a prediction percentage for how much of the text is human-generated content.\nIt scored my paragraph as 87% human-generated, with a recommendation to edit the text until theres less detectable AI content.\nBest for: B2B and enterprise and agencies looking to analyze and edit content before publishing.\nTests for: ChatGPT, GPT-3, DALL-E, Midjourney, Stable Diffusion\nHive offers a suite of AI detection tools for images, text, and deepfakes.\nThe text detection tool gives a confidence score for how likely something is AI-generated, and estimates which sections are most predictable. It also estimates which sections of text are more likely to be AI-generated. It works starting at 750 characters with a recommended length of 1500 characters.\nI had to input extra words to reach the character limit, and it predicted the paragraph was 99.99% likely to contain AI-generated content.\nThe media recognition tool identifies AI-generated media, gives a classification (AI-generated or not), confidence score ( 1), and image generation source (like DALL-E). (Documentation, tool page)\nThe deepfake detection tool tests if images or videos are deepfakes through facial classification. (Documentation)\nBest for: Screening work to detect AI content or for websites to detect and moderate AI-generated images and text.\nOpenAIs Text Classifier can distinguish between AI-generated text and human-written text. It works best with more than 1,000 characters and English text.\nOpenAI does note that it is not entirely reliable and only correctly identifies 26% of AI text and incorrectly labels human-written text as AI 9% of the time, but reliability increases for longer text. It recommends using the classifier as a complement to other testing methods.\nI outlined each tool's individual test score above, but heres a table comparing scores.\nFirst place is a tie between Originality.AI, GLTR, and Hive AI\nAI detection makes it a lot easier to distinguish between machine and human-generated text. As AI tools become more and more accurate, AI detection will remain important in helping people determine the legitimacy of the content they consume.\nGet expert marketing tips straight to your inbox, and become a better marketer. Subscribe to the Marketing Blog below.\nHubSpot uses the information you provide to us to contact you about our relevant content, products, and services. HubSpot will share the information you provide to us with the following partners, who will use your information for similar purposes: Jasper. You can unsubscribe from communications from HubSpot at any time. For more information, check out HubSpot's Privacy Policy. To unsubscribe from Jasper's communications, see Jasper's Privacy Policy.\nPop up for FREE GUIDE: HOW TO USE AI IN CONTENT MARKETING\nLearn how to use generative AI to scale your content operations.\nNurture and grow your business with customer relationship management software.",
    "AI Image Detector - a Hugging Face Space by umm-maybe",
    "Detect AI-generated Images & Deepfakes (Part 1) | by Jonathan Hui | MediumDetect AI-generated Images & Deepfakes (Part 1)Jonathan HuiFollow17 min readMar 17, 2020--1ListenShareWho wants to be a millionaire? More than 2,000 competitors say so for the total one million prize money in the Deepfake Detection Challenge (DFDC). The goal of the March 2020 challenge is to create technologies that detect Deepfakes and manipulated media.Updates: Lets have a quick followup on the competition since this article was first written. With 35K models submitted, the top winner for DFDC was from Selim Seferbekov whose model had an accuracy of 65% in spotting Deepfakes. The accuracy was a little bit lower than I expected as many fake videos in the dataset were not fabricated for high-quality productions. But this demonstrates how hard to generate an automatic detection solution and also there are plenty of rooms to improve.According to the Facebook paper:Selim Seferbekov, used MTCNN for face detection and an EfficientNet B-7 for feature encoding. Structured parts of faces were dropped during training as a form of augmentation. The second solution, WM, used the Xception architecture for frame-by-frame feature extraction, and a WSDAN model for augmentation. The third submission, NTechLab, used an ensemble of EfficientNets in addition to using the mixup augmentation during training.Due to kernel time limits (computation time) established in the competition, MTCNN detector is chosen for face detection over S3FD for speed. Then, Selim Seferbekov expanded the area by 30% and use this as input to EfficientNets to extract facial features. In addition, the training dataset is heavily augmented, including cutout and partial dropout (shown below). This improves the generalization of the detector. Congratulations!SourceIn December 2019, Facebook removed 682 accounts that allegedly used deceptive practices to push pro-Trump narratives to about 55 million users. As Facebook stated, some of these accounts used profile photos generated by artificial intelligence and masqueraded as Americans. It is widely reported that the photos are generated from a public website using StyleGAN in producing profile pictures. The photos below are generated by an improved version called StyleGAN2 which is also publicly available.SourceCan you spot which image below is fake? Which one is created by StyleGAN?Source (The fake one is generated by StyleGAN)This one is easy. It is the left one because of the artifacts present in many StyleGAN photos. Just for fun, there are a few more.All the images on the left are fakes. My accuracy in spotting StyleGAN photos is higher than 95%. But StyleGAN2 is far much harder. All the photos below are fake.SourceGAN and Deepfakes become more than research topics or engineers toys. Starting as an innovative concept or application, now it can be used as a communication weapon. If you want more examples, here is another widely distributed video created with Adobe After Effects and FakeApp (a Deepfakes application).Design & Implementation FlawsDesign and implementation usually come with shortcomings and mistakes. For example, the instance normalization method used in StyleGAN often triggers blob artifacts and color bleed in generated images. This reveals the fake images easily.However, like other GAN and Deepfakes technologies, countermeasures are introduced. For example, the blob artifacts in StyleGAN is already resolved by weight demodulation in StyleGAN2 as the alternative normalization method.For StyleGAN2, if you look in detail, you can still find some flaws. For example, the structure of the background below does not seem right. The rendered structures do not maintain the correct form of lines or shapes.Symmetry is hard to maintain also. For example, one ear may have an earring but not the other. In the following picture, the pose of the right shoulder does not match with the left shoulder below.Deepfakes ReviewIn Deepfakes, step  below builds a common encoder to encode the latent factors of pictures for two different persons. In steps  and , it builds two separate decoders to reconstruct the first and second photo respectively. To reconstruct the image correctly, the encoder must capture all the variants in a persons photos, i.e. the latent factors that apprehend information like the pose, the expression, illumination, etcLets replace Marys faces in a video with Amy. We will capture the latent factors of Marys face in the video and render it with Amys decoder. Therefore, the rendered Amy face will have the same pose, lighting, and emotional expression as the original video.However, if it is not done probably, this will turn into a cut & paste operations with obvious artifacts on the boundary where the face is pasted.To resolve that, the encoder can learn a mask to blend the new face with the original better.Nevertheless, the merging of the new face onto the original one is tricky. Ghosting effects, tone changes, and obvious boundaries usually give away the low budget productions including some videos in the DFDCs dataset.Source: RedditFace landmarksAnother technique can be applied to improve quality. The concept of swapping face using face landmarks has been done before the current AI era. An area of the face is cut off and wrap form its own landmarks to the target landmarks.Then Gaussian blur is applied to smooth out the edges. But the skin tones and lightness will probably not match. As discussed before, this can be addressed with Deepfakes.OpenCV (left), RightSome Deepfakes implementation detects the facial landmarks and warps the replaced face to match the original landmarks. This will create a better pose and match the shape and dimension of the original face better. To reduce the awkward boundaries, Gaussian blur is applied in particular on the edge area.Next, lets examine the low budget Deepfakes production first. Many high-budget versions still have some of these flaws but just far fewer and less subtle.Deepfakes flawsBlurryFaces in many Deepfakes videos are unusually blurry. There are two major reasons. First, the new face needs to blend well with the rest of the images. Therefore, filters are applied which will blur the face slightly. Second, many low-budget productions use low-resolution pictures of the faces to learn the encoder. Since training time grows exponentially with the face resolution, this relaxes the GPU memory requirement as well as the training time. In the early days, many low-budget productions use face resolution of 64  64 and produce blur faces.Now, many high-budget productions will select the input resolution carefully (usually with higher resolution). Combining with days of training using high-end graphics cards, the quality of the video can be significantly improve and hard to detect.We can also compare the sharpness, lighting and color tone with other faces in the video. If the other person is real, you may spot the difference easily.Source (left: the face is swapped, right: a real person in the video)However, in Jordan Peeles video on Obama, there is only one person in the video. Masks are applied to restrict the change to the mouth & jaw area of Obama only. Other parts of the face are not touched. But, if you look at the video closely, you will still find the mouth area is more blurry compared with the eyes.Created with Adobe After Effects & FakeApp by from Jordan Peele and BuzzFeedAgain, this is for low-budget productions only. Many high-budget Deepfakes videos are learned with higher-resolution faces with the final video in 1440p. So even the faces are slightly blurred, it still has higher fidelity than what we usually watch in the HD format (740p). This high-fidelity lowers our guard in considering them as fakes. But in the snapshot below, there are areas that Gaussian blur is unevenly applied which indicate the image has been manipulated.Modified from sourceHowever, there are videos where the original faces have heavy makeup or overexposed. It will not be easy to locate the flaws mentioned above if it is trained correctly.The snapshot on the left below is a high-budget Deepfakes video in high resolution (1440p). It has details superior to the HD version (740p) and hard to observe any blurriness mentioned before. This is just another example of how Deepfakes can overcome some of its previous preceptions, like poor fidelity.SourceSkin toneIn some swapped face, the skin tone looks un-nature.Source (right: the original impersonator)Or is it just a bad tanning session of the celebrities? Generated from DeepfakesOne way to overcome this problem is by selecting candidates with similar skin tones, hairstyles, and the shapes of the face to swap.Paul Rudd & Jimmy FallonHere, Paul Rudd's face is replaced by Jimmy Fallon's face.In addition, candidates are selected that are good at impersonating peoples voices, gestures, and expressions.Double eyebrowWhen we merge the replaced face with the original face, if the mask or the merging is not done probably, we may see two sets of eyebrows  one set from the new face and the other from the original face.SourceDouble chinA double chin can happen also but it is harder to tell whether it is natural or not if you do not know the original person well.Spatial inconsistencyWhile trying to spot abnormalities in the facial area, we can compare the face with other parts of the body. Obviously, you cannot put a 60 years actor face on a 20 somethings actress, in particular, that is Jennifer Lawrence. The skin texture and the smoothness of the arm will not match the face.Steve Buscemi on Jennifer Lawrence videoIn general, look for the differences, including tones, sharpness, and texture, between the impersonated faces and the rest of the video and the current video frame.While we explore spatial inconsistency, we can also explore the temporal inconsistency.FlickingOne of the major weaknesses of Deepfakes is that video frames are generated frame-by-frame independently. Such independence may generate video frames with noticeable different tones, lighting, and shadow compared with the last frame. When it is playing back, flicking occurs.Sometimes, the quality of the replaced frames is so bad that the bad frames are manually or automatically removed. If not too many frames are skipped, you may not notice it without paying too much attention.We take a couple of snapshots below. Even they are very close in time, the sharpness and tones are noticeably different.The diagram below shows another two frames with quite different RGB distributions.ShimmeringIf you playback the video below at 0.25 speed, skin shimmering and unnatural tone changes occur when the head is moving.In Deepfakes, quick movements often make it hard to create frames with proper temporal smoothness. The changes in the latent factors in the neighboring frames may be incorrectly exaggerated by the decoder. This is not easy to solve unless we add an extra term in the cost function to penalize such temporal jiggle during the training. (And, this may require some ad-hoc changes to the design and implementation).BorderIn Deepfakes, there are areas that you should pay special attention to in spotting fake videos. One is the border area of the face where it merges with the original.SourceBut for more serious productions, the artifacts will be less noticeable or unobservable. Better algorithms or manual manipulations may be done in masking the new faces on top of the background.SourceHere is another high-budget production. It is quite flawless unless you pay attention to the edges on Gillian Andersons face.Post-production video editingIn general, adding training data to match the face angle or applying automatic color augmentation during training will solve more artifacts mentioned in this article. Nevertheless, manual post-production video editing with a mask is often done to solve the remaining issues.TeethOne of the key shortfalls of most Deepfakes videos is the teeth area. It is hard for the decoder to reconstruct a small area that has a well-defined structure. Often, the teeth in Deepfakes are blurry.In other cases, the teeth are misaligned teeth, or the individual tooth is stretched or shrank.In one video, I find it that it renders too many teeth. Sometimes, there is a lot of ghost effects in rendering the teeth. And the teeth looks different across the video frames. Even for some high-budget Deepfakes videos that have high fidelity, the teeth can still render incorrectly. As shown below, a few teeth are connected together.PoseWhile I was comparing the Deepfakes reproduction on the Silence of the Lamb with the original one, I found out that a few seconds of the original clip is missing.I speculate that it contains a pose with the camera viewing from the jaw of Anthony Hopkins. It is highly likely that the producers do not have enough video frames from Willem Dafoe to learn the Deepfakes model to reproduce this scene correctly. So it is edited out manually. In many Deepfakes videos, the sideway view of the impersonator is usually one of the weakest links of the fake videos.While the Break Bad Deepfakes video does an excellent job of impersonating Donal Trump. Its side view is not doing so well.SourceNevertheless, this problem can be solved by adding relevant video frames in model training. We will discuss this later.Obscured objectAn obscured object moving across the face can sometimes confuse the Deepfakes model. The key reason is the model does not have enough data to learn such situations correctly. As in one high-budget production, someone takes a bite out of the lid obscuring the face on the left. Therefore, I often look for obscured faces and see if there is anything wrong.Glare & reflectionSome of the glare or reflection in Deepflakes looks exaggerated, missing or without the proper complexity. Again, this is the problem for Deepfakes to render small structures. Nevertheless, this usually increases my confidence in real videos rather than for fake videos.the right one is fakethe left one is fakeEyeglassIn many low-budget productions, the temple of the eyeglasses is missing.Lazy EyeWe build the Deepfakes model with still frames in 2-D. Operations including warping may lose important 3-D information during the process. For example, we may see some lazy eyes in the video,that does not present in the original video.This kind of problem can happen in GAN also as explained by the StyleGAN2 paper:In this example the teeth do not follow the pose but stay aligned to the camera, as indicated by the blue line.SourcePolitician & celebritiesFace shape and aspectPoliticians and celebrities are one major source of impersonation. Deepfakes are often applied to celebrity pornographic videos.In the majority of the case now, we dont replace the outlines of the face. Therefore, we can create a database of those public figures to spot any forgery. However, newer technology may apply GAN to replace the outlines of the face also. But this is still in the early phase. As a short note, as mistaken by many, most Deepfakes applications do not apply GAN.For example, the long forehead of Stallone in the Terminator Deepfakes does not look right for Stallone.High-budget productionsThe term high-budget production in this article does not necessarily mean projects spending tons of money. In this article, we actually refer to projects that have the right know-how people, decent computer graphics card, and a reasonable amount (days) of time to train the model. Collecting, selecting and cleanup of the training dataset is critical to the quality of the project. It is not hard to gain professional knowledge either. There are tons of online tutorials and free tools. You may need some trials and errors but no AI knowledge is needed. (Even AI knowledge may help, many guides will give you enough suggestions.) And post-production manual manipulations are often applied to produce the top-quality videos. Many people with video editing experience can learn the whole process quickly.In this article, we sound like Deepfakes are easy to spot visually. It is not true for the latest videos as the general public gains more know-how knowledge on producing them. There is no one-size-fit-all troubleshooting guide in detecting Deepfakes videos. Different videos many have different mistakes. Worst, fewer mistakes are made and harder to find. In later articles, we will look into some programmatic ways of detecting them. With the knowledge in this article, here are a couple of videos for you to analyze and apply what you learn.One of the obvious mistakes is the eye if you look closer. The pupil is a non-circle!As mentioned before, the boundary also reveals the Deepfake video.Breaking BadLets check out another video.However, the wrinkle around the eyes does not match the smoothness around the chin. In many Deepfakes videos of celebrities, this happens very often. But again, this can be simply a bad botox session.The shadow on the side of the face seems not natural. Unfortunately, it is not too obvious and turns into a definite factor to say it is fake.In addition, the scare on the face will be hard to reproduce as it is hard to collect data for Jared Kushner with scares. Instead, the reproduced frame shows blurred marks on the face only.Here is another fun Deepfakes videoand the original for you to detect any issues.Deepfakes in PoliticsWith the media attention on Deepfakes, the abuse of Deepfakes in politics is still relatively small in 2020. More likely, it will be used as a last-minute surprise rather than a daily attacking mechanism. Many existing political Deepfakes videos come with a disclosure that they are created with Deepfakes (like the ones below). But this can change when software like Reface, FaceSwap, and DeepFaceLab, are getting more popular by the general public.The videos below encourage the safeguard and the voting rights on the 2020 elections. It comes with a disclosure that the videos are fake at the end. They are filmed by two actors with similar face shapes as Putin and Kim respectively and imitate similar accents to recite the script. Then the faces are swapped with Putins and Kims faces using open-sourced Deepfakes package. Then, it is improved by post-production video editing. Because of the higher quality requirements, it took 10 days for the whole process which is longer than the average.But there remain some shortcomings that many high-quality videos overlook.If you pay close attention to the teeth, you will find the rendering is wrong once a while.Some area of the face is more bury compared with other parts.The movement in the chin and the edge area stood out compared with its background.Yet, the largest giveaway is the head movement. Many politicians speak with much larger and frequent head movements as we will demonstrate in later articles.Low-budget productionsWe tend to believe what we want to believe. A fake video on Nancy Pelosi was circulated around the Internet with slurred or drunken speech. This low-quality reproduction is not created by Deepfakes. Instead, the view is slowed down by 25% and the pitch is altered to make like she is slurring her words. The lesson learned here is low-quality fake videos can widely distribute also. Contents are pushed in the social platform by engagement algorithm. None of them passes through any journalist standard. So do check the source carefully. Information from social media is usually a bad source of information.SourceThe challenge of fake videos also imposes problems on real news. We will probably hear frequently as politicians shape their scandals as fakes. It happens before Deepfakes but now can be more confusing.MoreDeepfakes is just one of the many approaches to generate fake videos. Part 2 looks into more academic approaches in this area first.Detect AI-generated Images & Deepfakes (Part 2)Deepfakes has gained tremendous traction because many easy-to-use free software packages are available which require nomedium.comPart 3 of the series looks into the detail of two popular packages in generating Deepfakes: Faceswap & DeepFaceLab.Detect AI-generated Images & Deepfakes (Part 3)Two popular tools in generating Deepfakes video are Faceswap and DeepFaceLab. This article is not a tutorial for bothmedium.comFinally, we look at ways of detecting Deepfakes and fabricated images/videos with machine learning and deep learning.Detect AI-generated Images & Deepfakes (Part 4)Finally, our last part of the series looks at detecting Deepfakes videos with machine learning (ML) and/or deepmedium.comCredits & ReferencesTop 10 Deepfake VideosVulnerability assessment and detection of Deepfake videosTampered Speaker Inconsistency Detection with Phonetically Aware Audio-visual FeaturesCtrl Shift FaceIn Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye BlinkingDeepfake Starter KitKaggle Deepfake Detection IntroductionContributing Data to Deepfake Detection ResearchRecycle-GAN: Unsupervised Video RetargetingPhotos CreditsHead scarfFor the second woman head picture, I originally got it from a Royalty-free source but unfortunately, cannot trace its source anymore.Deep LearningComputer VisionArtificial IntelligenceProgrammingData Science----1FollowWritten by Jonathan Hui31K FollowersDeep LearningFollowMore from Jonathan HuiJonathan HuimAP (mean Average Precision) for Object DetectionAP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision7 min readMar 7, 2018--52Jonathan HuiMachine LearningSingular Value Decomposition (SVD) & Principal Component Analysis (PCA)In machine learning (ML), one of the most important linear algebra concepts is the singular value decomposition (SVD). With all the raw16 min readMar 6, 2019--31Jonathan HuiReal-time Object Detection with YOLO, YOLOv2 and now YOLOv3You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in18 min readMar 18, 2018--81Jonathan HuiSSD object detection: Single Shot MultiBox Detector for real-time processingSSD is designed for object detection in real-time. Faster R-CNN uses a region proposal network to create boundary boxes and utilizes those11 min readMar 14, 2018--36See all from Jonathan HuiRecommended from MediumThe PyCoachinArtificial CornerYoure Using ChatGPT Wrong! Heres How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering.7 min readMar 17--340Cameron R. WolfeinTowards Data ScienceUsing Transformers for Computer VisionAre Vision Transformers actually useful?13 min readOct 5, 2022--4ListsWhat is ChatGPT?9 stories31 savesStories to Help You Grow as a Software Developer19 stories29 savesLeadership30 stories11 savesStories to Help You Level-Up at Work19 stories24 savesSteinsDiffusion Model Clearly Explained!How does AI artwork work? Understanding the tech behind the rise of AI-generated art.7 min readDec 26, 2022--3Unbecoming10 Seconds That Ended My 20 Year MarriageIts August in Northern Virginia, hot and humid. I still havent showered from my morning trail run. Im wearing my stay-at-home mom4 min readFeb 16, 2022--765Aleid ter WeelinBetter Advice10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness.5 min readFeb 15, 2022--266Martin ThisseninMLearning.aiUnderstanding and Coding the Attention MechanismThe Magic Behind TransformersIn this article, Ill give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself.12 min readDec 6, 2022--2See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speech",
    "Differentiating between AI-generated images and real ones is becoming increasingly difficult. But there are a few tricks you can try.\nReaders like you help support MUO. When you make a purchase using links on our site, we may earn an affiliate commission. Read More.\nAre you looking at an AI-generated image? These days, it's hard to say, thanks in part to a group of incredible AI image generators like DALL-E 2 and Midjourney. But there is a way. Similar to identifying a photoshopped image, you can learn the markers that identify an AI image.\nThey often have bizarre visual distortions which you can train yourself to spot. And sometimes, the use of AI is plainly disclosed in the image description, so it's always worth checking. If all else fails, you can run the image through a GAN detector.\n1. Check the Title, Description, and Comments Section\nNot everyone agrees that you need to disclose the use of AI when posting images, but for those who do choose to, that information will either be in the title or description section of a post.\nAnother good place to look is in the comments section, where the author might have mentioned it. In the images above, for example, they included the complete prompt used to generate the artwork, which proves useful for anyone wanting to improve their own AI images.\nBesides the title, description, and comments section, you can also head to their profile page to look for clues as well. Keywords like Midjourney or DALL-E, the names of two popular AI art generators, are enough to let you know that the images you're looking at could be AI-generated.\nAnother important clue for identifying an AI-generated image is a watermark. DALL-E 2 places one on every photo you download from its site, though it may not be obvious at first. Can you spot the watermark in the image above?\nYou can find it in the bottom right corner of the picture, and it looks like five squares colored yellow, turquoise, green, red, and blue. If you see this watermark on an image you come across, then you can be sure it was created using DALL-E 2.\nThe problem is, it's really easy to download the same image without a watermark if you know how to do it, and doing so isn't against OpenAI's policy. So long as you \"don't mislead others about the nature of the work\", by telling them you made it yourself, or that it's a photograph of a real-life event, then OpenAI's guidelines permits you to remove the watermark.\nMidjourney, on the other hand, doesn't use watermarks at all, leaving it to users to choose if they want to credit AI in their images.\nYou may not notice them at first, but AI-generated images often share some odd visual markers that are most obvious when you take a closer look.\nFrom a distance, the image above shows several dogs sitting around a dinner table, but on closer inspection, you realize that some of the dog's eyes are missing, and other faces simply look like a smudge of paint.\nThe effect is similar to impressionist paintings, which are made up of short paint strokes that capture the essence of a subject. They are best viewed at a distance if you want to get a sense of what's going on in the scene, and the same is true of some AI-generated art.\nIt's usually the finer details that give away the fact that it's an AI-generated image, and that's true of people too.\nTake a closer look at the above AI-generated face, for example, taken from the website This Person Does Not Exist. Notice anything odd? It could fool just about anyone into thinking it's a real photo of a person, except for the missing section of the glasses and the bizarre way it seems to blend into their skin.\nHere are a few more markers that you might come across in AI-generated images of faces:\nA blurred background that looks more like a texture\nAsymmetry in the face (teeth off center, eyes are different sizes)\nEven when looking out for these AI markers, sometimes it's incredibly hard to tell the difference, and you might need to train yourself to spot fake media.\nAI image generators are powered by Generative Adversarial Networks, or GANs as they are often called. Knowing this, engineers at Mayachitra created a GAN detector in 2021, hoping to solve the problem of identifying AI-generated images.\nTesting the app in 2023, we found the results were a mixed bag. It seems that sometimes the app would be able to identify a GAN-generated image, and at other times it would get it completely wrong.\nSince the results are unreliable, it's best to use this tool in combination with other methods to test if an image is AI-generated. Another reason for mentioning the GAN detector is that further research will likely produce an app like this one day that is accurate, so it's worth keeping an eye out for developments.\nSlowly but surely progress is being made and when Microsoft released a deep fake detection tool, signs pointed to more large companies offering consumer-friendly tools for detecting AI images.\nWithout a doubt, AI generators will improve in the coming years, to the point where AI images will look so convincing that we won't be able to tell just by looking at them. At that point, you won't be able to rely on visual anomalies to tell an image apart.\nHopefully, by then, we won't need to because there will be an app or website that can check for us, similar to how we're now able to reverse image search.\nAs for now, people who use AI to create images should follow the advice of OpenAI and be honest about its involvement. It's not bad advice and takes just a moment to disclose in the title of a post or its description.\nAt the very least, don't mislead others by telling them you created a work of art when in reality it was made using DALL-E, Midjourney, or any of the other AI text-to-art generators.\nAI images are getting better and better every day, so figuring out if an artwork was made by a computer will take some detective work.\nCheck the title, description, and comments for any mention of AI, then take a closer look at the image for a watermark or odd visual distortions. You can always run the image through the GAN detector, but be wary of the results.\nAt the end of the day, using a combination of these methods is the best way to work out if you're looking at an AI-generated image.\nGarling has a Master's degree in Music and over a decade of experience using creative technologies. In particular, she loves writing about music production, film, and DIY electronics. Outside of writing, you will find her taking photos or editing audio.\nIf all social platforms closed down tomorrow, would your life be better or worse?\nOpenAI Launches a ChatGPT App for iOS (With Android to Follow)\nWhy OpenAI's CEO Is Calling for More AI Regulation (and What That Means)\nHow to Combine Two Columns in Microsoft Excel (Quick and Easy Method)\nMicrosoft Is Axing Three Excel Features Because Nobody Uses Them\nHow to Detect Installed Games on the Epic Games Launcher",
    "Forget DALL-E: This Is the Best AI Text-to-Image Generator...\nForget DALL-E: This Is the Best AI Text-to-Image Generator...\nIs Midjourney the superior AI text-to-art generator? Here's what you can do with it.\nReaders like you help support MUO. When you make a purchase using links on our site, we may earn an affiliate commission. Read More.\nIf you are into digital graphics, creating art can be a lengthy and tedious process. However, Midjourney has made it quicker, simpler, and more efficient to create stunning digital art if you can imagine it.\nThe AI tool generates graphics using prompts. All you need is a Discord account and join the beta program the company is currently developing. After rendering your image, you can use it or get inspiration to create your next masterpiece. Here's everything you need to know about Midjourney.\nMidjourney is an independent lab that works in AI, design, and human infrastructure, and is considered one of the top AI text-to-art generators. It uses text prompts to create digital art, and you can customize it with detailed instructions and advanced rendering commands.\nThe bot works through a Discord server. You can visit the official Midjourney server or set up their bot on your Discord server to utilize the AI for your artwork.\nAlthough the program is still developing, the AI understands your command and generates praiseworthy digital art. The website showcases your creations openly. Similarly, you can see what other users have created by visiting the Midjourney Community Showcase.\nMidjourney provides you an asset license for your creation which means you can use your image anywhere for a non-commercial purpose. However, the corporate plan gives you an exception; you can find it in detail in the paid plan section of this article.\nSince the AI works through a discord server, you first need to have a Discord account. You can get started with Discord in a few minutes from the official website. Then, either download the Discord app or log in through a browser. Once you are ready with your Discord account, follow the below steps to create your Midjourney account.\nClick on Sign In. It will redirect you to your Discord account for signing in.\nOnce signed in, click on Authorize, go back to the Midjourney homepage, and click Join the beta.\nConfirm your choice by clicking Accept Invite on the prompt.\nThat's all. Midjourney will now take you to your Discord account, where you can find the dedicated MidJjurney server.\nIf you are not automatically redirected to the Discord website, you can scroll down on the left panel in your Midjourney account and click the Discord logo. It will take you to the Midjourney server on Discord.\nOnce on the Midjourney Discord server, scroll down on the left panel and open any newbies channel.\nType \"/imagine\" followed by a space and start writing your prompt. A prompt can be any text instruction you want the AI to turn into an image.\nPress the Enter key to submit your prompt and wait for the rendering.\nThe free version of the service is bulky, and you may lose your design in hundreds of messages. For now, manually scroll to find your design; you can find your messages highlighted in orange.\nSimilar to how DALL-E 2 works, the bot will provide you with four iterations of the design. You can either upscale a particular design or get more versions of it. To upscale any iteration, tap on the U button followed by the respective quadrant number. Similarly, to get variations of any iteration, tap on the V button followed by the respective quadrant number.\nWhen you are satisfied with your art, you can download and use it.\nThe upscaled versions are a better rendition of one of the designs made using your prompt. In contrast, the variations keep the subject image as the base and develop more similar images for you. All your creations are available on your Midjourney account.\nThere are some useful parameters that will help you take your image to the next level. You can use one or more parameters to enhance or personalize your image. Keep in mind to add these parameters at the end of your prompt.\nTo add a parameter, you must first enter two hyphen lines (--) followed by the parameter.\nAspect Ratio: Use the \"--aspect\" or \"--ar\" parameter to define the desired aspect ratio for your image. For instance, you can insert \"--ar 3:2\" or \"--ar 2:1\" at the end of your prompt to generate the images with the mentioned ratio. The first digit will be the width of your image and the second one is the height.\nNegative Prompting: You can use the parameter \"--no\" to eliminate any elements from the image. For instance, if your prompt is \"Children's playground --no children,\" the generated image will show a playground but won't show any kids in it.\nLimit Details: If you want a blur or unfinished image, you can use the \"--stop\" parameter. It will stop the processing of the image at the mentioned percentage. For instance, if you write \"Flowers --stop 50,\" the AI will generate an image with only 50% details.\nStylize: The --stylize <number>, or --s <number> parameter adds an artistic style to your image. The Stylize value is at 2500 by default. You can either decrease or increase the number to get a minimal or strong stylization in your image.\nHigh Definition: Midjourney lets you use a different algorithm to generate high-definition images. While using the \"--hd\" parameter, the four quadrants will have inconsistent images. However, you will get a detailed image, which can be difficult with the standard algorithm.\nFind your image: Although it is not a parameter, this command will help you find your image in the chat if you lose it. Use the \"/show <jobid>\" command to summon your image. You can find the job ID in your gallery on the Midjourney website. Go to the gallery, click on the three horizontal dots below your image, and select the Copy job ID option.\nYou can read about a few more parameters on the official Midjourney documentation.\nAnother helpful feature while adding a prompt is the image prompt option. You can upload an image on the Discord server and copy the image URL at the start of your prompt. Midjourney will then use the image as inspiration to create the variations. The correct way to use the prompt is Image URL > Text > Parameters.\nYou can also use a few keywords in your text to create better images. For instance, adding keywords like \"octane render\", \"realistic\", \"hyperrealistic\", \"cinematic\", \"painted\", \"cartoon style\", etc. in your prompt will ask the AI to use the mentioned style for the image.\nMidjourney provides both free and paid plans. There are some limitations in the free plan; you can use only 25 prompts and can't use some of the features. However, the affordable paid plan can solve it without putting a dent in your pocket. Here are the paid plans and what comes with them:\nBasic Membership ($10/month): You get 200 GPU minutes on the Midjourney server. However, you don't get access to relax mode, which can save your GPU minutes.\nStandard Membership ($30/month): In the standard plan, you get 900 GPU minutes or 15 hours. In addition, you also get access to relax mode. You can use the \"/relax\" command to use the feature, resulting in additional time to create your image.\nPrivate Visibility ($20/month): For an additional cost of $20, Midjourney allows you to keep your artwork private, it won't be publicly available for others to see.\nIncremental Billing ($4/hour): If you have exhausted your plan, you can add one hour of GPU at a minimal cost of $4 to your plan. You can use this upgrade with any existing plan. Midjourney will ask you to use the metered fast minutes, and you can set a spending limit. The tool will only charge you for the GPU time you have used. For instance, if you set your spending limit to $20 and used only $15 worth of GPU minutes, you will be charged $15.\nCorporate Membership ($600/year): For an upfront amount of $600, you can get a corporate membership that allows you to use the generated images commercially. However, your company must have a gross revenue of $1 million/year. The corporate plan also provides the private mode at no extra cost.\nMidjourney proves to be a better AI text-to-image generator than its competitors like DALL-E 2. The tool provides top-notch AI image generation at affordable prices. You can also take advantage of the commands and parameters to get more accurate results.\nIn addition, since you can install the Midjourney bot on your private Discord server, it becomes exponentially easy to have a dedicated workspace for creating your art. Overall, Midjourney is one of the best text-to-image generators in the industry right now.\nShubham has over a decade of experience in the creative industry, specializing in photography and photo editing. As a former graphic designer and contributor of cybersecurity and Windows articles, his diverse background showcases his versatility and authoritativeness across subjects.\nCombining his technical proficiency in Photoshop with a passion for crafting compelling articles, he strives to inspire and educate readers, fostering a trusted connection built on reliability and authenticity.\nIf all social platforms closed down tomorrow, would your life be better or worse?\nOpenAI Launches a ChatGPT App for iOS (With Android to Follow)\nWhy OpenAI's CEO Is Calling for More AI Regulation (and What That Means)\nHow to Combine Two Columns in Microsoft Excel (Quick and Easy Method)\nMicrosoft Is Axing Three Excel Features Because Nobody Uses Them\nLayer 1 vs. Layer 2: Blockchain Layers Explained Simply\n10 Positive and Negative Impacts AI Is Already Having on Healthcare",
    "Microsoft Is Distributing a Deepfake Detection Tool\nMicrosoft Is Distributing a Deepfake Detection Tool\nMicrosoft Video Authenticator should make it easier to detect deepfakes.\nReaders like you help support MUO. When you make a purchase using links on our site, we may earn an affiliate commission. Read More.\nGiven how deepfakes are both potentially devastating to someone's reputation and very hard to spot when done right, they're a major security concern. Now, Microsoft is aiming to help stop deepfakes by releasing a tool that can detect them.\nThe threat posed by deepfakes is so substantial, it caused Facebook to ban all deepfake content.However, the biggest obstacleis how deepfakes are made to look convincing; as such, locating and confirming a deepfake is very difficult.\nAs reported on Microsoft On the Issues, the company wants to change that with its new tool. It's called theMicrosoft Video Authenticator, and it analyzes videos to look for the \"seams\" left behind from deepfake creation. These seams are invisible to human eyes, but a computer can spot them using frame-by-frame analysis.\nTo prevent malicious agents from reverse-engineering the technology,Microsoft Video Authenticator won't be a public release. Instead, Microsoft is distributing its system to news and political organizations. That way, those organizations can use the tool to separate the real from the fake.\nHowever, Microsoft has announced a second technology to identify deepfakes. The company proposes a system where a content creator can tag their videos with a unique \"signature.\" If the video is edited, it will generate a different signature than the original, thus identifying a deepfake.\nThe author of Deep Fakes and the Infocalypse, Nina Schick, said the following:\nThe only really widespread use we've seen so far is in non-consensual pornography against women.\nBut synthetic media is expected to become ubiquitous in about three to five years, so we need to develop these tools going forward.\nHowever, as detection capabilities get better, so too will the generation capability - it's never going to be the case that Microsoft can release one tool that can detect all kinds of video manipulation.\nDeepfakes are a significant problem for media credibility, and companies need a way to tell what's real from what's fiction. Microsoft has released a tool to help spot edited videos. However, whether or not the tool will be enough to keep up with deepfakedevelopers is yet to be seen.\nIf you're unsure of what deepfakes are and why they matter, it's definitely worth taking some time to learn about them. Deepfakes are poised to become this decade's equivalent of \"Photoshopping\" images and can cause severe damage if used maliciously.\nSimon is a Computer Science BSc graduate who has been writing about technology since 2014, and using Windows machines since 3.1. After working for an indie game studio, he found his passion for writing and decided to use his skill set to write about all things tech.\nIf all social platforms closed down tomorrow, would your life be better or worse?\nOpenAI Launches a ChatGPT App for iOS (With Android to Follow)\nWhy OpenAI's CEO Is Calling for More AI Regulation (and What That Means)\nHow to Combine Two Columns in Microsoft Excel (Quick and Easy Method)\nMicrosoft Is Axing Three Excel Features Because Nobody Uses Them\nInsta360 Flow AI Smartphone Gimbal Review: DJI Has Some Real Competition",
    "Can You Spot the Fake? 5 Free Online Tests to Detect and Learn About Fake Information\nCan You Spot the Fake? 5 Free Online Tests to Detect and Learn About Fake Information\nCan you spot fake news, fake social profiles, or fake photos and videos? Take these free online tests to find out.\nReaders like you help support MUO. When you make a purchase using links on our site, we may earn an affiliate commission. Read More.\nFake news. Fake social profiles. Fake photos and videos. Take these free online tests to see if you can spot the fakes, and learn how to detect the internet's attempts to hoodwink you.\nIt's getting harder and harder to trust anything you see on the internet. It seems like so much of it is full of misinformation. Pictures and videos are manipulated, social media trolls turn out to be bots, the media displays fake news. The onus is now on us, the average person, to learn the motivation behind disinformation, how our brain tricks us, and how to spot fakes.\n1. Spot The Troll (Web): Is It a Real Social Profile or a Fake Bot?\nBots and fake social media accounts are changing how we interact with the internet. Maleficent forces use these tools to push propaganda and false narratives that alter how we think about issues around us. You need to learn how to spot a troll from a real user on the internet.\nSpot The Troll is a browser game or quiz that creates eight different profiles. All of these are based on real-world occurrences. For each profile, you'll see their personal details and their last few posts on social media (whether it's Twitter, Facebook, or Instagram). Go through them, look for the signs, and decide if it is an authentic account or a professional troll.\nThe game is harder than you might think, but that's how clever trolls have become. After you give your answer, Spot The Troll will highlight all the indicators that you make the profile fake or real, so you can monitor for those in the real world.\nOnce you're done with the game, you should visit the Resources section as well. You'll find organizations fighting fake news and trolls, and must-read articles about disinformation.\nA group of academics, journalists, and media experts banded together to create an online game where you become a fake news media mogul. In Bad News, your objective is to build an online media empire, learning the tricks of how to spread fake news.\nYou have two meters to track: followers and credibility. You need to strike a balance between gaining followers with your sensational headlines while publishing things that seem credible. Gain as many followers as possible while earning fake credibility.\nThrough the game, you'll learn about the six tools of fake news media sites: emotion, polarization, impersonation, conspiracy, discredit, trolling. You'll make memes and publish articles that use these six evils of fake news into creating a false narrative online, and learn how they psychologically impact readers. As your followers and credibility rises, challenge your friends to do better.\nThe game is ideally intended for users between 15 and 35 years of age. There's a Bad News Junior for younger children. You can also optionally participate in an academic survey to see whether you can spot fake news headlines.\n3. Scam Spotter and Jigsaw Phishing (Web): Can You Spot the Scam or Phishing Message?\nConsumers reported almost $2 billion in scams in 2019 in the USA alone. The online scam and phishing market has evolved from the \"Nigerian prince\" trick, and you need to keep abreast of these changes. Two sites are here to tell you all about it, and give you a quiz to test how easily you'd fall for a scam.\nScam Spotter focuses on text message scams. Developed by the Cybercrime Support Network and Google, it gives you a five-part quiz to spot phishing attempts through texts. It breaks down common online scams into four types: online shopping, romance, good news, and bad news. For each type, it has three golden rules that you should keep in mind. Once you learn these 12 rules, you'll be ready to spot any sort of scam out there.\nMeanwhile, Jigsaw (one of Google's sister companies) has an eight-question quiz to spot email phishing. The quiz will show you how to detect phishing attempts and scams by looking out for things like attachments, email addresses, and grammar. It's one of the best ways to understand digital security and protect your privacy.\n4. Spot the Deepfake (Web): Can you Detect the Fake Video or Picture?\nUsing artificial intelligence (AU), one can now manipulate videos and images to a scary degree. They're called deepfakes and they look almost real. Since AI can make mind-blowing creations, you need to know the tell-tale signs of a deepfake image or video. These tests will show you how.\nSpot the Deepfake is Microsoft's 10-question quiz to educate you about deepfakes and try to detect them. It will point out both the signs in a video to watch out for, as well as the signs within yourself. Factors like motivation, emotional reaction, and others can be great indicators of deepfakes.\nMIT's Detect Fakes is another short quiz to compare two videos side-by-side. One is real, the other is fake. Can you figure out which one is the deepfake? MIT offers a hint for each test, and after you answer, educates you about the signs you should look for.\n'Which Face Is Real?' shows a real person's photo alongside a picture generated by AI. You have to click on the real one. Try the test, you'll be shocked by how often you are faked out into thinking an AI-created photo is a real person.\n5. Clearer Thinking (Web): Cognitive Bias Tests and How Your Mind Tricks You\nYou think someone else is biased, prejudiced, irrational, skeptical, negative, or has some other such pre-existing mindset. You might not be too different. The human mind is fallible to several cognitive biases and logical fallacies. Clearer Thinking will help you test yours, and teach you a bit about how your brain is tricking you.\nThe main part of the site is the set of quizzes and questionnaires. You can find how rational you are, what your intrinsic values are or your primal beliefs about the world, and other ways to understand yourself. But you can also go beyond that to understand the world, make better decisions, improve your mood, and accomplish your goals.\nThe last section is especially interesting though, called Recognize Faulty Logic. Since so much of the internet plays host to bad arguments, both well-meaning and in bad faith, you need to be able to spot these. In the information overload we face, you'll become a more informed citizen when you can recognize rhetorical fallacies and detect weak arguments.\nThe rest of the website at Clearer Thinking is worth browsing too. You'll find videos about how your brain works, a blog full of articles on the human mind, and free resources for teachers.\nAs you can see, it's easier to spot fake news or trolls online once you develop a habit of critical thinking. Yes, it seems troublesome to question everything you see on the internet, but isn't that better than developing deep, ingrained biases that affect who you are as a person?\nIn fact, if you become more aware of your own biases and thinking patterns, you'll be able to spot fakes faster. There are some excellent free online brain tests to find your cognitive patterns. Once you realize your own brain's potential, you'll be ready to take on any disinformation.\nMihir Patkar has been writing on technology and productivity for over 14 years at some of the top media publications across the world. He has an academic background in journalism.\nIf all social platforms closed down tomorrow, would your life be better or worse?\nOpenAI Launches a ChatGPT App for iOS (With Android to Follow)\nWhy OpenAI's CEO Is Calling for More AI Regulation (and What That Means)\nHow to Combine Two Columns in Microsoft Excel (Quick and Easy Method)\nMicrosoft Is Axing Three Excel Features Because Nobody Uses Them",
    "AI generated Content Detection - Illuminarty - Home\nIs it an AI or a human artist that created your image?\nDetect AI generated images, synthetic, tampered images and Deepfake.\nWe offer various functionalities to help our users understand the\nCheck whether or not an image has been AI-generated,\nfrom which AI model it has been generated from, and which regions of the\nFind out the probability of AI generation for a given image.\nIlluminarty combines various computer vision algorithms to provide\nthe likelihood of the image being generated from one of the public\nFind out the probability of AI generation for a given text.\nIlluminarty combines various NLP algorithms to provide\nthe likelihood of the text being generated from one of the public\nAI generation models. We denote the specific passages from the given text that are most likely the result\nWe are working on a web browser extension which let us use our detectors while we surf on the internet.\nIntegrate automated AI detection into your service through our\nAPI. The Illuminarty API allows you to detect whether an image was\nAI-generated and find the detectors that were the most likely to\nAI detection will always be free, but we offer additional features as a monthly subscription to sustain the service.\nWe provide a separate service for communities and enterprises, please contact us if you would like an arrangement.\nWe will always provide the basic AI detection functionalities for free.\nBasic functionalities of Illuminarty, made for individuals.\nFull functionalities of Illuminarty, made for professionals.\nFor the integration of Illuminarty into custom services.\nThe information contained in this website is for general information\npurposes only. The information is provided by Illuminarty and while we\nendeavour to keep the information up to date and correct, we make no\nrepresentations or warranties of any kind, express or implied, about the\ncompleteness, accuracy, reliability, suitability or availability with\nrespect to the website or the information, products, services, or related\ngraphics contained on the website for any purpose. Any reliance you place on\nsuch information is therefore strictly at your own risk. Use of this site is",
    "Fact check: How can I spot AI-generated images?  DW  04/09/2023\nSkip to contentSkip to main menuSkip to more DW sitesLatest videosLatest audioRegionsAfricaAsiaEuropeLatin AmericaMiddle EastNorth AmericaGermanyTopicsClimateEqualityHealthHuman RightsMigrationTechnologyCategoriesBusinessScienceEnvironmentCultureSportsLive TVLatest audioLatest videosIn focusUkraine WarTurkey ElectionsThis screenshot purports to show the arrest of Russian President Vladimir Putin, but the image is fakeMediaGlobal issuesFact check: How can I spot AI-generated images?Joscha Weber | Kathrin Wesolowski | Thomas Sparrow04/09/2023April 9, 2023Midjourney, DALL-E, DeepAI  images created with artificial intelligence tools are flooding social media. Some carry the risk of spreading false information. Which images are real and which are not? Here are a few tips.https://p.dw.com/p/4PnBKAdvertisementIt has never been easier to create images that look shockingly realistic but are actually fake.\nAnyone with an internet connection and access to a tool that uses artificial intelligence (AI)can create photorealistic images within seconds, and they can then spread them on social networks at breakneck speed.\nIn the last few days, many of these images have gone viral: Russian President Vladimir Putin apparently being arrested, or Tesla CEO Elon Musk holding hands with General Motors CEO Mary Barra, just to name two examples.\nThe problem is that both AI images show events that never happened. Even photographers have published portraits that turn out to be images created with artificial intelligence.\nAnd while some of these images may be funny, they can also pose real dangers in terms of disinformation and propaganda, according to experts consulted by DW.\nThis AI-generated viral photo purports to show Tesla head Elon Musk with GM CEO Mary Barra. It is fake\nPictures showing the arrest of politicians likePutin or former US President Donald Trump can be verified fairly quickly by users if they check reputable media sources.\nOther images are more difficult, such as those in which the people in the picture are not so well-known, AI expert Henry Ajder toldDW.\nOne example: a German member of parliament for the far-right AfD party spread an AI-generated image of screaming men on his Instagram account in order to show he was against the arrival of refugees.\nAnd it's not just AI-generated images of people that can spread disinformation, according to Ajder.\nHe said there have been examples of users creating events that never happened.\nThis was the case with a severe earthquake that is said to have shaken the Pacific Northwest of the United States and Canadain 2001.\nBut this earthquake never happened, and the images shared on Reddit were AI-generated.\nAnd this can be a problem, according to Ajder. \"If you're generating a landscape scene as opposed to a picture of a human being, it might be harder to spot,\" he explained.\nHowever, AI tools do make mistakes, even if they are evolving rapidly.Currently, as of April 2023, programs like Midjourney, DALL-E and DeepAI have their glitches, especially with images that show people.\nDW's fact-checking team has compiled some suggestions that can help you gauge whether an image is fake. But one initial word of caution: AI tools are developing so rapidly that these tips only reflect the current state of affairs.Fact check: How to spot AI images?To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\nMany images generated by AI look real at first glance.\nThat's why our first suggestion is to look closely at the picture. To do this, search for the image in the highest-possible resolution and then zoom in on the details.\nEnlarging the picture will reveal inconsistencies and errors that may have gone undetected at first glance.\nIf you're unsure whether an image is real or generated by AI, try to find its source.\nYou may be able to see some information on where the image was first postedby reading comments published by other usersbelow the picture.\nOr you may carry out a reverse image search.To do this, upload the image to tools like Google Image Reverse Search, TinEyeor Yandex, and you may find the original source of the image.\nThe results of these searches may also show links to fact checks done by reputable media outlets which provide further context.\nDo the depicted people have correct body proportions?\nIt'snot uncommon for AI-generated images to show discrepancies when it comes to proportions, with hands being too small or fingers too long, for example. Or the head and feet don't match the rest of the body.\nPutin is supposed to have knelt down in front of Xi Jinping, but a closer looks shows that the picture is fakeImage: Twitter/DW\nThis is the case with thepicture above, in whichPutin is supposed to have knelt down in front of Chinese PresidentXi Jinping. The kneeling person's shoe is disproportionately large and wide, and the calf appears elongated. The half-covered head is also very large and does not match the rest of the body in proportion.\nRead more about this fake in our dedicated fact check.\nHands are currently the main source of errors in AI image programs like Midjourney or DALL-E.\nPeople frequently have a sixth finger, such as the policeman to Putin's left in our picture at the very top.\nOr also in thesepictures of Pope Francis, which you've probably seen.\nBut did you realize that Pope Francis seems to only have four fingers in the right picture? And did you notice that his fingers on the left are unusually long? These photos are fake.\nOther common errors in AI-generated images include peoplewith far too many teeth, or glasses frames that are oddly deformed, or ears that have unrealistic shapes, such as in the aforementioned fake image of Xi and Putin.\nWithin a few seconds, image generators such as the Random Face Generatorcreate fake images of people who do not even exist. And even if the images look deceptively genuine, it's worth paying attention to unnatural shapes in ears, eyes or hair, as well as deformations in glasses or earrings, as the generator often makes mistakes.Surfaces that reflect, such as helmet visors, also cause problems for AI programs, sometimes appearing to disintegrate, as in the alleged Putin arrest.\nAI expert Henry Ajder warned, however, that newer versions of programs like Midjourney are becoming better at generating hands, which means that users won't be able to rely on spotting these kinds of mistakes much longer.\n5. Does the image look artificial and smoothed out?\nThe app Midjourney in particular creates many images that seem too good to be true.\nFollow your gut feeling here: Can such a perfect image with flawless people really be real?\n\"The faces are too pure, the textiles that are shown are also too harmonious,\" Andreas Dengel of the German Research Center for AI toldDW.\nPeople's skin in many AI images is often smooth and free of any irritation, and even their hair and teeth are flawless. This is usually not the case in real life.\nMany images also have an artistic, shiny, glittery look that even professional photographers have difficulty achieving in studio photography.\nAI tools often seem to design idealimages that are supposed to be perfect and please as many people as possible.\nThe background of an image can often reveal whether it was manipulated.\nHere, too, objects can appear deformed;for example,street lamps.\nIn a few cases, AI programs clone people and objects and use them twice. And it's not uncommon for the background of AI images to be blurred.\nBut even this blurring can contain errors, like theexample above, which purports to show an angry Will Smith at the Oscars.The background is not merely out of focus but appears artificially blurred.\nMany AI-generated images can currently still be debunked with a little research.But technology is getting better and mistakes are likely to become rarer in the future.CanAI detectors like Hugging Facehelp us to detect manipulation?\nBased on our findings, detectors provide clues, but nothing more.\nThe experts we interviewed tend to advise against their use, saying the tools are not developed enough. Even genuine photos are declared fake and vice versa.\nTherefore, in case of doubt, the best thing users can do to distinguish real events from fakes is to use their common sense, rely on reputable media and avoid sharing the pictures.\nEditor's note: Due to legal and journalistic reasons, DW does not currently publish images created with generative AI programs. As an exception, we might show AI images when they are the subject of reporting, for example a review of the capabilities of AI or verification of fake images. In this case, we clearly indicate that the pictures shown are created by AI.\nThis article was updatedon April 11to include the Random Face Generator.\nJoscha Weber Editor and fact-checker focusing on separating facts from fiction and uncovering disinformation.@joschaweberThomas Sparrow Political and security correspondent, fact checker@Thomas_SparrowSend us your feedbackYour feedbackAdvertisementSkip next section Explore moreExplore moreHugging Face AI detectorhuggingface.coAI photos showing excessive teethwww.dailymail.co.ukNew York Times article on AI-generated faceswww.nytimes.comShow more storiesSkip next section Related topicsRelated topicsFact checkArtificial intelligenceSkip next section DW's Top StoryDW's Top StoryG7 nations unveil new sanctions against RussiaConflicts3 hours ago3 hours agoUkraine updates: Zelenskyy attends Arab League SummitConflicts43 minutes ago43 minutes agoG7 vs. China: United against Beijing's growing power?Politics19 hours ago19 hours agoPage 1 of 2Skip next section More stories from DWMore stories from DWAfricaMorocco: Unauthorized brokers obstructing Schengen visasMorocco: Unauthorized brokers obstructing Schengen visasPolitics05/18/2023May 18, 2023More from AfricaAsiaEXCLUSIVE: Pakistan's Imran Khan fears rearrestEXCLUSIVE: Pakistan's Imran Khan fears rearrestLaw and Justice23 hours ago23 hours agoMore from AsiaGermanyGermany: Defense minister battles Bundeswehr bureaucracyGermany: Defense minister battles Bundeswehr bureaucracyPolitics22 hours ago22 hours agoMore from GermanyEuropeItaly: Death toll mounts in wake of severe floodingItaly: Death toll mounts in wake of severe floodingNature and Environment18 hours ago18 hours ago10 imagesMore from EuropeMiddle EastSaudi Arabia's Neom: A prestigious project with a dark sideSaudi Arabia's Neom: A prestigious project with a dark sidePolitics05/18/2023May 18, 2023More from Middle EastNorth AmericaUS: Public support drops for funding arms for UkraineUS: Public support drops for funding arms for UkraineConflicts05/18/2023May 18, 202302:42 minMore from North AmericaLatin AmericaColombia's president retracts claim children were rescuedColombia's president retracts claim children were rescuedCatastrophe17 hours ago17 hours agoMore from Latin AmericaGo to homepageAdvertisementAbout DWWho we arePressDW Global Media ForumDW offersLearn GermanDW AkademieServiceNewslettersReceptionFAQContactTopics from A to ZB2BSales & DistributionTravelAdvertisingFollow us on 2023 Deutsche WellePrivacy PolicyLegal noticeDigital accessibility",
    "AI Image Classification & Authenticity Detection | AI or Not\nJoin Our Waitlist for the Upcoming Beta Test!We're working to advance our services and APIs to handle text, audio, and video formats. Sign up now!Join the waitlist\nDrop your image anywhereWe only accept png and jpeg ProcessingCancelReportHelp us improve by leaving a report if you disagree with the resultSend\nThank you! Your submission has been received!Oops! Something went wrong while submitting the form.How to install1. Open Manage ExtensionsOpen Google Chrome and click on the Extensions button, then go to the Manage Extensions tab2. Turn on Developer modeIn the upper right corner, switch Toggle to enable Developer mode3. Upload extensionDownload\nSorry, the Image cannot be displayed. But you still can see your resultAI or Not May 9 2022 versionAI or NotDetermine whether an image has been generated byartificial intelligence or a humanAI or Not May 9 2022 versionAI-generated imageHuman created imageThere is a * chance that this image wascreated by**\nShare our service on social media to help friends detect generative art\nDrag and dropor upload your imageTap to upload yourimageWe support jpeg, png, webp, gif, tiff, bmp. 10Mb of maximum size.orAI or not?Something went wrong. Try again.By uploading an image or URL you agree to our Terms of ServiceDrop your image anywhere or upload from your deviceWe can identify images made byStable DiffusionMidJourneyDALL-EGANNeed to process alarge number ofImages?Optic API is perfect for bulk image analysis. Seamlessly integrate our advanced detection capabilities into your platform and gain valuableinsights.95%Accuracy2000+ QPSThroughput<0.5 SecLatencyGet API access\nDetect AI-Generated Images with our Telegram BotJust share your image with our Telegram bot @ai_or_not_bot, and itwill automatically respond with thedetection result.\nTry it on TelegramTry Optic Browser Extension (Beta)Join the ranks of beta testers for our innovative Chrome extension that detects AI-generated images on any website. Be among the first to experience this game-changing tool.install extension\nHow to installFAQIs my data secure when using Optic AI or Not?\nWe take data privacy and security very seriously. Uploaded images and inputted URLs are not stored on our servers longer than necessary for analysis, and we adhere to industry best practices and relevant data protection regulations. Please refer to our Privacy Policy for more details.How do I contact the Optic AI or Not team for support or inquiries?\nYou can contact our team by visiting the \"Contact Us\" page on our website. Fill out the contact form with your name, email address, and message, and our team will respond to your inquiry as soon as possible.How can I become a beta tester for the Optic AI or Not Chrome Extension?\nTo become a beta tester for our Chrome extension, visit the \"Chrome Extension\" section on our website and click the \"Sign Up for Beta Access\" button. Fill in the required information, and we will get in touch with you when the beta testing phase begins.Can I use Optic AI or Not for bulk image analysis?\nYes, we offer the Optic AI or Not API for bulk image analysis and seamless integration into your platform. Please feel the form and get our API and documentation page for more information on how to get started.How accurate is Optic AI or Not?\nOptic AI or Not uses own state-of-the-art algorithms and is designed to provide highly accurate results. Which image formats does Optic AI or Not support?\nOptic AI or Not supports popular image formats, including JPEG and PNG. If you have an image in another format, please convert it to a supported format before uploading.How do I upload an image or provide a URL for analysis?\nTo upload an image, click the \"Upload Image\" button and select the image file from your device. To provide a URL, simply paste the image URL into the \"Enter Image URL\" field and click \"Analyze Image.\"Is Optic AI or Not free to use?\nYes, Optic AI or Not is free to use for analyzing single images. We offer a premium API service for bulk image analysis or commercial use. Please refer to our API documentation for more details on pricing and usage.How does Optic AI or Not work?\nOptic AI or Not uses advanced algorithms and machine learning techniques to analyze images and detect signs of AI generation. Our service compares the input image to known patterns, artifacts, and characteristics of various AI models and human-made images to determine the origin of the content.What is Optic AI or Not?\nOptic AI or Not is a web service that helps users quickly and accurately determine whether an image has been generated by artificial intelligence (AI) or created by a human. If the image is AI-generated, our service identifies the AI model used (mid-journey, stable diffusion, or DALL-E).\nCompanyHomeAbout usCareersNewsContactGet in touchTwitterDiscordLinkedinLegalTermsPrivacyBy clicking Accept All Cookies, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. View our Privacy Policy for more information.PreferencesDenyAccept\nPrivacy Preference CenterWhen you visit websites, they may store or retrieve data in your browser. This storage is often necessary for the basic functionality of the website. The storage may be used for marketing, analytics, and personalization of the site, such as storing your preferences. Privacy is important to us, so you have the option of disabling certain types of storage that may not be necessary for the basic functioning of the website. Blocking categories may impact your experience on the website.Reject all cookiesAllow all cookiesManage Consent Preferences by CategoryEssentialAlways ActiveThese items are required to enable basic website functionality.MarketingEssentialThese items are used to deliver advertising that is more relevant to you and your interests. They may also be used to limit the number of times you see an advertisement and measure the effectiveness of advertising campaigns. Advertising networks usually place them with the website operators permission.PersonalizationEssentialThese items allow the website to remember choices you make (such as your user name, language, or the region you are in) and provide enhanced, more personal features. For example, a website may provide you with local weather reports or traffic news by storing data about your current location.AnalyticsEssentialThese items help the website operator understand how its website performs, how visitors interact with the site, and whether there may be technical issues. This storage type usually doesnt collect information that identifies a visitor.Confirm my preferences and close",
    "AI-generated images only going to get harder to detect, experts sayThe Last Ride  Take our news quiz  A balanced diet  The CrosswordFor You U.S. Sports Entertainment Life Money Tech Travel OpinionONLY AT USA TODAY:Newsletters For Subscribers From the Archives Crossword eNewspaper MagazinesInvestigations Weather Forecast Podcasts Video Humankind Just CuriousPets Food Reviewed Coupons Blueprint Best Auto Insurance Best Pet Insurance Best Travel Insurance Best Credit Cards Best CD Rates Best Personal Loans\nFACT CHECKArtificial IntelligenceAdd TopicAI-generated images already fool people. Why experts say they'll only get harder to detect. Chris MuellerUSA TODAYTwo weeks before Donald Trump set foot inside a New York courtroom, images of the former president being tackled and carried away by a groupof police officers went viral on social media.The sensational images first appeared on Twitter days after Trump claimed his arrest was imminent. They then migrated to other platforms, amassing tens of thousands of likes and shares along the way.There was just one problem: They were entirely fake.The realistic-looking images of Trump's fictitious encounter with police, along with a wide variety of other fake images that have spread online recently, were created with image generators powered by artificial intelligence.At the moment, it's still possible to look closely at images generated by AI and find clues they're not real. One of the Trump arrest images showed him with three legs, for example. And afake image of Russian President Vladimir Putin being arrested showed only three fingers on one of his hands, a common issue with these sorts of images.Fact check:Photos showing Trump arrested by law enforcement are computer-generatedBut multiple experts told USA TODAY it's only a matter of time before there will be no way to visually differentiate between a real image and an AI-generated image.\"I'm very confident in saying that in the long run, it will be impossible to tell the difference between a generated image and a real one,\" said James O'Brien, a computer science professor at the University of California, Berkeley. \"The generated images are just going to keep getting better.\"With AI, fake images can be made easier and faster than ever beforeThe idea of making fake images is far from new. For years, experts have had the ability to create realistic-looking fakes using photo editing software.\"A skilled artist could create an image that you really need to study to determine if it's real or an artist's creation,\" O'Brien said.But what used to take hours of effort and at least some level of expertise can now be done in minutes by someone with no training. All it typically takes to make an AI-generated image is a written prompt, limited only by the user's imagination.The availability of the technology and the ease with which it can be used has made it possible for almost anyone to make realistic fake images, said V.S. Subrahmanian, a computer science professor at Northwestern UniversityFact check:Experts say images showing Putin's arrest are fakeSubrahmanian pointed to a fake image of Pope Francis wearing a white puffer jacket as an example of an image created in obscurity that suddenly spread across social media, with many people believing it was real.The image was created by a 31-year-old construction worker from the Chicago area who, after taking psychedelics, decided to create the images onMidjourney, a popular AI image generator, according to Buzzfeed News.\"It's kind of comical, but at the same time it's chilling because you know if you can do this, you can put the pope or any world leader, or any of us, in some kind of position that we would prefer not to be in,\" said Subrahmanian. And the AI problem will get worse quickly.The technology behind AI image generators is improving so rapidly, experts say, the visual clues that now give away fake images will disappear entirely in the not-so-distant future, making it virtually impossible to tell what's fake and what's not.\"People aren't going to know what to believe and what not to believe,\" Subrahmanian said. \"People are going to end up believing things that are false.\"O'Brien agreed:\"Arewe going to have to suspect everything of being fake? I think the answer is yes.\"Education, transparency key to navigating AI landscape, experts saySubrahmanian said AI's increasing complexity and ease of access exacerbate the existing misinformation problem, since many users already \"don't always exercise a lot of judgment when they see something.\"The primary blind spot is online claims that align with the user's view of the world, according to astudyO'Brien worked onexamining how people evaluated the authenticity of online images.The study foundparticipants with higher digital media literacy were more likely to be skeptical of images, but it also found those same participants were more likely to deem images credible if they aligned with the user's pre-existing views.\"We're manipulated by people showing us things we would like to believe,\" O'Brien said. \"By showing us things we would like to believe, they're able to lead us in whatever direction they want.\"Ari Lightman, a digital media professor at Carnegie Mellon University, said people need more education on the topic from a younger age, even if the technology gets to a point where the average person can't tell the difference between a real image and a fake image.\"We need to understand that we can't stifle innovation or creation, but there are some drawbacks,\" he said.Fact check:Viral images of a beached great white shark are fakeFact check:Images show artificial intelligence concept, not satanic hotel in TexasThere also needs to be more transparency about where information is coming from online, said Lightman, who suggested AI-generated images could be labeled with a watermark or a stamp.While it's possible AI could be used to detect fake images, even that solution has its own limitations, O'Brien said.\"Then you just end up with an arms race,\" he said. \"As soon as I build a new detector, people can go look at it, see how it works and figure out a way to make the generator trick it.\"So, O'Brien said, instead of relying on technology to solve the problem, people's attitudes toward images they see online need to change. He compared it to the way most people now treat scam emails that promise large amounts of money.People will need to develop a similar filter for social media, especially if an image  or a video, or any piece of media  seems too good to be true. And that change needs to happen quickly, O'Brien said, preferably before it becomes impossible to tell apart real and fake images.\"By then, I hope we've gotten to a place where we don't trust images as much,\" he said. \"If not, we're in trouble.\"Featured Weekly AdAbout Us Newsroom Staff Ethical Principles Request a Correction Press Releases Accessibility Sitemap Subscription Terms & Conditions Terms of Service Your California Privacy Rights / Privacy Policy Privacy PolicyDo Not Sell / Share / Target My InfoCookie SettingsContact Us Help Center My Account Give Feedback Get Home Delivery eNewspaper USA TODAY Shop USA TODAY Print Editions Licensing & Reprints Advertise With Us Careers Internships Support Local BusinessNews Tips Submitting letters to the editor Podcasts Newsletters Mobile Apps Facebook Twitter Instagram LinkedIn Pinterest YouTube Reddit Flipboard RSS10Best Reviewed Jobs Sports Betting Sports Weekly Studio Gannett Classifieds Coupons Blueprint Auto Insurance Pet Insurance Travel Insurance Credit Cards Banking Personal Loans 2023 USA TODAY, a division of Gannett Satellite Information Network, LLC."
  ],
  "site:reddit.com camera sensor attestation": [
    "There is a mistyped Knox user certificate installed on my phone. Should I be concerned?",
    "Enthusiast community for Intel's Next Unit of Computing (NUC) products.\nI am running ESXi7 on a new NUC10i5FNK host and am receiving errors relating to TPM enablement and attestation.\nI have followed the virten.net guide for BIOS settings for the NUC10 series on ESXi, and have:\nIn light of this, I still receive the following error within vCenter for my NUC host:\n\"TPM 2.0 device detected but a connection cannot be established.\"\nI have disabled the TPM alarm definition so at least this alarm is suppressed...",
    "How can I find out what certificate authority is doing this?",
    "The place for discussing OnePlus and their products. /r/OnePlus is a fan subreddit and is not affiliated with OnePlus.\nI need some feature confirmation on LineageOS before I switch.\nHI, I'm daily driving a Oneplus 7 (not pro) and I need confirmation from someone using the OS on some critical features before I spend the hours wiping my phone and getting it back up to speed.\nCellular and texting via WI-FI/VoWIFI (very important)\nHelp on this is appreciated. Any quirks or issues you know of are also important information.\nEdit: Thanks for everyone's input. I think I'll hold off a little longer while I read more into hiding magisk and how to deal with overprotective banking apps.\nArchived post. New comments cannot be posted and votes cannot be cast.",
    "This subreddit is devoted to Shortcuts. Shortcuts is an Apple app for automation on iOS, iPadOS, and macOS.\nI have created a simple shortcut to generate a French covid 19 attestation offline (read this article about security issues https://www.broken-by-design.fr/posts/attestation-covid-19/)\nQR code identical to the one generated by the government website\nAutomation possible by providing as input a dictionary (I would also like to update it to be able to set the date)",
    "The official subreddit for the open source, privacy friendly mobile OS, CalyxOS.\nAn applied research project furthering the mission of the non-profit Calyx Institute.\nCalyxOS 3.5.1 - Android 12L is now available for all supported devices, i.e. Pixel 3 - Pixel 6, and Fairphone 4, OnePlus 8T, 9 and 9 Pro\nIt is currently in the beta channel, and will be bumped to the stable channel later after additional testing and feedback. Factory images will be available at that time as well.\nIt has been bumped to the stable channel, factory image links are up too.\nGoing forward, we'll be testing the releases even more and make sure things like this get caught early, especially before builds hit stable.\nThis was merely a UI bug. A code change by Google this month made it so that the Phone app accessing cellular information like it always has got reported as location access.\nmicroG SafetyNet Basic attestation is now working. Device Registration needs to be enabled.\nIf you had hidden Gallery before this update, you cannot unhide. Workaround, run adb shell pm install-existing com.android.gallery3d from a computer.\nNew icons don't show up for all apps. Workaround: Change Icon Shape from Settings -> Display, and change it back.\nDefault ringtone / notification / alarm reset. One time thing, we changed sounds for new devices\nFor most users BT audio will now work, however if BT audio still isn't working for you, then:",
    "The (un)official home of #teampixel and the #madebygoogle lineup on Reddit. Get support, learn new information, and hang out in the subreddit dedicated to Pixel, Nest, Chromecast, the Assistant, and a few more things from Google.\nThere's still a glimmer of hope for the Pixel 6 series getting Face Unlock",
    "News for Android developers with the who, what, where, when and how of the Android community.\nWe're on the Android engineering team. Ask us Anything about Android 11 updates to the Android Platform! (starts July 9)\nWere the Android engineering team, and we are excited to participate in another AMA on r/androiddev next week, on July 9th!\nFor our launch of the Android 11 Beta, we introduced #11WeeksOfAndroid, where next week were diving deep into Android 11 Compatibility, with a look at some of the new tools and milestones. As part of the week, were hosting an AMA on the recent updates weve made to the platform in Android 11.\nThis is your chance to ask us technical questions related to Android 11 features and changes. Please note that we want to keep the conversation focused strictly on the engineering of the platform.\nWe'll start answering questions on Thursday, July 9 at 12:00 PM PST / 3:00 PM EST (UTC 1900) and will continue until 1:20 PM PST / 4:20 PM EST. Feel free to submit your questions ahead of time. This thread will be used for both questions and answers. Please adhere to our community guidelines when participating in this conversation.\nWell have many participants in this AMA from across Android, including:\nChet Haase, Android Chief Advocate, Developer Relations\nDianne Hackborn, Manager of the Android framework team (Resources, Window Manager, Activity Manager, Multi-user, Printing, Accessibility, etc.)\nJacob Lehrbaum, Director, Android Developer Relations\nRomain Guy, Manager of the Android Toolkit/Jetpack team\nStephanie Cuthbertson, Senior Director of Product Management, Android\nYigit Boyar, TLM on Architecture Components; +RecyclerView, +Data Binding\nAdam Powell, TLM on UI toolkit/framework; views, Compose\nIan Lake, Software Engineer, Jetpack (Fragments, Activity, Navigation, Architecture Components)\nAndroid Studio AMA on July 30th (part of the Android Developer Tools week of #11WeeksOfAndroid)\nAndroid Jetpack & Jetpack Compose on August 27th (part of the UI week of #11WeeksOfAndroid)",
    "This community-run r/realme subreddit is all about the discussion on Realme smartphones and IoT products.\nHow I installed a CUSTOM ROM (AOSIP) on my REALME X2 PRO GLOBAL 12GB (Realme UI/Android 10). This method should work for any X2 PRO (as the guides I followed say that it should), i.e. CN version or phone with color OS. (GCAM with all three lenses full support, Root with MAGISK, DC dimming, etc. )\nBeautiful day today. My phone is finally free from BS Realme Bloat (goddamn app checker/piece of shit browser pre-installed/ hot apps nonsense and numerous others) and god knows what else (likely Chinese spy software). Its a beast thats been freed from its chains and is now running clean software.\nTypical concerns: Bluetooth/Fingerprint scanner/90Hz/All three lenses all working fine. Theres a GCAM 7 that handles all three lenses and the honestly the realme default camera app doesnt stand a chance with against the correctly set GCAM (more info below). My only issue is that the fingerprint scanner uses old color os graphics (which I dont really like) and performs a bit slower. Other than this everything works flawlessly. Ask me to test a specific thing, or if you want screenshots. Ill happily help you if Im not too busy.\nTheres a beautiful easy to follow guide here : https://gg-help.web.app/realme/x2-pro,\n(IGNORE) only problem is if your phone is running on Realme UI/Android 10 then you must downgrade. This guide has the downgrade steps that work fine but his Flashing the Custom ROM step has a link thats not working. The Global devices or Chinese devices link in step one that should get you a colour os zip file to downgrade to.\nTo solve this: you must download the ozip file from realme, for the my global version I found it here (https://download.c.realme.com/osupdate/RMX1931EX_11_OTA_0100_all_X2KmrxzKYeI8.ozip ). If youre on the CN version and cant find it let me know, Ill try to help if Im free.\nAnd thats it, that was the only hiccup, the rest of his instructions are FLAWLESS. (/IGNORE)\n2) I followed the steps, got my AOSIP running, changed the hideous default wallpaper, then followed the completing installation: extra steps he gives in the end (these are beautifully detailed, easy to follow instructions aswell) to install MAGISK, a GCAM port with the correct settings (if youre here just for the GCAM; there you go), and to enable 90 Hz and DC dimming.\nIts not the easiest process, might take up your entire morning. But good lord is the outcome worth it.\nSeriously guys ask me anything, ask for screenshots, Ill try and respond to you all. My phone is my baby right now, any excuse to use it more Ill probably take it lol.\nEdit: no need to change the ozip file to zip Orange fox has native support, and shout out to the dude who wrote the damn guide (u/davwheat), he's in the comments!\nEdit 2: no need to go looking for an Ozip file on your own anymore either, the guide linked here has everything now!",
    "As announced on July 27th, and on Sept 14th, 2021, The Team Formerly Known As PrivacyTools.io  the entirety of the team providing privacy-related advice & services to you for the past couple years  has transitioned to PrivacyGuides.org and r/PrivacyGuides."
  ],
  "site:reddit.com attested camera sensor": [
    "The go-to subreddit for anyone interested in IMAX and IMAX filmmaking/cinematography. Not associated with IMAX Corporation.\nTo my knowledge, all IMAX cameras that can record in the 1.43 aspect ratio use IMAX's stupidly expensive and difficult to handle 70MM film.\nI'd love to see an MCU movie shot in the 1.43 aspect ratio, but I really can't blame them, it just isn't worth the hassle over using a cheaper, smaller, and more easy to handle 1.90 IMAX certified camera.\nDo you think in the future it's possible IMAX will make a digital camera with that massive 16K sensor?",
    "The go-to subreddit for anyone interested in IMAX and IMAX filmmaking/cinematography. Not associated with IMAX Corporation.\nWill the upcoming ALEXA 35 be a \"certified camera\"?\nhttps://ymcinema.com/2022/05/24/meet-the-alexa-35-full-camera-brochure-has-leaked/\nIt's neither full-frame nor 8K but it does have a slightly higher resolution than the LF, and that's a certified camera. The color science and dynamic range seems advanced enough for the DMR process compared to the other cameras. Plus, the full Open Gate can still give you that IMAX ratio for scenes a la Dune/Eternals.",
    "A place to converse with the photographic community here on Reddit.\nWell formulated posts with clear accurate titles will help, the better the post the better the response.\nBeginner or Pro all interest levels in photography are welcome to contribute.\nI was editing video and saw a spot on all of my clips and noticed this on my sensor.",
    "/r/photography is a place to politely discuss the tools, technique and culture of photography.\nThis is not a good place to simply share cool photos/videos or promote your own work and projects, but rather a place to discuss photography as an art and post things that would be of interest to other photographers.\nDoes anyone know what these dots are (and how I can get rid of them)? Taken at f/22 at infinity to make them show up...",
    "Android news, reviews, tips, and discussions about rooting, tutorials, and apps. Generic discussion about phones/tablets is allowed, but technical-support, upgrade related questions or buy/sell posts, app recommendations and carrier-related issues should be asked in their respective subreddits!\nOnePlus 10 Pro Unboxing & First Impression: Good looking but lacks surprises.\nArchived post. New comments cannot be posted and votes cannot be cast.",
    "The place for discussing OnePlus and their products. /r/OnePlus is a fan subreddit and is not affiliated with OnePlus.\nOneplus confirms that the ultrawide on the 9 Pro is a 50MP sensor\nArchived post. New comments cannot be posted and votes cannot be cast.",
    "An unofficial community to discuss Apple devices and software, including news, rumors, opinions and analysis pertaining to the company located at One Apple Park Way.",
    "**The Oculus subreddit, a place for Oculus fans to discuss VR.**\nApparently doc-ok managed to hack into the Rift camera, and it appears to be 1200x900",
    "The (un)official home of #teampixel and the #madebygoogle lineup on Reddit. Get support, learn new information, and hang out in the subreddit dedicated to Pixel, Nest, Chromecast, the Assistant, and a few more things from Google.\nWhy are my Pixel 6 photos SO overprocessed? I feel like I'm missing a setting.\nI am a professional photographer. I bought the Pixel 6 because I had played with my friend's iPhone 13 and that was the first time I was really impressed with a phone's photo quality -- but there's no way I was paying more than the price of a DSLR for a phone. So the Pixel 6 sounded perfect.\nSo far I am having a hard time getting good images out of the camera.\nTaken in good light, there is a TON of sharpening and HDR/clarity on the JPG that isn't necessary in good light -- and the photo still doesn't look very sharp.\nThe second is the RAW version that I popped into Photoshop. Now, I noticed that the photo was far from sharp in RAW form -- quite far from sharp -- but overall I rather easily made it look much better than the default pic\nIs it possible that main camera isn't focusing right and the AI is just trying to compensate with too much artificial sharpening? Everyone seems quite happy with the camera on this phone so I'm wondering if there's just something wrong with mine."
  ],
  "site:reddit.com image verification": [
    "Have questions about moderating your subreddit? We might be able to help!\nSorry, this post was deleted by the person who originally posted it.",
    "Ask questions and post articles about the Go programming language and related tools, events etc.\nIs there and way with Go to verify if an image file is cartoon or not?\nIs the JPG an actual photo of someone real or just a really good anime version?",
    "",
    "Your designs on custom products. Shipped directly to your customers.\nAbout a week ago I ordered one sample mug just so I could see what the quality would be like. The order seemed to go through but shortly after I received an email saying there was an error and that I would need to repurchase. I contacted support asking for them to look into it, as I just wanted to make sure I wouldn't be charged twice or something. Well they've contacted me today asking for verification. They want a picture of myself with my ID, with my credit card, and a picture of a piece of my mail. I'm not saying this isn't legit.... but I'm super uncomfortable with the idea of doing this. I understand why verification is needed sometimes but everyone I've spoken to who has used this site has said they've never needed to do this. What's more is, I've only ordered one single mug.\nIf just feels really weird for me and I'm not comfortable with it, but I'm bummed. I assume that means I won't be able to order from here? I've heard good things but idk. Has anyone else encountered this?",
    "Void Linux is a general operating system based on the Linux Kernel.\nHow to correctly verify iso image before burning it to usb\nSorry, this post was deleted by the person who originally posted it.",
    "Garena is a platform which publishes games for the SEA (South-East Asian) region, including titles such as League of Legends and PUBG. Keep in mind that this subreddit is for the community, so no Garena staff here.\nthe verification image is not showing how to fix this guys???",
    "Tails is a live operating system aimed at protecting your privacy and anonymity that you can start on almost any computer from a USB stick or a DVD.\nIf you use a Mac, Linux, or have OpenSSL you can verify the image using a SHA256 key based on what the project posts using the browser extension (e.g, if you disable scripts, etc.)\n% openssl dgst -sha256 ~/Downloads/tails-amd64-4.5.img\nSHA256(/Users/xxxxxxxxx/Downloads/tails-amd64-4.5.img)= b992d32826d572d80ddad5a7506e86daed1726661b52ba5a88513eae1e2cda65\nhttps://tails.boum.org/install/v2/Tails/amd64/stable/latest.json and for the 4.5 release is listed as.\nb992d32826d572d80ddad5a7506e86daed1726661b52ba5a88513eae1e2cda65\nCompare the output to the published fingerprint as a quick form of verification. Any tool capable of generating a SHA256 hash will do.\nArchived post. New comments cannot be posted and votes cannot be cast.",
    "Subreddit for Rise of Kingdoms - an epic strategy game with unrivaled degrees of freedom\nVerification image doesn't load - Stays white/blank on Emulator.\nHi. I finally found the time to sit on computer and play the game there too and let the phone charge so i installed an Emulator, i chose MeMu because i saw the options were fit for my low-end notebook but i can't play the game. The verification is at red 00:00 seconds time and requires me to complete it before i can do anything but when i click for it, it just shows a white frame for where the picture was supposed to be.\nIs there any solution to this or any clue as to what could be causing it if it has anything to do with the client side? And if it's from Lilith's side, is there a chance this could please be fixed? Because so far, playing this game is simply impossible for me since my phone can barely deal with it right now due to battery consumption and overheating.",
    "Dedicated to the branch of forensic science encompassing the recovery and investigation of material found in digital devices, often in relation to computer crime. This field involves the application of several information security principles and aims to provide for attribution and event reconstruction following forth from audit processes. This subreddit is not limited to just personal computers and encompasses all media that may also fall under digital forensics (e.g., cellphones, video, etc.).\nExperiencing very slow verification speed using FTK Imager\nUsers are experiencing significantly slow FTK Imager verification times when verifying their images from our file servers (WS2019 Standard VMs) for their processed images. Processing to our file servers may go at 60 MB/s yet verifying those same images may go at 7 MB/s. This is happening for all users regardless of what data they are trying to verify. We host the virtual file servers on Dell R730s running Hyper-V.\nI have tried setting exclusions in Windows Defender for the network locations, it does speed up some image verification as it no longer scans the files, but it always seems to sink back down to 7-15 MB/s for most images. When the data is verified locally and not on the network, verification speeds seem to match the processing speeds of 60 MB/s.\nThe file servers themselves have 10Gb vNICs and handle data copying easily and doesn't slow down when transferring data to our users, data is still transferred around 80-90 MB/s to th user's 1Gb computers.\nAny insight is appreciated, these slow verification times are hindering workflow!\n-Servers are Dell R730s with SAS connections to storage enclosures running HDDs and using Microsoft Storage Spaces Direct.\n-Connections are 10Gb to switches and 1Gb to user computers.",
    "A subreddit to ask questions (and get answers) about Reddit Tech Support.\nIm trying to join a subreddit and need to provide a photo for proof I meet the subs requirements. Ive DMd with the mod (not chat) but cannot for the life of me figure out how to send them the photo on the app or desktop.\nIve tried uninstalling/reinstalling the app, and theres no individual mod that I can start a chat with. I asked them for help too but know its not their responsibility to show me.\nAnyone run into this issue before? Would really love to join the sub and Im getting frustrated. Thanks!\nArchived post. New comments cannot be posted and votes cannot be cast."
  ],
  "site:reddit.com metadata image edits": [
    "Hi, im trying to edit exif of an image so that I can add Location metadata in it.\nI tried using this package https://pub.dev/packages/edit_exif but I can't even get it to show the metadata.",
    "This subreddit is a place for all things related to the Dart programming language and its tools.\nHi, im trying to edit exif of an image so that I can add Location metadata in it.\nI tried using this package https://pub.dev/packages/edit_exif but I can't even get it to show the metadata.",
    "A place for all things related to the Rust programming languagean open-source systems language that emphasizes performance, reliability, and productivity.\nLooking for a Crate to edit Exif Metadata for JPEG and TIFF images\nHello guys, I'm a very beginner in Rust (just about half the book now).\nI need to edit Exif Metadata of thousands of photos both in JPEG and TIFF format so I thought I should take this opportunity to write a simple Rust console application but I can't find any Crate. It seems they're all only for reading data or they don't support TIFF files.\nIn C# there're some Nuget packages doing what I search but I find it pity not to go with Rust and pass on this opportunity to improve my Rust skills.\nHowever I want to do this rather quickly, no need to spend many hours writing my own Exif editing Crate (I really don't have the free time for that)\nSo is what I'm looking for not existing yet in Rust?",
    "new to jellyfin, how to make editing images and metadata easier?\nI'm migrating 2000 movies and 250 tv shows from plex to jellyfin. I'm trying to adjust the media info to be at the bare minimum level, like getting proper posters, and identifying the proper titles, but the ui is just not fun to work with. Like, how come the page takes me back to the top every time I update an image? Aside from that, having to edit so many entries manually is just tedious.\nI'm not looking for anything like plex meta manager, I'm just curious if there's anything I can do to streamline this process? I hear some ppl mention tinymediamanager, is that worth buying to help me with this? Or are there any kind of tools I can use that aren't the jellyfin media player? it's just such a slow and painful process to do it like this, surely there must be a better way. Thanks in advance for any recommendations",
    "Welcome to the unofficial Stable Diffusion subreddit! We encourage you to share your awesome generations, discuss the various repos, news about releases, and more! Be sure to check out the pinned post for our rules and tips on how to get started! Prompt sharing is highly encouraged, but not required.\nEXIF, IPTC and/or XMP metadata to store image generation parameters: which one is the best?\nI changed optimized_img2img.py and optimized_txt2img.py slightly, including:\nA --prefix argument (to be added to the folder name, to have a better folder management).\nAutomatically saving all parameters to a IPTC tag (\"caption/abstract\") when the output is a JPEG file.\nWorks great, specially using Irfanview as the default image viewer on Windows: just press [I] and click [IPTC info] to see the entire command line (including seed, so you can just copy and paste it to the command line to produce the same result):\nI'm not sure if EXIF, IPTC or XMP metadata would be better for this, though (it might depend on what software you already use, where and how you want to share the images, etc.), but most sharing sites, apps and editing software strips metadata anyway (of course, there are exceptions), so this might be more helpful to remember/retrieve the parameters yourself than sharing on Reddit, for example.\nI might put my code on GitHub, but there are already other pull requests for EXIF and XMP, so I'm wondering about specific reasons for people to choose among EXIF, XMP and IPTC.",
    "News, discussion, and community support for Synology devices\nI need to batch edit the metadata in some photos in Synology Photos using the filename. From what I've read this would have been easy on DSM 6 with exiftool but that package isn't available on DSM 7 and it doesn't appear it ever will be. Is there an alternative?",
    "Discussion about Kotlin, a statically typed programming language for the JVM, Android, JavaScript, and native.\nBuilding my first android app and hit a hard section. Is there an \"easy\" way or a proper way to do this? I've been experimenting with ExifInterface and I've been able to use the TAG_XMP to get the rdf-xml data but it seems like it's not properly decoded. Some characters are not registered in the header, there is a lot of empty whitespaces. Plus there doesn't seem to be a way to move around it like you can with a json just interate through.\nI'm wondering if there is a library or better way to edit the metadata. Specifically looking to mostly add tags(Ideally be able to able to collect them from the photo, add/remove tags, sort the list, and then add them back) to the jpgs and add or adjust date taken. I'd appreciate any help. If there is a better place to ask please let me know.",
    "An unofficial community to discuss Apple devices and software, including news, rumors, opinions and analysis pertaining to the company located at One Apple Park Way.\n[update (11/17/14) - IOS 8.1.1 resolves this bug for edits applied via the native Photos app]\nAfter running through a number of test cases I've determined that photo editing is a huge mess in iOS 8.0.2. After editing an image (using the native Photos.app or an image editing extension):\nAll metadata, including geotag, date taken, and other EXIF, is lost upon export or access by any app.\nThe edited photo cannot be synchronized back to a computer via Image Capture or iPhoto (desktop) - only the original will transfer\nReverting back to the original image in Photos.app restores the lost metadata\nTo test for yourself, edit a photo and then upload it to Dropbox, Flickr, Google+ Photos, or use PhotoSync to send it to some other destination. You'll see that the edited version transfers but loses all of the metadata that enriches how most of these sites automatically organize and tag your photos. I've also tried using Metapho and other apps to view the photo EXIF locally on the phone (e.g. pre-export) - same result, all fields are blank for edited images.\nThis bug effectively makes photo editing on iOS useless for me. I rely on EXIF tags and other photo metadata so that I don't have to manually sort and organize images. I also used to import DSLR photos to my iPad with the camera connection kit for quick in-field edits, but no longer can do so without destroying metadata.\nIf anyone's on iOS 8.1 beta I'd be curious whether this bug has been fixed. I also encourage anyone encountering similar issues to submit a bug report or use Apple's web-based feedback form - based on conversations with a few app developers, this has been a problem throughout the iOS 8 beta period, and additional feedback might help Apple realize these fixes need to be prioritized.",
    "Is it possible to add/edit camera metadata for photos?"
  ],
  "site:reddit.com AI-generated images detection": [
    "Hello everyone, the last weeks AI art has been making the rounds in a lot of places and there seems to be an increasing want of people for something which makes it easier to know when people generated AI art in places where this is not supposed to be posted.mm_maybe has built an open source AI art detector tool. Because he currently can't post he asked if I could post about this on his behalf. The model is trained on many AI artworks to be able to identify them. In many cases the model will correctly identify if something is AI generated art or a human made artwork. It should be added that the model is not always a 100% accurate, there are cases where it might accidentally identify something as AI art or human made where it isn't, but he wants people to report this, preferably with details, so that the model can be improved. We are currently trying to find reasons for incorrect results to resolve them. This means that you CAN'T use this tool to 100% determine if something is AI art, but it will have a high chance of giving correct results.To make it possible for it to be improved more easily, the code will be available for others to fork it (copy and build upon it) and add improvements to it.The model can be downloaded and run locally on a computer as well.You can try out a demo here, sometimes the demo can give an error because of huggingface, but if you run it locally it should always work: https://huggingface.co/spaces/umm-maybe/AI-image-detectorHe wrote an article explaining it in more detail and his motivations here:\nhttps://medium.com/@matthewmaybe/can-an-ai-learn-to-identify-ai-art-545d9d6af226",
    "A place for artists from redditgetsdrawn, sketchdaily, and other art-related subs to come together and discuss all non-business things related to art, including technique, art crit, media, culture, art history, etc.\nI am trying to build a detection bot for AI-generated images\nI was not really sure if you would be interested in this topic,but since my main goal was to try to provide this service to some artists & art consumers, I just decided to come up and ask for some feedback\nfor some context, I began making this bot when I heard from an artist that works on the pixiv fanbox that some people started uploading images that were generated by doing a retouch from the other artworks using AI software (e.g. waifudiffusion).\nthe website tells you how probable it is for the image to be made from an AI\nmy question is whether this would actually be of help to you guys, I recently heard from an artist that if the bot malfunctions and deems your artwork as an AI-generated image then this could be quite detrimental, which is exactly the opposite of what I want.\ndo you think this tool would be useful regardless? and what would be the best follow-up for those artists when these malfunctions happen?\nArchived post. New comments cannot be posted and votes cannot be cast.",
    "Some pictures are obviously wonky but some are really good like thisfacedoesnotexist.com and it's obvious that it will only get better. Humans might not be able to understand but can a computer?",
    "[P] Detect AI generated images vs. authentic photos\nThere's more and more content generated by GANs, I thought it would be useful to detect whether a piece of content is generated by AI. I'm starting with images and here's a demo https://www.galeras.io/. Suggestions are welcome!",
    "",
    "Following news and developments on ALL sides of the AI art debate (and more)\nI've been working on a concept for the past few weeks for a tool that can tell whether an image was generated using one of many Latent Diffusion tools that doesn't rely on watermarks (invisible or otherwise).\nI'm curious about everyone's thoughts on this. I see benefits to both sides of the AI argument, but neither group is a monolith, and I don't represent you guys.\nAI isn't quite ready to be trained with AI generated art. This tool has the potential to help filter out images that may reinforce some of a model's bad habits.\nRegardless of the ethics surrounding the issue, people don't like to be tricked. I've seen posts that have implied that lying about the provenance of an image is the smartest thing for an individual AI assisted artist to do. A scandal of widespread dishonesty would do nothing to help normalize this technology. Much to the credit of the the AI Art community, these people face heavy criticism.\nComputers can play chess, and Go better than everyone on earth. It's fun to see how well computers can play against people, but they don't belong in every tournament. A tool that can reliably determine whether an image was made with the method determined in the rules would help promote the spirit of such competitions.\nSome communities are about the process, or the medium. A photograph, for example, doesn't belong in a community about photo-realism. If you're engaged in the process, or the community disagrees with the philosophy behind generative AI assisted art, I think you'd benefit quite a bit from this.\nI, of course, have my own thoughts on the matter (and talk about this regularly on my main account) but I'd like to avoid coloring anyone's opinions by stating where I sit in this argument. Again, I don't represent you guys, nor do I claim to represent you. No group is a monolith, and I'm just one person. If possible, I'd like to hear your guy's thoughts and opinions on the matter to help build my understanding of how things might work.",
    "Welcome to the unofficial Stable Diffusion subreddit! We encourage you to share your awesome generations, discuss the various repos, news about releases, and more! Be sure to check out the pinned post for our rules and tips on how to get started! Prompt sharing is highly encouraged, but not required.\nHow do you prevent AI detection tools from identifying an image as AI generated? I use AI as about 20% of my workflow with the other 80% being photoshop digital painting an other modifications. I spend about 8 hours per image with only about an hour spent in stable diffusion. My images are still detected and flagged as AI.",
    "Welcome to the unofficial Stable Diffusion subreddit! We encourage you to share your awesome generations, discuss the various repos, news about releases, and more! Be sure to check out the pinned post for our rules and tips on how to get started! Prompt sharing is highly encouraged, but not required.\nA new Witch-hunt begins: ai-generated-content-detection\nhttps://hivemoderation.com/ai-generated-content-detection\nhttps://twitter.com/ZakugaMignon/status/1636822057559248898\nhttps://twitter.com/ZakugaMignon/status/1636734847048732674",
    "I am trying to build a bot for detecting AI-generated images\nI made a website for a PoC, but currently its not really working wellespecially for those images that have some human-like brush styleshttps://www.illuminarty.ai/\nThis started off as a pure hobby based but I started getting a bit of some traction from the art communities (mainly anime), now I am seriously considering if I should start spending more time on this but I lack brain cells\nis anyone here interested in this topic? do you think this would be of any use?"
  ]
}